{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9a63031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Input, \\\n",
    "                                        BatchNormalization, Embedding, Masking,\\\n",
    "                                        Bidirectional, Conv1D, MaxPooling1D, Flatten, concatenate, MaxPooling1D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "import lightgbm\n",
    "\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 300\n",
    "\n",
    "from datetime import datetime, timedelta, date\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cx_Oracle\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import joblib\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47fd78d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time(start_date:str, end_date:str, hour:int):\n",
    "        start = datetime.strptime(start_date, '%d/%m/%Y')\n",
    "        end = datetime.strptime(end_date, '%d/%m/%Y')\n",
    "\n",
    "        dates = []\n",
    "        while start<=end:\n",
    "            row = [start]\n",
    "            dates.append(row)\n",
    "            start += timedelta(hours=hour)\n",
    "\n",
    "        return pd.DataFrame(dates, columns=['TIMESTAMP'])\n",
    "    \n",
    "def query_status(eq_id):\n",
    "    try:\n",
    "        oracle_string = \"oracle+cx_oracle://{username}:{password}@{hostname}:{port}/{database}\"\n",
    "        engine = create_engine(\n",
    "            oracle_string.format(\n",
    "                username = 'TFM4CEBERUS',\n",
    "                password = 'TFM4CEBERUS',\n",
    "                hostname = 'ome-db.bth.infineon.com',\n",
    "                port = '1538',\n",
    "                database = 'ome'\n",
    "                )\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "    query = f\"\"\"select EQ_ID, TIMESTAMP_START, TIMESTAMP_END, DURATION, STATE_NAME, LEVEL3_NAME, LEVEL3 \n",
    "            from (SELECT\n",
    "              eq.eq_id, eq.name, eq.eq_type_ident\n",
    "            , data.timestamp_start,data.timestamp_end\n",
    "            , ROUND((data.timestamp_end - data.timestamp_start)*24*60*60,0) AS Duration\n",
    "            , data.tr25_3_status,data.tr25_4_status,data.tr25_5_status,data.eq_status\n",
    "            , level5s.state_name\n",
    "            , level5.state_name Level5_Name, level5.state_sign Level5\n",
    "            , level4.state_name Level4_Name, level4.state_sign Level4\n",
    "            , level3.state_name Level3_Name, level3.state_sign Level3\n",
    "            ,mh.device\n",
    "            ,mh.package,\n",
    "            mh.lotid as lot,\n",
    "            mh.product,\n",
    "            mh.operation\n",
    "\n",
    "            FROM OMEDATA.EQUIPMENT_STATE_HISTORY data\n",
    "            , OMEADMIN.EQUIPMENT_INSTANCES eq\n",
    "            , V_EQ_STATES level5s\n",
    "            , OMEADMIN.DEF_STANDARD_STATEMODEL level5\n",
    "            , OMEADMIN.DEF_STANDARD_STATEMODEL level4\n",
    "            , OMEADMIN.DEF_STANDARD_STATEMODEL level3\n",
    "            , OMEDATA.METAKEY_HISTORY mh\n",
    "\n",
    "            WHERE data.eq_ident  = eq.eq_ident\n",
    "            AND  data.eq_status = level5s.state_ident(+)\n",
    "            AND level5.state_ident = data.tr25_5_status\n",
    "            AND level4.state_ident = data.tr25_4_status\n",
    "            AND level3.state_ident = data.tr25_3_status\n",
    "            AND  data.metakey_ident =mh.ident(+)\n",
    "            and data.timestamp_start > sysdate - 1050)\n",
    "            where eq_id = '{eq_id}'\n",
    "            ORDER BY TIMESTAMP_START\"\"\"\n",
    "\n",
    "    status = pd.read_sql(query, engine)\n",
    "    status.columns = map(lambda x: str(x).upper(), status.columns) \n",
    "\n",
    "    return status\n",
    "\n",
    "def query_alarm(eq_id):\n",
    "    try:\n",
    "        oracle_string = \"oracle+cx_oracle://{username}:{password}@{hostname}:{port}/{database}\"\n",
    "        engine = create_engine(\n",
    "            oracle_string.format(\n",
    "                username = 'TFM4CEBERUS',\n",
    "                password = 'TFM4CEBERUS',\n",
    "                hostname = 'ome-db.bth.infineon.com',\n",
    "                port = '1538',\n",
    "                database = 'ome'\n",
    "                )\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "    query = f\"\"\"select ei.eq_id, ei.name, ea.alarm_id, ea.alarm_text, ea.alarm_type, ac.name as alarm_class,\n",
    "                ah.timestamp_start,ah.timestamp_end, mh.lot\n",
    "                from OMEADMIN.equipment_instances ei\n",
    "                join OMEADMIN.equipment_alarms ea on (ei.eq_type_ident(+) = ea.eq_type_ident)\n",
    "                join OMEDATA.ALARM_HISTORY ah on (ea.alarm_id = ah.alarm_id and ah.eq_ident = ei.eq_ident)\n",
    "                join OMEDATA.METAKEY_HISTORY mh on (ah.metakey_ident = mh.ident)\n",
    "                join OMEADMIN.EQUIPMENT_ALARM_CLASSES ac on (ac.IDENT = ea.ALARM_CLASS_IDENT and ac.eq_type_ident = ea.eq_type_ident)\n",
    "                where ah.timestamp_start > sysdate - 3000\n",
    "                and ei.eq_id = '{eq_id}'\n",
    "                order by ah.timestamp_start\"\"\"\n",
    "    \n",
    "    alarm = pd.read_sql(query, engine)\n",
    "    alarm.columns = map(lambda x: str(x).upper(), alarm.columns) \n",
    "\n",
    "    return alarm\n",
    "\n",
    "def aggregate(timeframe_table, lookback_window, status_table):\n",
    "    statename_df = pd.DataFrame(columns=status_table[\"STATE_NAME\"].unique())\n",
    "\n",
    "    for idx, row in timeframe_table.iterrows():\n",
    "        end = row[\"TIMESTAMP\"]\n",
    "        start = end - timedelta(hours=lookback_window)\n",
    "\n",
    "        ## count the frequencies of each statename, include everything since feature engineering would be performed\n",
    "        filtered_statename = status_table.loc[(status_table[\"TIMESTAMP_START\"] >= start) & \n",
    "                                              (status_table[\"TIMESTAMP_START\"] <= end)]\n",
    "        unique = filtered_statename[\"STATE_NAME\"].unique()\n",
    "        status_dict = {key:int(sum(filtered_statename.loc[filtered_statename.STATE_NAME==key][\"DURATION\"])) \n",
    "                       for key in unique}\n",
    "        \n",
    "        statename_df = statename_df.append(status_dict, ignore_index=True)\n",
    "            \n",
    "    statename_df = statename_df.fillna(0)\n",
    "    cols = statename_df.columns\n",
    "    statename_df[cols] = statename_df[cols].astype('int')\n",
    "    return statename_df\n",
    "\n",
    "\n",
    "def status_sequence(input_table, status_table, hour, scaled=False):\n",
    "        status_seq = []\n",
    "        duration_seq = []\n",
    "        \n",
    "        # validation check\n",
    "        if status_table.iloc[0][\"TIMESTAMP_START\"] > input_table.iloc[0][\"TIMESTAMP\"]:\n",
    "            raise Exception(\"Timeframe table must be a subset of the status table\")\n",
    "        if status_table.iloc[len(status_table)-1][\"TIMESTAMP_START\"] <= input_table.iloc[len(input_table)-1][\"TIMESTAMP\"]:\n",
    "                raise Exception(\"Timeframe table must be a subset of the status table\")\n",
    "        \n",
    "        for idx, row in input_table.iterrows():\n",
    "            end = row[\"TIMESTAMP\"]\n",
    "            start = end - timedelta(hours=hour)\n",
    "            \n",
    "            condition = (status_table[\"TIMESTAMP_START\"]>=start) & (status_table[\"TIMESTAMP_START\"]<=end)\n",
    "\n",
    "            table = status_table[condition]\n",
    "            status_seq.append(table[\"STATE_NAME\"].values)\n",
    "            if scaled:\n",
    "                duration_seq.append(table[\"SCALED_DURATION\"].values)\n",
    "            else:\n",
    "                duration_seq.append(table[\"DURATION\"].values)\n",
    "\n",
    "        return status_seq, duration_seq\n",
    "\n",
    "\n",
    "def major_down(input_df, status_table, hour, threshold): \n",
    "        hour = pd.Timedelta(hours=hour)\n",
    "        major_down = []\n",
    "\n",
    "        for idx, row in input_df.iterrows():\n",
    "            start = row['TIMESTAMP']\n",
    "            end = start+hour\n",
    "            frame = status_table[(status_table['TIMESTAMP_START']>start) & (status_table['TIMESTAMP_START']<end)]\n",
    "            UD = frame.loc[frame['LEVEL3']=='UDT']\n",
    "            \n",
    "            # disregard \"waiting\" in statename\n",
    "\n",
    "            if len(UD) == 0: #no record within this 6 hours:\n",
    "                major_down.append(0)\n",
    "            else:\n",
    "                time_diff = (UD['TIMESTAMP_END']-UD['TIMESTAMP_START']).dt.seconds\n",
    "                if any(time_diff>=threshold): #threshold = 3600s\n",
    "                    major_down.append(1)\n",
    "                else:\n",
    "                    major_down.append(0)\n",
    "        return np.array(major_down)\n",
    "\n",
    "def query_CAMSTAR(eq_id):\n",
    "    try:\n",
    "        oracle_string = \"oracle+cx_oracle://{username}:{password}@{hostname}:{port}/{database}\"\n",
    "        engine = create_engine(\n",
    "            oracle_string.format(\n",
    "                username = 'bth_odsprod',\n",
    "                password = 'bth_odsprodbth',\n",
    "                hostname = 'odsprod-db.bth.infineon.com',\n",
    "                port = '1523',\n",
    "                database = 'odsprod'\n",
    "                )\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "    query = f\"\"\"select EQUIPMENTNAME AS EQ_ID, TRACKINTIMESTAMP, TRACKOUTTIMESTAMP from A_WIPEQUIPMENTHISTORY t\n",
    "                where t.equipmentname = '{eq_id}'\n",
    "                ORDER BY TRACKINTIMESTAMP\"\"\"\n",
    "\n",
    "    status = pd.read_sql(query, engine)\n",
    "\n",
    "    return status\n",
    "\n",
    "def label_encode(statename_seq): # do this the manual way as we are not certain if sklearn LabelEncoder can handle 3D array\n",
    "    all_unique_statename = [set(ele) for ele in statename_seq]\n",
    "    unique_statenames = set()\n",
    "    for ele in all_unique_statename:\n",
    "        unique_statenames |= ele\n",
    "    \n",
    "    enc_label = 1  #start encoding from 1 as we have to pad the sequence with 0\n",
    "    mapping_dict = {}\n",
    "    for ele in unique_statenames:\n",
    "        mapping_dict[ele] = enc_label\n",
    "        enc_label += 1\n",
    "\n",
    "    enc_array = []\n",
    "    #X_seq is a 3D array\n",
    "    for timestamp in statename_seq:\n",
    "        tmp_arr = []\n",
    "        for ele in timestamp:\n",
    "            tmp_arr.append(mapping_dict[ele])\n",
    "        enc_array.append(np.array(tmp_arr))\n",
    "\n",
    "    return np.array(enc_array), len(unique_statenames)+1, mapping_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6c7b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "wba124_alarm = pd.read_excel(\"Data/WBA124_FullAlarm.xlsx\", usecols = \"B,C,D,F,M\")\n",
    "wba124_status = query_status('WBA124')\n",
    "# do not have to clean status data since the only information we need is that major down time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17533d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_filter = (wba124_status.LEVEL3=='UDT') & (wba124_status.DURATION>=3600)\n",
    "wba124_failure = wba124_status.loc[query_filter].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc483dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# drop rows with no alarm end time\n",
    "# EDA suggests that there are rows with null values\n",
    "wba124_alarm_n = wba124_alarm.loc[wba124_alarm.DT_CLEAR.notnull()]\n",
    "\n",
    "\n",
    "##### FEATURE ENGINEERING: compute the time since last alarm #####\n",
    "wba124_alarm_n['TIME SINCE LAST ALARM'] = (wba124_alarm_n['DT_SET'] - wba124_alarm_n['DT_SET'].shift(1)).dt.seconds\n",
    "\n",
    "\n",
    "##### Slice alarm table to make sure it is a subset of the status table #####\n",
    "status_start = wba124_status.iloc[0]['TIMESTAMP_START']\n",
    "query_filter = wba124_alarm_n.DT_SET > status_start\n",
    "wba124_alarm_n = wba124_alarm_n[query_filter]\n",
    "\n",
    "\n",
    "##### Calculate Time to Failure #####\n",
    "failure_idx = 0\n",
    "time_to_failure = []\n",
    "for idx, row in wba124_alarm_n.iterrows():\n",
    "    failure_time = wba124_failure.iloc[failure_idx]['TIMESTAMP_START']\n",
    "    \n",
    "    while row['DT_SET'] >= failure_time:\n",
    "        failure_idx += 1\n",
    "        failure_time = wba124_failure.iloc[failure_idx]['TIMESTAMP_START']\n",
    "\n",
    "    ttf = (failure_time-row['DT_SET']).total_seconds()\n",
    "    time_to_failure.append(ttf)\n",
    "\n",
    "wba124_alarm_n['TTF'] = time_to_failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdac7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Visualize TTF #####\n",
    "date = wba124_alarm_n['DT_SET'].dt.date\n",
    "ttf_y = wba124_alarm_n['TTF'].values\n",
    "\n",
    "# notice that there is a long period of time with no alarm at all\n",
    "plt.figure(figsize=(16,9))\n",
    "plt.plot(date, ttf_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb67e6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wba124_alarm_n.iloc[26850:26900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028dea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Calculate Time Since Last Failure #####\n",
    "# time_since_failure = []\n",
    "# wba124_idx = 0\n",
    "# for idx, row in wba124_failure.iterrows():\n",
    "#     if idx == len(wba124_failure)-1:\n",
    "#         break\n",
    "    \n",
    "#     next_failure_time = wba124_failure.iloc[idx+1]['TIMESTAMP_START']\n",
    "#     while wba124_alarm_n.iloc['DT_SET'].iloc[wba124_idx] <= next_failure_time:\n",
    "#         tsf = (row['TIMESTAMP_START']-wba124_alarm_n.iloc['DT_SET'].iloc[wba124_idx]).total_seconds()\n",
    "#         time_since_failure.append(tsf)\n",
    "#         wba124_idx += 1\n",
    "        \n",
    "\n",
    "# fail_idx = 0\n",
    "# for idx, row in wba124_alarm_n.iterrows():\n",
    "#     if row['DT_SET'] >= wba124_failure.iloc[fail_idx]['TIMESTAMP_START']:\n",
    "#         tsf = (row['DT_SET'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476ac7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = wba124_alarm_n['TTF']\n",
    "\n",
    "lb = LabelEncoder()\n",
    "enc_alarm = lb.fit_transform(wba124_alarm_n['Alarm ID'])\n",
    "wba124_alarm_n['ENC ALARM'] = enc_alarm\n",
    "\n",
    "features = ['ENC ALARM', 'TIME SINCE LAST ALARM']\n",
    "df = wba124_alarm_n[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae114fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = int(0.7*len(df))\n",
    "val_idx = int(0.8*(len(df)))\n",
    "\n",
    "X_train, y_train = df[:train_idx], target[:train_idx]\n",
    "X_val, y_val = df[train_idx:val_idx], target[train_idx:val_idx]\n",
    "X_test, y_test = df[val_idx:], target[val_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8026d285",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reg = lightgbm.LGBMRegressor(class_weight='balanced', random_state=42)\n",
    "reg.fit(X_train, y_train,\n",
    "       eval_set=[(X_val, y_val)],\n",
    "       eval_metric='mae')\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_predicted = reg.predict(X_test)\n",
    "\n",
    "rms = mean_squared_error(y_test, y_predicted, squared=True)\n",
    "rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3911ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(len(y_test[:100]))\n",
    "plt.figure(figsize=(16,9))\n",
    "plt.plot(x, y_predicted[:100], color='red')\n",
    "plt.plot(x, y_test[:100], color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad93081",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733cc3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Univariate Forecasting #####\n",
    "\n",
    "# check for stationarity\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "y = target.values\n",
    "result = adfuller(y)\n",
    "\n",
    "# we notice that the p-value is much smaller than 0.05 meaning that TTF is stationary\n",
    "print('ADF Statistic: %f' % result[0])\n",
    "print('p-value: %f' % result[1])\n",
    "print('Critical Values:')\n",
    "for key, value in result[4].items():\n",
    "    print('\\t%s: %.3f' % (key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e0d7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Check how many alarms happen on each month from 2018 #####\n",
    "\n",
    "wba124_alarm_n['YEAR MONTH'] = wba124_alarm_n['DT_SET'].dt.strftime('%Y-%m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb196bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_by_month = wba124_alarm_n.groupby(['YEAR MONTH']).size()\n",
    "\n",
    "plt.plot(freq_by_month.index, freq_by_month.values)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6b95e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Check alarm frequencies #####\n",
    "wba124_alarm_n['Alarm ID'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f61752",
   "metadata": {},
   "outputs": [],
   "source": [
    "wba124_alarm_n.reset_index(drop=True, inplace=True)\n",
    "wba124_alarm_n.loc[wba124_alarm_n['Alarm ID']==147]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2c2811",
   "metadata": {},
   "outputs": [],
   "source": [
    "wba124_alarm_n.iloc[35055:35070]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082cfb82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENV-MAL-VE",
   "language": "python",
   "name": "env-mal-ve_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
