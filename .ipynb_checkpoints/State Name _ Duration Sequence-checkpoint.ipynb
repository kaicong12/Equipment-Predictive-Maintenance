{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d533d7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, Embedding, concatenate, Reshape, Flatten, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 300\n",
    "ec = LabelEncoder() # encode the categorical status\n",
    "sc = StandardScaler() # scale the whole dataset to make the model learn faster\n",
    "\n",
    "class status_LSTM:\n",
    "    def __init__(self, eq_id, hour_horizontal, hour_vertical):\n",
    "        self.eq_id = eq_id\n",
    "        self.status_table = self.query_status()\n",
    "        self.hour_horizontal = hour_horizontal\n",
    "        self.hour_vertical = hour_vertical\n",
    "        \n",
    "        self.start_date = self.status_table.iloc[0][\"TIMESTAMP_START\"].date() + timedelta(days=1) # add one day to make it start from 00:00:00\n",
    "        self.end_date = self.status_table.iloc[len(self.status_table)-1][\"TIMESTAMP_START\"].date()\n",
    "        self.important_state_name = [\"Break down Maintenance\", \"Utility Problem\", \"Maintenance\", \"IT Problem\", \\\n",
    "                                    \"Waiting For Spares\", \"MOTOR ERROR\", \"Half Yearly PM\", \"IT Maintenance\", \\\n",
    "                                    \"Machine Failure\", \"HANG UP\", \"Waiting For Repair\", \"BAD WEDGE FORM\", \"INDEXER PROBLEM\",\\\n",
    "                                    \"PC Buyoff Failed\", \"SHORT TAIL\", \"WIRE BREAK\"]\n",
    "        \n",
    "        self.timeframe_table = self.generate_time(self.start_date.strftime(\"%d/%m/%Y\"), self.end_date.strftime(\"%d/%m/%Y\"), \\\n",
    "                                                  self.hour_horizontal, self.hour_vertical)\n",
    "\n",
    "        self.impt_state_seq, self.impt_duration_seq = self.status_sequence(self.timeframe_table, self.status_table, important=\"YES\")\n",
    "        self.major_down_arr = self.major_down(self.timeframe_table, self.status_table, 6, 3600)\n",
    "         \n",
    "    def query_status(self):\n",
    "            try:\n",
    "                oracle_string = \"oracle+cx_oracle://{username}:{password}@{hostname}:{port}/{database}\"\n",
    "                engine = create_engine(\n",
    "                    oracle_string.format(\n",
    "                        username = 'TFM4CEBERUS',\n",
    "                        password = 'TFM4CEBERUS',\n",
    "                        hostname = 'ome-db.bth.infineon.com',\n",
    "                        port = '1538',\n",
    "                        database = 'ome'\n",
    "                        )\n",
    "                    )\n",
    "            except Exception as e:\n",
    "                print(str(e))\n",
    "\n",
    "            query = f\"\"\"select EQ_ID, TIMESTAMP_START, TIMESTAMP_END, DURATION, STATE_NAME, LEVEL3_NAME, LEVEL3 \n",
    "                    from (SELECT\n",
    "                      eq.eq_id, eq.name, eq.eq_type_ident\n",
    "                    , data.timestamp_start,data.timestamp_end\n",
    "                    , ROUND((data.timestamp_end - data.timestamp_start)*24*60*60,0) AS Duration\n",
    "                    , data.tr25_3_status,data.tr25_4_status,data.tr25_5_status,data.eq_status\n",
    "                    , level5s.state_name\n",
    "                    , level5.state_name Level5_Name, level5.state_sign Level5\n",
    "                    , level4.state_name Level4_Name, level4.state_sign Level4\n",
    "                    , level3.state_name Level3_Name, level3.state_sign Level3\n",
    "                    ,mh.device\n",
    "                    ,mh.package,\n",
    "                    mh.lotid as lot,\n",
    "                    mh.product,\n",
    "                    mh.operation\n",
    "\n",
    "                    FROM OMEDATA.EQUIPMENT_STATE_HISTORY data\n",
    "                    , OMEADMIN.EQUIPMENT_INSTANCES eq\n",
    "                    , V_EQ_STATES level5s\n",
    "                    , OMEADMIN.DEF_STANDARD_STATEMODEL level5\n",
    "                    , OMEADMIN.DEF_STANDARD_STATEMODEL level4\n",
    "                    , OMEADMIN.DEF_STANDARD_STATEMODEL level3\n",
    "                    , OMEDATA.METAKEY_HISTORY mh\n",
    "\n",
    "                    WHERE data.eq_ident  = eq.eq_ident\n",
    "                    AND  data.eq_status = level5s.state_ident(+)\n",
    "                    AND level5.state_ident = data.tr25_5_status\n",
    "                    AND level4.state_ident = data.tr25_4_status\n",
    "                    AND level3.state_ident = data.tr25_3_status\n",
    "                    AND  data.metakey_ident =mh.ident(+)\n",
    "                    and data.timestamp_start > sysdate - 1050)\n",
    "                    where eq_id = '{self.eq_id}'\n",
    "                    ORDER BY TIMESTAMP_START\"\"\"\n",
    "\n",
    "            status = pd.read_sql(query, engine)\n",
    "            status.columns = map(lambda x: str(x).upper(), status.columns) \n",
    "\n",
    "            return status\n",
    "\n",
    "    def generate_time(self, start_date:str, end_date:str, hours_row:int, hour:int):\n",
    "        start = datetime.strptime(start_date, '%d/%m/%Y')\n",
    "        end = datetime.strptime(end_date, '%d/%m/%Y')\n",
    "\n",
    "        dates = []\n",
    "        while start+timedelta(hours=hours_row)<=end:\n",
    "            row = [start, start+timedelta(hours=hours_row)]\n",
    "            dates.append(row)\n",
    "            start += timedelta(hours=hour)\n",
    "\n",
    "        return pd.DataFrame(dates, columns=['TIMESTAMP_START', 'TIMESTAMP_END'])\n",
    "\n",
    "\n",
    "    def major_down(self, input_df, status_table, hour, threshold):\n",
    "            hour = pd.Timedelta(hours=hour)\n",
    "            major_down = []\n",
    "            \n",
    "            # timeframe table must be a subset of the status table to correctly determine major down\n",
    "            if status_table.iloc[0][\"TIMESTAMP_START\"] >= input_df.iloc[0][\"TIMESTAMP_START\"]:\n",
    "                raise Exception(\"Timeframe table must be a subset of the status table\")\n",
    "            if status_table.iloc[len(status_table)-1][\"TIMESTAMP_START\"] <= input_df.iloc[len(input_df)-1][\"TIMESTAMP_START\"]:\n",
    "                raise Exception(\"Timeframe table must be a subset of the status table\")\n",
    "\n",
    "            for idx, row in input_df.iterrows():\n",
    "                start = row['TIMESTAMP_END']\n",
    "                end = start+hour\n",
    "                frame = status_table[(status_table['TIMESTAMP_START']>start) & (status_table['TIMESTAMP_START']<end)]\n",
    "                UD = frame.loc[frame['LEVEL3']=='UDT']\n",
    "\n",
    "                if len(UD) == 0: #no record within this 6 hours:\n",
    "                    major_down.append(0)\n",
    "                else:\n",
    "                    time_diff = (UD['TIMESTAMP_END']-UD['TIMESTAMP_START']).dt.seconds\n",
    "                    if any(time_diff>threshold):\n",
    "                        major_down.append(1)\n",
    "                    else:\n",
    "                        major_down.append(0)\n",
    "            return major_down\n",
    "\n",
    "    def status_sequence(self, input_table, status_table, important=None):\n",
    "        status_seq = []\n",
    "        duration_seq = []\n",
    "        \n",
    "        # validation check\n",
    "        if status_table.iloc[0][\"TIMESTAMP_START\"] > input_table.iloc[0][\"TIMESTAMP_START\"]:\n",
    "            raise Exception(\"Timeframe table must be a subset of the status table\")\n",
    "        if status_table.iloc[len(status_table)-1][\"TIMESTAMP_START\"] <= input_table.iloc[len(input_table)-1][\"TIMESTAMP_START\"]:\n",
    "                raise Exception(\"Timeframe table must be a subset of the status table\")\n",
    "        \n",
    "        for idx, row in input_table.iterrows():\n",
    "            start = row[\"TIMESTAMP_START\"]\n",
    "            end = row[\"TIMESTAMP_END\"]\n",
    "            \n",
    "            condition = (status_table[\"TIMESTAMP_START\"]>=start) & (status_table[\"TIMESTAMP_START\"]<=end)\n",
    "            if important == \"YES\":\n",
    "                condition = (status_table[\"TIMESTAMP_START\"]>=start) & (status_table[\"TIMESTAMP_START\"]<=end) & \\\n",
    "                                (status_table.STATE_NAME.isin(self.important_state_name))\n",
    "            \n",
    "            table = status_table[condition]\n",
    "            status_seq.append(table[\"STATE_NAME\"].values)\n",
    "            duration_seq.append(table[\"DURATION\"].values)\n",
    "\n",
    "        return status_seq, duration_seq\n",
    "\n",
    "    def preprocess(self, status_seq, duration_seq):\n",
    "        X_seq = []\n",
    "        for ele, dur in zip(status_seq, duration_seq):\n",
    "            tmp = []\n",
    "            for idx in range(len(ele)):\n",
    "                tmp.append([ele[idx], int(dur[idx])])\n",
    "            X_seq.append(tmp)\n",
    "        return np.array(X_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4d09a0",
   "metadata": {},
   "source": [
    "# Training without excluding unimportant state name\n",
    "##### Skipped for now, training took too long (one epoch 30 mins!) as the padded time stamp contains too many status values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49663381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hour = 24\n",
    "# start = datetime.now()\n",
    "# print(f\"Training by looking back {hour} hours of alarm data\")\n",
    "# wba124 = status_LSTM(\"WBA124\", hour, 3)\n",
    "\n",
    "# # pad the alarm to train on LSTM\n",
    "# unpadded_status_arr = wba124.encoded_status_seq\n",
    "# unpadded_duration_arr = wba124.duration_seq\n",
    "\n",
    "# padded_status_arr = np.zeros([len(unpadded_status_arr),len(max(unpadded_status_arr,key = lambda x: len(x)))])\n",
    "# padded_duration_arr = np.zeros([len(unpadded_duration_arr),len(max(unpadded_duration_arr,key = lambda x: len(x)))])\n",
    "# for i,j in enumerate(unpadded_status_arr):\n",
    "#     padded_status_arr[i][0:len(j)] = j\n",
    "#     padded_duration_arr[i][0:len(j)] = unpadded_duration_arr[i]\n",
    "\n",
    "# # standard scale for the model to learn faster\n",
    "# padded_X_seq = wba124.preprocess(padded_status_arr, padded_duration_arr)\n",
    "# for i in range(padded_X_seq.shape[1]):\n",
    "#     padded_X_seq[:, i, :] = sc.fit_transform(padded_X_seq[:, i, :])\n",
    "\n",
    "# #train_val_test split\n",
    "# val_percentage = 0.2\n",
    "# test_percentage = 0.1\n",
    "\n",
    "# test_index = int(len(padded_X_seq) * (1-test_percentage))\n",
    "# val_index = int(len(padded_X_seq) * (1- val_percentage - test_percentage))\n",
    "\n",
    "# X_train_seq, X_val_seq, X_test_seq = padded_X_seq[:val_index], padded_X_seq[val_index:test_index], padded_X_seq[test_index:]\n",
    "# y_train_seq, y_val_seq, y_test_seq = wba124.major_down_arr[:val_index], wba124.major_down_arr[val_index:test_index], wba124.major_down_arr[test_index:]\n",
    "\n",
    "# X_train_seq = X_train_seq.reshape(X_train_seq.shape[0], X_train_seq.shape[1], 2)\n",
    "# X_val_seq = X_val_seq.reshape(X_val_seq.shape[0], X_val_seq.shape[1], 2)\n",
    "# X_test_seq = X_test_seq.reshape(X_test_seq.shape[0], X_test_seq.shape[1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7c957ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Training took too long, one epoch 30 MINS!!! ##\n",
    "# seq_result = {}\n",
    "\n",
    "# class_weights = class_weight.compute_class_weight('balanced',\n",
    "#                                              np.unique(y_train_seq),\n",
    "#                                              y_train_seq)\n",
    "# class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# #need to reinitialize the model because x_train_seq changes in shape\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(128, input_shape=(X_train_seq.shape[1:]), return_sequences=True))\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "# model.add(LSTM(128, input_shape=(X_train_seq.shape[1:])))\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "# model.add(Dense(32, activation = 'relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(Dense(2, activation = 'softmax'))\n",
    "\n",
    "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# history = model.fit(np.array(X_train_seq), np.array(y_train_seq), \n",
    "#                 batch_size=BATCH_SIZE, epochs=EPOCHS, \n",
    "#                 validation_data=(np.array(X_val_seq), np.array(y_val_seq)),\n",
    "#                 class_weight = class_weights_dict)\n",
    "\n",
    "# evaluate = model.evaluate(np.array(X_test_seq), np.array(y_test_seq)) #loss, mse\n",
    "\n",
    "# seq_result[hour] = evaluate\n",
    "# end = datetime.now()\n",
    "# time = end - start\n",
    "# print(f\"Training took a total of {time.seconds} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674cbf9b",
   "metadata": {},
   "source": [
    "# Training with only important state name and duration\n",
    "## Padding the sequence with max length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e95b7ee6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training by looking back 12 hours of important STATE NAME data\n",
      "Epoch 1/100\n",
      "184/184 [==============================] - 311s 2s/step - loss: 0.7231 - accuracy: 0.6478 - val_loss: 0.7749 - val_accuracy: 0.6466\n",
      "Epoch 2/100\n",
      "184/184 [==============================] - 306s 2s/step - loss: 0.6856 - accuracy: 0.7147 - val_loss: 0.7434 - val_accuracy: 0.6436\n",
      "Epoch 3/100\n",
      "184/184 [==============================] - 308s 2s/step - loss: 0.6854 - accuracy: 0.7670 - val_loss: 0.7929 - val_accuracy: 0.6442\n",
      "Epoch 4/100\n",
      "184/184 [==============================] - 302s 2s/step - loss: 0.6791 - accuracy: 0.7599 - val_loss: 0.7272 - val_accuracy: 0.6472\n",
      "Epoch 5/100\n",
      "184/184 [==============================] - 305s 2s/step - loss: 0.6819 - accuracy: 0.7747 - val_loss: 0.7005 - val_accuracy: 0.6472\n",
      "Epoch 6/100\n",
      "184/184 [==============================] - 305s 2s/step - loss: 0.6775 - accuracy: 0.7858 - val_loss: 0.6901 - val_accuracy: 0.6478\n",
      "Epoch 7/100\n",
      "184/184 [==============================] - 304s 2s/step - loss: 0.6778 - accuracy: 0.7924 - val_loss: 0.7236 - val_accuracy: 0.6424\n",
      "Epoch 8/100\n",
      "184/184 [==============================] - 310s 2s/step - loss: 0.6755 - accuracy: 0.7912 - val_loss: 0.7798 - val_accuracy: 0.6442\n",
      "Epoch 9/100\n",
      "184/184 [==============================] - 306s 2s/step - loss: 0.6777 - accuracy: 0.7914 - val_loss: 0.7351 - val_accuracy: 0.6424\n",
      "Epoch 10/100\n",
      "184/184 [==============================] - 306s 2s/step - loss: 0.6779 - accuracy: 0.7919 - val_loss: 0.7181 - val_accuracy: 0.6412\n",
      "Epoch 11/100\n",
      "184/184 [==============================] - 303s 2s/step - loss: 0.6786 - accuracy: 0.7878 - val_loss: 0.7162 - val_accuracy: 0.6412\n",
      "Epoch 12/100\n",
      "184/184 [==============================] - 302s 2s/step - loss: 0.6769 - accuracy: 0.7921 - val_loss: 0.7514 - val_accuracy: 0.6424\n",
      "Epoch 13/100\n",
      "184/184 [==============================] - 303s 2s/step - loss: 0.6771 - accuracy: 0.7796 - val_loss: 0.7612 - val_accuracy: 0.6418\n",
      "Epoch 14/100\n",
      "184/184 [==============================] - 310s 2s/step - loss: 0.6747 - accuracy: 0.7941 - val_loss: 0.8133 - val_accuracy: 0.6454\n",
      "Epoch 15/100\n",
      "184/184 [==============================] - 315s 2s/step - loss: 0.6757 - accuracy: 0.7921 - val_loss: 0.7874 - val_accuracy: 0.6460\n",
      "Epoch 16/100\n",
      "184/184 [==============================] - 315s 2s/step - loss: 0.6755 - accuracy: 0.7917 - val_loss: 0.7184 - val_accuracy: 0.6466\n",
      "Epoch 17/100\n",
      "184/184 [==============================] - 321s 2s/step - loss: 0.6774 - accuracy: 0.7946 - val_loss: 0.7233 - val_accuracy: 0.6454\n",
      "Epoch 18/100\n",
      "184/184 [==============================] - 319s 2s/step - loss: 0.6746 - accuracy: 0.7939 - val_loss: 0.7481 - val_accuracy: 0.6448\n",
      "Epoch 19/100\n",
      "184/184 [==============================] - 319s 2s/step - loss: 0.6778 - accuracy: 0.7921 - val_loss: 0.7066 - val_accuracy: 0.6460\n",
      "Epoch 20/100\n",
      "184/184 [==============================] - 318s 2s/step - loss: 0.6753 - accuracy: 0.7893 - val_loss: 0.7105 - val_accuracy: 0.6395\n",
      "Epoch 21/100\n",
      "184/184 [==============================] - 317s 2s/step - loss: 0.6760 - accuracy: 0.7905 - val_loss: 0.7149 - val_accuracy: 0.6460\n",
      "Epoch 22/100\n",
      "184/184 [==============================] - 317s 2s/step - loss: 0.6785 - accuracy: 0.7914 - val_loss: 0.7548 - val_accuracy: 0.6466\n",
      "Epoch 23/100\n",
      "184/184 [==============================] - 316s 2s/step - loss: 0.6753 - accuracy: 0.7929 - val_loss: 0.7534 - val_accuracy: 0.6460\n",
      "Epoch 24/100\n",
      "184/184 [==============================] - 320s 2s/step - loss: 0.6748 - accuracy: 0.7926 - val_loss: 0.7213 - val_accuracy: 0.6466\n",
      "Epoch 25/100\n",
      "184/184 [==============================] - 320s 2s/step - loss: 0.6769 - accuracy: 0.7936 - val_loss: 0.7397 - val_accuracy: 0.6478\n",
      "Epoch 26/100\n",
      "184/184 [==============================] - 316s 2s/step - loss: 0.6769 - accuracy: 0.7917 - val_loss: 0.7261 - val_accuracy: 0.6466\n",
      "Epoch 27/100\n",
      "184/184 [==============================] - 315s 2s/step - loss: 0.6785 - accuracy: 0.7979 - val_loss: 0.7102 - val_accuracy: 0.6466\n",
      "Epoch 28/100\n",
      "184/184 [==============================] - 319s 2s/step - loss: 0.6782 - accuracy: 0.7926 - val_loss: 0.7370 - val_accuracy: 0.6454\n",
      "Epoch 29/100\n",
      "184/184 [==============================] - 317s 2s/step - loss: 0.6776 - accuracy: 0.7929 - val_loss: 0.7422 - val_accuracy: 0.6460\n",
      "Epoch 30/100\n",
      "184/184 [==============================] - 319s 2s/step - loss: 0.6758 - accuracy: 0.7921 - val_loss: 0.7173 - val_accuracy: 0.6466\n",
      "Epoch 31/100\n",
      "184/184 [==============================] - 321s 2s/step - loss: 0.6765 - accuracy: 0.7936 - val_loss: 0.7232 - val_accuracy: 0.6466\n",
      "Epoch 32/100\n",
      "184/184 [==============================] - 321s 2s/step - loss: 0.6754 - accuracy: 0.7926 - val_loss: 0.7269 - val_accuracy: 0.6383\n",
      "Epoch 33/100\n",
      "184/184 [==============================] - 318s 2s/step - loss: 0.6734 - accuracy: 0.7910 - val_loss: 0.7817 - val_accuracy: 0.6430\n",
      "Epoch 34/100\n",
      "184/184 [==============================] - 318s 2s/step - loss: 0.6775 - accuracy: 0.7931 - val_loss: 0.7323 - val_accuracy: 0.6436\n",
      "Epoch 35/100\n",
      "184/184 [==============================] - 318s 2s/step - loss: 0.6766 - accuracy: 0.7895 - val_loss: 0.7397 - val_accuracy: 0.6418\n",
      "Epoch 36/100\n",
      "184/184 [==============================] - 320s 2s/step - loss: 0.6752 - accuracy: 0.7914 - val_loss: 0.7257 - val_accuracy: 0.6430\n",
      "Epoch 37/100\n",
      "184/184 [==============================] - 316s 2s/step - loss: 0.6762 - accuracy: 0.7909 - val_loss: 0.7700 - val_accuracy: 0.6442\n",
      "Epoch 38/100\n",
      "184/184 [==============================] - 333s 2s/step - loss: 0.6767 - accuracy: 0.7914 - val_loss: 0.7303 - val_accuracy: 0.6436\n",
      "Epoch 39/100\n",
      "184/184 [==============================] - 315s 2s/step - loss: 0.6773 - accuracy: 0.7914 - val_loss: 0.7159 - val_accuracy: 0.6412\n",
      "Epoch 40/100\n",
      "184/184 [==============================] - 319s 2s/step - loss: 0.6756 - accuracy: 0.7832 - val_loss: 0.7437 - val_accuracy: 0.6228\n",
      "Epoch 41/100\n",
      "184/184 [==============================] - 317s 2s/step - loss: 0.6791 - accuracy: 0.7648 - val_loss: 0.7353 - val_accuracy: 0.6180\n",
      "Epoch 42/100\n",
      "184/184 [==============================] - 316s 2s/step - loss: 0.6809 - accuracy: 0.7640 - val_loss: 0.7279 - val_accuracy: 0.6198\n",
      "Epoch 43/100\n",
      "184/184 [==============================] - 319s 2s/step - loss: 0.6772 - accuracy: 0.7875 - val_loss: 0.7395 - val_accuracy: 0.6472\n",
      "Epoch 44/100\n",
      "184/184 [==============================] - 315s 2s/step - loss: 0.6725 - accuracy: 0.8001 - val_loss: 0.7453 - val_accuracy: 0.6478\n",
      "Epoch 45/100\n",
      "184/184 [==============================] - 318s 2s/step - loss: 0.6745 - accuracy: 0.7994 - val_loss: 0.7290 - val_accuracy: 0.6454\n",
      "Epoch 46/100\n",
      "184/184 [==============================] - 314s 2s/step - loss: 0.6742 - accuracy: 0.7968 - val_loss: 0.7212 - val_accuracy: 0.6472\n",
      "Epoch 47/100\n",
      "184/184 [==============================] - 312s 2s/step - loss: 0.6725 - accuracy: 0.7982 - val_loss: 0.7317 - val_accuracy: 0.6484\n",
      "Epoch 48/100\n",
      "184/184 [==============================] - 315s 2s/step - loss: 0.6751 - accuracy: 0.7968 - val_loss: 0.7145 - val_accuracy: 0.6466\n",
      "Epoch 49/100\n",
      "184/184 [==============================] - 318s 2s/step - loss: 0.6766 - accuracy: 0.7980 - val_loss: 0.7043 - val_accuracy: 0.6466\n",
      "Epoch 50/100\n",
      "184/184 [==============================] - 318s 2s/step - loss: 0.6760 - accuracy: 0.7967 - val_loss: 0.7235 - val_accuracy: 0.6472\n",
      "Epoch 51/100\n",
      "184/184 [==============================] - 318s 2s/step - loss: 0.6746 - accuracy: 0.7972 - val_loss: 0.7067 - val_accuracy: 0.6460\n",
      "Epoch 52/100\n",
      "184/184 [==============================] - 316s 2s/step - loss: 0.6762 - accuracy: 0.8009 - val_loss: 0.7208 - val_accuracy: 0.6657\n",
      "Epoch 53/100\n",
      "184/184 [==============================] - 317s 2s/step - loss: 0.6808 - accuracy: 0.8455 - val_loss: 0.6418 - val_accuracy: 0.8391\n",
      "Epoch 54/100\n",
      "184/184 [==============================] - 317s 2s/step - loss: 0.6982 - accuracy: 0.8833 - val_loss: 0.6631 - val_accuracy: 0.8391\n",
      "Epoch 55/100\n",
      "184/184 [==============================] - 317s 2s/step - loss: 0.6939 - accuracy: 0.9033 - val_loss: 0.6757 - val_accuracy: 0.8391\n",
      "Epoch 56/100\n",
      "184/184 [==============================] - 314s 2s/step - loss: 0.6941 - accuracy: 0.9017 - val_loss: 0.6834 - val_accuracy: 0.8391\n",
      "Epoch 57/100\n",
      "184/184 [==============================] - 318s 2s/step - loss: 0.6922 - accuracy: 0.8025 - val_loss: 0.7056 - val_accuracy: 0.1609\n",
      "Epoch 58/100\n",
      "184/184 [==============================] - 316s 2s/step - loss: 0.6924 - accuracy: 0.6349 - val_loss: 0.6892 - val_accuracy: 0.8391\n",
      "Epoch 59/100\n",
      "184/184 [==============================] - 315s 2s/step - loss: 0.6918 - accuracy: 0.8650 - val_loss: 0.6918 - val_accuracy: 0.8391\n",
      "Epoch 60/100\n",
      "184/184 [==============================] - 314s 2s/step - loss: 0.6923 - accuracy: 0.7858 - val_loss: 0.6935 - val_accuracy: 0.1609\n",
      "Epoch 61/100\n",
      "184/184 [==============================] - 317s 2s/step - loss: 0.6931 - accuracy: 0.3755 - val_loss: 0.6965 - val_accuracy: 0.1609\n",
      "Epoch 62/100\n",
      "184/184 [==============================] - 318s 2s/step - loss: 0.6924 - accuracy: 0.4361 - val_loss: 0.6959 - val_accuracy: 0.1609\n",
      "Epoch 63/100\n",
      "184/184 [==============================] - 317s 2s/step - loss: 0.6922 - accuracy: 0.5400 - val_loss: 0.6921 - val_accuracy: 0.8391\n",
      "Epoch 64/100\n",
      "184/184 [==============================] - 317s 2s/step - loss: 0.6923 - accuracy: 0.5823 - val_loss: 0.6907 - val_accuracy: 0.8391\n",
      "Epoch 65/100\n",
      "184/184 [==============================] - 323s 2s/step - loss: 0.6920 - accuracy: 0.2260 - val_loss: 0.6942 - val_accuracy: 0.1609\n",
      "Epoch 66/100\n",
      "184/184 [==============================] - 323s 2s/step - loss: 0.6921 - accuracy: 0.4775 - val_loss: 0.6818 - val_accuracy: 0.8391\n",
      "Epoch 67/100\n",
      "184/184 [==============================] - 318s 2s/step - loss: 0.6928 - accuracy: 0.5988 - val_loss: 0.6925 - val_accuracy: 0.8391\n",
      "Epoch 68/100\n",
      "184/184 [==============================] - 318s 2s/step - loss: 0.6923 - accuracy: 0.1771 - val_loss: 0.6923 - val_accuracy: 0.8391\n",
      "Epoch 69/100\n",
      "184/184 [==============================] - 319s 2s/step - loss: 0.6919 - accuracy: 0.6858 - val_loss: 0.6911 - val_accuracy: 0.8391\n",
      "Epoch 70/100\n",
      "184/184 [==============================] - 316s 2s/step - loss: 0.6923 - accuracy: 0.8837 - val_loss: 0.6899 - val_accuracy: 0.8391\n",
      "Epoch 71/100\n",
      "184/184 [==============================] - 320s 2s/step - loss: 0.6921 - accuracy: 0.2778 - val_loss: 0.6914 - val_accuracy: 0.8391\n",
      "Epoch 72/100\n",
      "184/184 [==============================] - 338s 2s/step - loss: 0.6922 - accuracy: 0.6165 - val_loss: 0.6899 - val_accuracy: 0.8391\n",
      "Epoch 73/100\n",
      "184/184 [==============================] - 396s 2s/step - loss: 0.6921 - accuracy: 0.8888 - val_loss: 0.6919 - val_accuracy: 0.8391\n",
      "Epoch 74/100\n",
      "184/184 [==============================] - 366s 2s/step - loss: 0.6921 - accuracy: 0.6916 - val_loss: 0.6929 - val_accuracy: 0.8391\n",
      "Epoch 75/100\n",
      "184/184 [==============================] - 324s 2s/step - loss: 0.6920 - accuracy: 0.5857 - val_loss: 0.6910 - val_accuracy: 0.8391\n",
      "Epoch 76/100\n",
      "184/184 [==============================] - 318s 2s/step - loss: 0.6937 - accuracy: 0.4528 - val_loss: 0.6921 - val_accuracy: 0.8391\n",
      "Epoch 77/100\n",
      "184/184 [==============================] - 320s 2s/step - loss: 0.6923 - accuracy: 0.8316 - val_loss: 0.6943 - val_accuracy: 0.1609\n",
      "Epoch 78/100\n",
      "184/184 [==============================] - 322s 2s/step - loss: 0.6922 - accuracy: 0.5908 - val_loss: 0.6974 - val_accuracy: 0.1609\n",
      "Epoch 79/100\n",
      "184/184 [==============================] - 324s 2s/step - loss: 0.6922 - accuracy: 0.1081 - val_loss: 0.6945 - val_accuracy: 0.1609\n",
      "Epoch 80/100\n",
      "184/184 [==============================] - 326s 2s/step - loss: 0.6921 - accuracy: 0.2371 - val_loss: 0.6936 - val_accuracy: 0.1609\n",
      "Epoch 81/100\n",
      "184/184 [==============================] - 323s 2s/step - loss: 0.6921 - accuracy: 0.7655 - val_loss: 0.6923 - val_accuracy: 0.8391\n",
      "Epoch 82/100\n",
      "184/184 [==============================] - 321s 2s/step - loss: 0.6920 - accuracy: 0.8290 - val_loss: 0.6947 - val_accuracy: 0.1609\n",
      "Epoch 83/100\n",
      "184/184 [==============================] - 320s 2s/step - loss: 0.6921 - accuracy: 0.1449 - val_loss: 0.6939 - val_accuracy: 0.1609\n",
      "Epoch 84/100\n",
      "184/184 [==============================] - 322s 2s/step - loss: 0.6921 - accuracy: 0.2130 - val_loss: 0.6950 - val_accuracy: 0.1609\n",
      "Epoch 85/100\n",
      "184/184 [==============================] - 324s 2s/step - loss: 0.6921 - accuracy: 0.7325 - val_loss: 0.6943 - val_accuracy: 0.1609\n",
      "Epoch 86/100\n",
      "184/184 [==============================] - 324s 2s/step - loss: 0.6920 - accuracy: 0.5879 - val_loss: 0.6940 - val_accuracy: 0.1609\n",
      "Epoch 87/100\n",
      "184/184 [==============================] - 321s 2s/step - loss: 0.6924 - accuracy: 0.6240 - val_loss: 0.6923 - val_accuracy: 0.8391\n",
      "Epoch 88/100\n",
      "184/184 [==============================] - 326s 2s/step - loss: 0.6922 - accuracy: 0.6008 - val_loss: 0.6943 - val_accuracy: 0.1609\n",
      "Epoch 89/100\n",
      "184/184 [==============================] - 324s 2s/step - loss: 0.6921 - accuracy: 0.2253 - val_loss: 0.6949 - val_accuracy: 0.1609\n",
      "Epoch 90/100\n",
      "184/184 [==============================] - 320s 2s/step - loss: 0.6922 - accuracy: 0.8135 - val_loss: 0.6941 - val_accuracy: 0.1609\n",
      "Epoch 91/100\n",
      "184/184 [==============================] - 318s 2s/step - loss: 0.6920 - accuracy: 0.1042 - val_loss: 0.6938 - val_accuracy: 0.1609\n",
      "Epoch 92/100\n",
      "184/184 [==============================] - 323s 2s/step - loss: 0.6919 - accuracy: 0.7718 - val_loss: 0.6934 - val_accuracy: 0.1609\n",
      "Epoch 93/100\n",
      "184/184 [==============================] - 322s 2s/step - loss: 0.6928 - accuracy: 0.7723 - val_loss: 0.6941 - val_accuracy: 0.1609\n",
      "Epoch 94/100\n",
      "184/184 [==============================] - 322s 2s/step - loss: 0.6925 - accuracy: 0.4327 - val_loss: 0.6907 - val_accuracy: 0.8391\n",
      "Epoch 95/100\n",
      "184/184 [==============================] - 323s 2s/step - loss: 0.6926 - accuracy: 0.2267 - val_loss: 0.6971 - val_accuracy: 0.1609\n",
      "Epoch 96/100\n",
      "184/184 [==============================] - 323s 2s/step - loss: 0.6922 - accuracy: 0.1303 - val_loss: 0.6954 - val_accuracy: 0.1609\n",
      "Epoch 97/100\n",
      "184/184 [==============================] - 325s 2s/step - loss: 0.6922 - accuracy: 0.7142 - val_loss: 0.6948 - val_accuracy: 0.1609\n",
      "Epoch 98/100\n",
      "184/184 [==============================] - 325s 2s/step - loss: 0.6926 - accuracy: 0.3822 - val_loss: 0.6925 - val_accuracy: 0.8391\n",
      "Epoch 99/100\n",
      "184/184 [==============================] - 320s 2s/step - loss: 0.6929 - accuracy: 0.3389 - val_loss: 0.6991 - val_accuracy: 0.1609\n",
      "Epoch 100/100\n",
      "184/184 [==============================] - 317s 2s/step - loss: 0.6926 - accuracy: 0.1705 - val_loss: 0.6958 - val_accuracy: 0.1609\n",
      "27/27 [==============================] - 10s 361ms/step - loss: 0.6965 - accuracy: 0.0751\n",
      "Training took a total of 32005 seconds\n"
     ]
    }
   ],
   "source": [
    "seq_result = {}\n",
    "# lookback = [24,48,72,96]\n",
    "\n",
    "# for hour in lookback:\n",
    "hour = 12\n",
    "start = datetime.now()\n",
    "print(f\"Training by looking back {hour} hours of important STATE NAME data\")\n",
    "wba124 = status_LSTM(\"WBA124\", hour, 3)\n",
    "\n",
    "# pad the alarm to train on LSTM\n",
    "unpadded_status_arr = wba124.encoded_impt_state_seq\n",
    "unpadded_duration_arr = wba124.impt_duration_seq\n",
    "\n",
    "padded_status_arr = np.zeros([len(unpadded_status_arr),len(max(unpadded_status_arr,key = lambda x: len(x)))])\n",
    "padded_duration_arr = np.zeros([len(unpadded_duration_arr),len(max(unpadded_duration_arr,key = lambda x: len(x)))])\n",
    "for i,j in enumerate(unpadded_status_arr):\n",
    "    padded_status_arr[i][0:len(j)] = j\n",
    "    padded_duration_arr[i][0:len(j)] = unpadded_duration_arr[i]\n",
    "\n",
    "# standard scale for the model to learn faster\n",
    "padded_X_seq = wba124.preprocess(padded_status_arr, padded_duration_arr)\n",
    "for i in range(padded_X_seq.shape[1]):\n",
    "    padded_X_seq[:, i, :] = sc.fit_transform(padded_X_seq[:, i, :])\n",
    "\n",
    "#train_val_test split\n",
    "val_percentage = 0.2\n",
    "test_percentage = 0.1\n",
    "\n",
    "test_index = int(len(padded_X_seq) * (1-test_percentage))\n",
    "val_index = int(len(padded_X_seq) * (1- val_percentage - test_percentage))\n",
    "\n",
    "X_train_seq, X_val_seq, X_test_seq = padded_X_seq[:val_index], padded_X_seq[val_index:test_index], padded_X_seq[test_index:]\n",
    "y_train_seq, y_val_seq, y_test_seq = wba124.major_down_arr[:val_index], wba124.major_down_arr[val_index:test_index], wba124.major_down_arr[test_index:]\n",
    "\n",
    "X_train_seq = X_train_seq.reshape(X_train_seq.shape[0], X_train_seq.shape[1], 2)\n",
    "X_val_seq = X_val_seq.reshape(X_val_seq.shape[0], X_val_seq.shape[1], 2)\n",
    "X_test_seq = X_test_seq.reshape(X_test_seq.shape[0], X_test_seq.shape[1], 2)\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                             np.unique(y_train_seq),\n",
    "                                             y_train_seq)\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "#need to reinitialize the model because x_train_seq changes in shape\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X_train_seq.shape[1:]), return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(LSTM(256, input_shape=(X_train_seq.shape[1:])))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(32, activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(np.array(X_train_seq), np.array(y_train_seq), \n",
    "                batch_size=BATCH_SIZE, epochs=EPOCHS, \n",
    "                validation_data=(np.array(X_val_seq), np.array(y_val_seq)),\n",
    "                class_weight = class_weights_dict)\n",
    "\n",
    "evaluate = model.evaluate(np.array(X_test_seq), np.array(y_test_seq)) #loss, mse\n",
    "\n",
    "seq_result[hour] = evaluate\n",
    "end = datetime.now()\n",
    "time = end - start\n",
    "print(f\"Training took a total of {time.seconds} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d6bb9b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{12: [0.6964600682258606, 0.07508939504623413]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "647691a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5872, 127, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_seq.shape # this is padded with the max length of all sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d34ca8",
   "metadata": {},
   "source": [
    "# Training with only important state name and duration\n",
    "## Padding the sequence with the average length\n",
    "#### https://towardsdatascience.com/using-tensorflow-ragged-tensors-2af07849a7bd\n",
    "#### Apparently can also greatly help boost accracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62f498d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training by looking back 12 hours of important STATE NAME data\n",
      "Epoch 1/100\n",
      "184/184 [==============================] - 7s 24ms/step - loss: 0.6573 - accuracy: 0.6008 - val_loss: 0.6721 - val_accuracy: 0.5317\n",
      "Epoch 2/100\n",
      "184/184 [==============================] - 4s 20ms/step - loss: 0.6587 - accuracy: 0.6308 - val_loss: 0.7247 - val_accuracy: 0.5287\n",
      "Epoch 3/100\n",
      "184/184 [==============================] - 4s 20ms/step - loss: 0.6578 - accuracy: 0.6294 - val_loss: 0.7386 - val_accuracy: 0.5179\n",
      "Epoch 4/100\n",
      "184/184 [==============================] - 4s 20ms/step - loss: 0.6577 - accuracy: 0.6369 - val_loss: 0.6469 - val_accuracy: 0.5329\n",
      "Epoch 5/100\n",
      "184/184 [==============================] - 4s 20ms/step - loss: 0.6568 - accuracy: 0.6388 - val_loss: 0.6866 - val_accuracy: 0.5400\n",
      "Epoch 6/100\n",
      "184/184 [==============================] - 4s 20ms/step - loss: 0.6561 - accuracy: 0.6448 - val_loss: 0.7881 - val_accuracy: 0.4671\n",
      "Epoch 7/100\n",
      "184/184 [==============================] - 4s 20ms/step - loss: 0.6570 - accuracy: 0.6284 - val_loss: 0.7811 - val_accuracy: 0.5048\n",
      "Epoch 8/100\n",
      "184/184 [==============================] - 4s 20ms/step - loss: 0.6562 - accuracy: 0.6518 - val_loss: 0.6804 - val_accuracy: 0.5585\n",
      "Epoch 9/100\n",
      "184/184 [==============================] - 4s 20ms/step - loss: 0.6543 - accuracy: 0.6571 - val_loss: 0.7157 - val_accuracy: 0.5281\n",
      "Epoch 10/100\n",
      "184/184 [==============================] - 4s 20ms/step - loss: 0.6501 - accuracy: 0.6567 - val_loss: 0.7952 - val_accuracy: 0.5018\n",
      "Epoch 11/100\n",
      "184/184 [==============================] - 4s 20ms/step - loss: 0.6568 - accuracy: 0.6526 - val_loss: 0.6695 - val_accuracy: 0.5914\n",
      "Epoch 12/100\n",
      "184/184 [==============================] - 4s 20ms/step - loss: 0.6511 - accuracy: 0.6705 - val_loss: 0.6972 - val_accuracy: 0.5388\n",
      "Epoch 13/100\n",
      "184/184 [==============================] - 4s 20ms/step - loss: 0.6519 - accuracy: 0.6617 - val_loss: 0.6488 - val_accuracy: 0.5914\n",
      "Epoch 14/100\n",
      "184/184 [==============================] - 4s 20ms/step - loss: 0.6519 - accuracy: 0.6405 - val_loss: 0.7458 - val_accuracy: 0.5299\n",
      "Epoch 15/100\n",
      "184/184 [==============================] - 4s 20ms/step - loss: 0.6524 - accuracy: 0.6683 - val_loss: 0.6917 - val_accuracy: 0.5376\n",
      "Epoch 16/100\n",
      "184/184 [==============================] - 4s 20ms/step - loss: 0.6526 - accuracy: 0.6478 - val_loss: 0.6309 - val_accuracy: 0.5944\n",
      "Epoch 17/100\n",
      "184/184 [==============================] - 4s 20ms/step - loss: 0.6529 - accuracy: 0.6547 - val_loss: 0.6912 - val_accuracy: 0.5341\n",
      "Epoch 18/100\n",
      "184/184 [==============================] - 4s 20ms/step - loss: 0.6462 - accuracy: 0.6514 - val_loss: 0.7621 - val_accuracy: 0.5000\n",
      "Epoch 19/100\n",
      "184/184 [==============================] - 4s 20ms/step - loss: 0.6546 - accuracy: 0.6528 - val_loss: 0.7202 - val_accuracy: 0.5364\n",
      "Epoch 20/100\n",
      "184/184 [==============================] - 4s 20ms/step - loss: 0.6488 - accuracy: 0.6629 - val_loss: 0.8265 - val_accuracy: 0.5024\n",
      "Epoch 21/100\n",
      "184/184 [==============================] - 4s 20ms/step - loss: 0.6490 - accuracy: 0.6617 - val_loss: 0.7111 - val_accuracy: 0.5442\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.6558 - accuracy: 0.6348\n",
      "Training took a total of 214 seconds\n",
      "Training by looking back 24 hours of important STATE NAME data\n",
      "Epoch 1/100\n",
      "184/184 [==============================] - 10s 41ms/step - loss: 0.6737 - accuracy: 0.5688 - val_loss: 0.7781 - val_accuracy: 0.4421\n",
      "Epoch 2/100\n",
      "184/184 [==============================] - 7s 37ms/step - loss: 0.6652 - accuracy: 0.5852 - val_loss: 0.7464 - val_accuracy: 0.4528\n",
      "Epoch 3/100\n",
      "184/184 [==============================] - 7s 37ms/step - loss: 0.6682 - accuracy: 0.5116 - val_loss: 0.6935 - val_accuracy: 0.4869\n",
      "Epoch 4/100\n",
      "184/184 [==============================] - 7s 37ms/step - loss: 0.6669 - accuracy: 0.6005 - val_loss: 0.6779 - val_accuracy: 0.5090\n",
      "Epoch 5/100\n",
      "184/184 [==============================] - 7s 37ms/step - loss: 0.6642 - accuracy: 0.5941 - val_loss: 0.7293 - val_accuracy: 0.4886\n",
      "Epoch 6/100\n",
      "184/184 [==============================] - 7s 37ms/step - loss: 0.6615 - accuracy: 0.6118 - val_loss: 0.6325 - val_accuracy: 0.5078\n",
      "Epoch 7/100\n",
      "184/184 [==============================] - 7s 37ms/step - loss: 0.6627 - accuracy: 0.5891 - val_loss: 0.6608 - val_accuracy: 0.5245\n",
      "Epoch 8/100\n",
      "184/184 [==============================] - 7s 37ms/step - loss: 0.6615 - accuracy: 0.5864 - val_loss: 0.7125 - val_accuracy: 0.4594\n",
      "Epoch 9/100\n",
      "184/184 [==============================] - 7s 37ms/step - loss: 0.6573 - accuracy: 0.5994 - val_loss: 0.7237 - val_accuracy: 0.3799\n",
      "Epoch 10/100\n",
      "184/184 [==============================] - 7s 37ms/step - loss: 0.6614 - accuracy: 0.5766 - val_loss: 0.6915 - val_accuracy: 0.5048\n",
      "Epoch 11/100\n",
      "184/184 [==============================] - 7s 37ms/step - loss: 0.6588 - accuracy: 0.6082 - val_loss: 0.7816 - val_accuracy: 0.4211\n",
      "Epoch 12/100\n",
      "184/184 [==============================] - 7s 37ms/step - loss: 0.6630 - accuracy: 0.5925 - val_loss: 0.7667 - val_accuracy: 0.4516\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.6869 - accuracy: 0.5520\n",
      "Training took a total of 191 seconds\n",
      "Training by looking back 48 hours of important STATE NAME data\n",
      "Epoch 1/100\n",
      "183/183 [==============================] - 16s 74ms/step - loss: 0.6807 - accuracy: 0.5789 - val_loss: 0.6709 - val_accuracy: 0.4318\n",
      "Epoch 2/100\n",
      "183/183 [==============================] - 13s 71ms/step - loss: 0.6852 - accuracy: 0.5938 - val_loss: 0.7737 - val_accuracy: 0.3929\n",
      "Epoch 3/100\n",
      "183/183 [==============================] - 13s 71ms/step - loss: 0.6760 - accuracy: 0.5620 - val_loss: 0.7209 - val_accuracy: 0.4252\n",
      "Epoch 4/100\n",
      "183/183 [==============================] - 13s 71ms/step - loss: 0.6693 - accuracy: 0.6032 - val_loss: 0.7198 - val_accuracy: 0.4264\n",
      "Epoch 5/100\n",
      "183/183 [==============================] - 13s 71ms/step - loss: 0.6772 - accuracy: 0.5697 - val_loss: 0.8186 - val_accuracy: 0.2727\n",
      "Epoch 6/100\n",
      "183/183 [==============================] - 13s 71ms/step - loss: 0.6718 - accuracy: 0.5675 - val_loss: 0.6510 - val_accuracy: 0.4707\n",
      "Epoch 7/100\n",
      "183/183 [==============================] - 13s 71ms/step - loss: 0.6725 - accuracy: 0.5795 - val_loss: 0.6780 - val_accuracy: 0.4330\n",
      "Epoch 8/100\n",
      "183/183 [==============================] - 13s 71ms/step - loss: 0.6705 - accuracy: 0.5952 - val_loss: 0.7011 - val_accuracy: 0.4300\n",
      "Epoch 9/100\n",
      "183/183 [==============================] - 13s 71ms/step - loss: 0.6862 - accuracy: 0.5730 - val_loss: 0.6952 - val_accuracy: 0.2297\n",
      "Epoch 10/100\n",
      "183/183 [==============================] - 13s 71ms/step - loss: 0.6809 - accuracy: 0.5357 - val_loss: 0.6966 - val_accuracy: 0.3642\n",
      "Epoch 11/100\n",
      "183/183 [==============================] - 13s 71ms/step - loss: 0.6746 - accuracy: 0.5557 - val_loss: 0.8071 - val_accuracy: 0.2990\n",
      "27/27 [==============================] - 0s 17ms/step - loss: 0.7383 - accuracy: 0.4170\n",
      "Training took a total of 257 seconds\n",
      "Training by looking back 72 hours of important STATE NAME data\n",
      "Epoch 1/100\n",
      "183/183 [==============================] - 23s 111ms/step - loss: 0.6856 - accuracy: 0.5933 - val_loss: 0.7814 - val_accuracy: 0.3419\n",
      "Epoch 2/100\n",
      "183/183 [==============================] - 20s 107ms/step - loss: 0.6819 - accuracy: 0.5490 - val_loss: 0.7339 - val_accuracy: 0.4246\n",
      "Epoch 3/100\n",
      "183/183 [==============================] - 20s 107ms/step - loss: 0.6818 - accuracy: 0.5926 - val_loss: 0.7013 - val_accuracy: 0.4204\n",
      "Epoch 4/100\n",
      "183/183 [==============================] - 20s 107ms/step - loss: 0.6781 - accuracy: 0.6155 - val_loss: 0.7990 - val_accuracy: 0.4012\n",
      "Epoch 5/100\n",
      "183/183 [==============================] - 20s 111ms/step - loss: 0.6789 - accuracy: 0.5757 - val_loss: 0.7402 - val_accuracy: 0.4222\n",
      "Epoch 6/100\n",
      "183/183 [==============================] - 20s 108ms/step - loss: 0.6768 - accuracy: 0.6082 - val_loss: 0.7955 - val_accuracy: 0.3898\n",
      "Epoch 7/100\n",
      "183/183 [==============================] - 20s 111ms/step - loss: 0.6788 - accuracy: 0.5911 - val_loss: 0.7578 - val_accuracy: 0.3641\n",
      "27/27 [==============================] - 1s 30ms/step - loss: 0.6987 - accuracy: 0.5455\n",
      "Training took a total of 253 seconds\n",
      "Training by looking back 96 hours of important STATE NAME data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "183/183 [==============================] - 29s 145ms/step - loss: 0.6942 - accuracy: 0.5405 - val_loss: 0.6265 - val_accuracy: 0.8406\n",
      "Epoch 2/100\n",
      "183/183 [==============================] - 26s 141ms/step - loss: 0.6867 - accuracy: 0.5602 - val_loss: 0.7821 - val_accuracy: 0.3649\n",
      "Epoch 3/100\n",
      "183/183 [==============================] - 26s 142ms/step - loss: 0.6895 - accuracy: 0.5961 - val_loss: 0.7141 - val_accuracy: 0.4386\n",
      "Epoch 4/100\n",
      "183/183 [==============================] - 26s 142ms/step - loss: 0.6836 - accuracy: 0.5972 - val_loss: 0.6407 - val_accuracy: 0.7669\n",
      "Epoch 5/100\n",
      "183/183 [==============================] - 28s 152ms/step - loss: 0.6902 - accuracy: 0.5288 - val_loss: 0.7047 - val_accuracy: 0.4272\n",
      "Epoch 6/100\n",
      "183/183 [==============================] - 30s 162ms/step - loss: 0.6797 - accuracy: 0.6085 - val_loss: 0.7907 - val_accuracy: 0.2331\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.7332 - accuracy: 0.3353\n",
      "Training took a total of 278 seconds\n"
     ]
    }
   ],
   "source": [
    "seq_result = {}\n",
    "\n",
    "lookback = [12,24,48,72,96]\n",
    "for hour in lookback:\n",
    "    start = datetime.now()\n",
    "    print(f\"Training by looking back {hour} hours of important STATE NAME data\")\n",
    "    wba124 = status_LSTM(\"WBA124\", hour, 3)\n",
    "\n",
    "    # pad the alarm to train on LSTM\n",
    "    unpadded_status_arr = wba124.encoded_impt_state_seq\n",
    "    unpadded_duration_arr = wba124.impt_duration_seq\n",
    "\n",
    "    mean_length = int(np.mean([len(x) for x in unpadded_status_arr]))\n",
    "    padded_status_arr = np.zeros([len(unpadded_status_arr), mean_length])\n",
    "    padded_duration_arr = np.zeros([len(unpadded_duration_arr), mean_length])\n",
    "\n",
    "    for i,j in enumerate(unpadded_status_arr):\n",
    "        padded_status_arr[i][0:len(j)] = j[:mean_length]\n",
    "        padded_duration_arr[i][0:len(unpadded_duration_arr[i])] = unpadded_duration_arr[i][:mean_length]\n",
    "\n",
    "    # standard scale for the model to learn faster\n",
    "    padded_X_seq = wba124.preprocess(padded_status_arr, padded_duration_arr)\n",
    "    for i in range(padded_X_seq.shape[1]):\n",
    "        padded_X_seq[:, i, :] = sc.fit_transform(padded_X_seq[:, i, :])\n",
    "\n",
    "    #train_val_test split\n",
    "    val_percentage = 0.2\n",
    "    test_percentage = 0.1\n",
    "\n",
    "    test_index = int(len(padded_X_seq) * (1-test_percentage))\n",
    "    val_index = int(len(padded_X_seq) * (1- val_percentage - test_percentage))\n",
    "\n",
    "    X_train_seq, X_val_seq, X_test_seq = padded_X_seq[:val_index], padded_X_seq[val_index:test_index], padded_X_seq[test_index:]\n",
    "    y_train_seq, y_val_seq, y_test_seq = wba124.major_down_arr[:val_index], wba124.major_down_arr[val_index:test_index], wba124.major_down_arr[test_index:]\n",
    "\n",
    "    X_train_seq = X_train_seq.reshape(X_train_seq.shape[0], X_train_seq.shape[1], 2)\n",
    "    X_val_seq = X_val_seq.reshape(X_val_seq.shape[0], X_val_seq.shape[1], 2)\n",
    "    X_test_seq = X_test_seq.reshape(X_test_seq.shape[0], X_test_seq.shape[1], 2)\n",
    "\n",
    "    class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train_seq),\n",
    "                                                 y_train_seq)\n",
    "    class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "    #need to reinitialize the model because x_train_seq changes in shape\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(256, input_shape=(X_train_seq.shape[1:]), return_sequences=True))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(LSTM(256, input_shape=(X_train_seq.shape[1:])))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(32, activation = 'relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(2, activation = 'softmax'))\n",
    "\n",
    "    callbacks = [EarlyStopping(monitor='val_accuracy', mode='max', patience=5)]\n",
    "\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(np.array(X_train_seq), np.array(y_train_seq), \n",
    "                    batch_size=BATCH_SIZE, epochs=EPOCHS, \n",
    "                    validation_data=(np.array(X_val_seq), np.array(y_val_seq)),\n",
    "                    class_weight = class_weights_dict, callbacks=callbacks)\n",
    "\n",
    "    evaluate = model.evaluate(np.array(X_test_seq), np.array(y_test_seq)) #loss, mse\n",
    "\n",
    "    seq_result[hour, mean_length] = evaluate\n",
    "    end = datetime.now()\n",
    "    time = end - start\n",
    "    print(f\"Training took a total of {time.seconds} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "072062c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(12, 6): [0.6557784676551819, 0.6348448395729065],\n",
       " (24, 12): [0.6868782043457031, 0.5519713163375854],\n",
       " (48, 24): [0.7382815480232239, 0.4169653654098511],\n",
       " (72, 37): [0.6986674070358276, 0.5454545617103577],\n",
       " (96, 49): [0.7332104444503784, 0.3353293538093567]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b72690",
   "metadata": {},
   "source": [
    "# Training with Embedding Layer, multiple inputs\n",
    "\n",
    "### one categorical variable + one numerical variable\n",
    "#### model overfits very badly, introduce some callbacks and Dropout\n",
    "#### TODO: check if the target value is computed correctly, values looks suspicious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99715694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode(statename_seq): # do this the manual way as we are not certain if sklearn LabelEncoder can handle 3D array\n",
    "    all_unique_statename = [set(ele) for ele in statename_seq]\n",
    "    unique_statenames = set()\n",
    "    for ele in all_unique_statename:\n",
    "        unique_statenames |= ele\n",
    "    \n",
    "    enc_label = 1  #start encoding from 1 as we have to pad the sequence with 0\n",
    "    mapping_dict = {}\n",
    "    for ele in unique_statenames:\n",
    "        mapping_dict[ele] = enc_label\n",
    "        enc_label += 1\n",
    "\n",
    "        enc_array = []\n",
    "        \n",
    "    #X_seq is a 3D array\n",
    "    for timestamp in statename_seq:\n",
    "        tmp_arr = []\n",
    "        for ele in timestamp:\n",
    "            tmp_arr.append(mapping_dict[ele])\n",
    "        enc_array.append(np.array(tmp_arr))\n",
    "\n",
    "    return np.array(enc_array), len(unique_statenames)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "297acf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_datapoint(impt_state_seq, impt_duration_seq, major_down_arr):\n",
    "    print(f\"Remove {len([ele for ele in impt_state_seq if len(ele)==0])} rows on WBA124 out of {len(impt_state_seq)} as it has no data\")\n",
    "    bool_arr = [len(ele)==0 for ele in impt_state_seq] #this is to find the index to remove for bot y array and X_seq\n",
    "    idx_remove = np.where(bool_arr)[0]\n",
    "    \n",
    "    major_down_arr = np.delete(np.array(major_down_arr), idx_remove) # remove the corresponding y value as well\n",
    "    impt_state_seq = np.delete(impt_state_seq, idx_remove) #remove rows with no state name\n",
    "    impt_duration_seq = np.delete(impt_duration_seq, idx_remove)\n",
    "    return impt_state_seq, impt_duration_seq, major_down_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db9e05e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomly_select_negative(X_seq, X_dur, major_down_arr, ratio):\n",
    "    bool_arr = [ele==0 for ele in major_down_arr]\n",
    "    major_arr = np.array(major_down_arr)[np.where(bool_arr)[0]]\n",
    "    print(len(major_arr), len(major_down_arr))\n",
    "    negative = X_seq[np.where(bool_arr)[0]]\n",
    "    duration = X_dur[np.where(bool_arr)[0]]\n",
    "    \n",
    "    positive_bool_arr = [ele==1 for ele in major_down_arr]\n",
    "    positive_major_arr = np.array(major_down_arr)[np.where(positive_bool_arr)[0]]\n",
    "    print(len(positive_major_arr))\n",
    "    positive = X_seq[np.where(positive_bool_arr)[0]]\n",
    "    positive_dur = X_dur[np.where(positive_bool_arr)[0]]\n",
    "    \n",
    "    discard, keep, dur_discard, dur_keep, target_discard, target_keep = train_test_split(negative, duration, major_arr, test_size=ratio)\n",
    "    \n",
    "    handpicked = np.concatenate((positive, keep))\n",
    "    dur = np.concatenate((positive_dur, dur_keep))\n",
    "    target = np.concatenate((positive_major_arr, target_keep))\n",
    "    \n",
    "    if len(handpicked) != len(dur) or len(handpicked) != len(target):\n",
    "        raise Exception(\"Length of training inputs are different\")\n",
    "    \n",
    "    return handpicked, dur, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "431ef386",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove 2786 rows on WBA124 out of 8389 as it has no data\n",
      "Remove 2900 rows on WBA124 out of 8381 as it has no data\n",
      "Remove 2215 rows on WBA124 out of 8381 as it has no data\n",
      "Remove 2675 rows on WBA124 out of 8389 as it has no data\n",
      "Remove 2478 rows on WBA124 out of 8389 as it has no data\n",
      "25078 28875\n",
      "3797\n",
      "Epoch 1/300\n",
      "166/166 [==============================] - 9s 25ms/step - loss: 0.6807 - precision: 0.5000 - recall: 0.2897 - accuracy: 0.5691 - val_loss: 0.6747 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_accuracy: 0.5693\n",
      "Epoch 2/300\n",
      "166/166 [==============================] - 3s 19ms/step - loss: 0.6775 - precision: 0.4926 - recall: 0.2177 - accuracy: 0.5663 - val_loss: 0.6770 - val_precision: 0.5012 - val_recall: 0.4698 - val_accuracy: 0.5702\n",
      "Epoch 3/300\n",
      "166/166 [==============================] - 3s 19ms/step - loss: 0.6732 - precision: 0.5198 - recall: 0.2309 - accuracy: 0.5767 - val_loss: 0.6761 - val_precision: 0.5128 - val_recall: 0.4171 - val_accuracy: 0.5783\n",
      "Epoch 4/300\n",
      "166/166 [==============================] - 3s 19ms/step - loss: 0.6726 - precision: 0.5013 - recall: 0.2608 - accuracy: 0.5697 - val_loss: 0.6707 - val_precision: 0.5202 - val_recall: 0.3535 - val_accuracy: 0.5811\n",
      "Epoch 5/300\n",
      "166/166 [==============================] - 3s 19ms/step - loss: 0.6766 - precision: 0.5080 - recall: 0.1528 - accuracy: 0.5712 - val_loss: 0.6747 - val_precision: 0.5119 - val_recall: 0.3315 - val_accuracy: 0.5759\n",
      "Epoch 6/300\n",
      "166/166 [==============================] - 3s 19ms/step - loss: 0.6748 - precision: 0.4974 - recall: 0.2533 - accuracy: 0.5680 - val_loss: 0.6706 - val_precision: 0.5672 - val_recall: 0.1668 - val_accuracy: 0.5863\n",
      "Epoch 7/300\n",
      "166/166 [==============================] - 3s 19ms/step - loss: 0.6737 - precision: 0.5031 - recall: 0.2129 - accuracy: 0.5703 - val_loss: 0.6763 - val_precision: 0.5798 - val_recall: 0.1515 - val_accuracy: 0.5872\n",
      "Epoch 8/300\n",
      "166/166 [==============================] - 3s 19ms/step - loss: 0.6745 - precision: 0.5117 - recall: 0.2011 - accuracy: 0.5731 - val_loss: 0.6714 - val_precision: 0.5115 - val_recall: 0.3161 - val_accuracy: 0.5754\n",
      "Epoch 9/300\n",
      "166/166 [==============================] - 3s 19ms/step - loss: 0.6739 - precision: 0.4996 - recall: 0.2796 - accuracy: 0.5689 - val_loss: 0.6727 - val_precision: 0.5208 - val_recall: 0.3030 - val_accuracy: 0.5797\n",
      "Epoch 10/300\n",
      "166/166 [==============================] - 3s 19ms/step - loss: 0.6750 - precision: 0.5173 - recall: 0.2234 - accuracy: 0.5756 - val_loss: 0.6723 - val_precision: 0.5634 - val_recall: 0.1658 - val_accuracy: 0.5853\n",
      "Epoch 11/300\n",
      "166/166 [==============================] - 3s 19ms/step - loss: 0.6735 - precision: 0.5116 - recall: 0.2028 - accuracy: 0.5731 - val_loss: 0.6716 - val_precision: 0.5078 - val_recall: 0.3227 - val_accuracy: 0.5735\n",
      "Epoch 12/300\n",
      "166/166 [==============================] - 3s 19ms/step - loss: 0.6736 - precision: 0.5112 - recall: 0.1905 - accuracy: 0.5727 - val_loss: 0.6733 - val_precision: 0.5185 - val_recall: 0.0154 - val_accuracy: 0.5697\n",
      "Epoch 13/300\n",
      "166/166 [==============================] - 3s 19ms/step - loss: 0.6792 - precision: 0.4781 - recall: 0.1817 - accuracy: 0.5619 - val_loss: 0.6770 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_accuracy: 0.5693\n",
      "Epoch 14/300\n",
      "166/166 [==============================] - 3s 19ms/step - loss: 0.6732 - precision: 0.4972 - recall: 0.1150 - accuracy: 0.5686 - val_loss: 0.6761 - val_precision: 0.5714 - val_recall: 0.0220 - val_accuracy: 0.5716\n",
      "Epoch 15/300\n",
      "166/166 [==============================] - 3s 19ms/step - loss: 0.6734 - precision: 0.5113 - recall: 0.1984 - accuracy: 0.5729 - val_loss: 0.6758 - val_precision: 0.5292 - val_recall: 0.2689 - val_accuracy: 0.5820\n",
      "Epoch 16/300\n",
      "166/166 [==============================] - 3s 19ms/step - loss: 0.6709 - precision: 0.5116 - recall: 0.2616 - accuracy: 0.5742 - val_loss: 0.6770 - val_precision: 0.5294 - val_recall: 0.0296 - val_accuracy: 0.5707\n",
      "Epoch 17/300\n",
      "166/166 [==============================] - 3s 19ms/step - loss: 0.6706 - precision: 0.5380 - recall: 0.1585 - accuracy: 0.5788 - val_loss: 0.6717 - val_precision: 0.5243 - val_recall: 0.0593 - val_accuracy: 0.5716\n",
      "Epoch 18/300\n",
      "166/166 [==============================] - 3s 19ms/step - loss: 0.6684 - precision: 0.5126 - recall: 0.2414 - accuracy: 0.5742 - val_loss: 0.6711 - val_precision: 0.5771 - val_recall: 0.1109 - val_accuracy: 0.5820\n",
      "Epoch 19/300\n",
      "166/166 [==============================] - 3s 19ms/step - loss: 0.6673 - precision: 0.5287 - recall: 0.1901 - accuracy: 0.5780 - val_loss: 0.6722 - val_precision: 0.5185 - val_recall: 0.3699 - val_accuracy: 0.5806\n",
      "Epoch 20/300\n",
      "166/166 [==============================] - 3s 19ms/step - loss: 0.6662 - precision: 0.5226 - recall: 0.3051 - accuracy: 0.5805 - val_loss: 0.6802 - val_precision: 0.4442 - val_recall: 0.2711 - val_accuracy: 0.5400\n",
      "Epoch 21/300\n",
      "166/166 [==============================] - 3s 19ms/step - loss: 0.6661 - precision: 0.5307 - recall: 0.3147 - accuracy: 0.5848 - val_loss: 0.6757 - val_precision: 0.4603 - val_recall: 0.1844 - val_accuracy: 0.5556\n",
      "Epoch 22/300\n",
      "166/166 [==============================] - 3s 19ms/step - loss: 0.6653 - precision: 0.5392 - recall: 0.3442 - accuracy: 0.5907 - val_loss: 0.6732 - val_precision: 0.5164 - val_recall: 0.3447 - val_accuracy: 0.5787\n",
      "Epoch 23/300\n",
      "166/166 [==============================] - 3s 19ms/step - loss: 0.6678 - precision: 0.5202 - recall: 0.2656 - accuracy: 0.5780 - val_loss: 0.6700 - val_precision: 0.5203 - val_recall: 0.0703 - val_accuracy: 0.5716\n",
      "Epoch 24/300\n",
      "166/166 [==============================] - 3s 19ms/step - loss: 0.6692 - precision: 0.5347 - recall: 0.2300 - accuracy: 0.5820 - val_loss: 0.6788 - val_precision: 0.5469 - val_recall: 0.2239 - val_accuracy: 0.5858\n",
      "Epoch 25/300\n",
      "166/166 [==============================] - 3s 19ms/step - loss: 0.6688 - precision: 0.5385 - recall: 0.2730 - accuracy: 0.5860 - val_loss: 0.6741 - val_precision: 0.5051 - val_recall: 0.4336 - val_accuracy: 0.5730\n",
      "Epoch 26/300\n",
      "166/166 [==============================] - 3s 19ms/step - loss: 0.6675 - precision: 0.5208 - recall: 0.3191 - accuracy: 0.5801 - val_loss: 0.6700 - val_precision: 0.5488 - val_recall: 0.1295 - val_accuracy: 0.5792\n",
      "Epoch 27/300\n",
      "166/166 [==============================] - 3s 19ms/step - loss: 0.6674 - precision: 0.5211 - recall: 0.2766 - accuracy: 0.5788 - val_loss: 0.6699 - val_precision: 0.5312 - val_recall: 0.2711 - val_accuracy: 0.5830\n",
      "Epoch 28/300\n",
      "166/166 [==============================] - 3s 19ms/step - loss: 0.6658 - precision: 0.5449 - recall: 0.3143 - accuracy: 0.5915 - val_loss: 0.6690 - val_precision: 0.5147 - val_recall: 0.4621 - val_accuracy: 0.5806\n",
      "Epoch 29/300\n",
      "166/166 [==============================] - 3s 19ms/step - loss: 0.6643 - precision: 0.5371 - recall: 0.2862 - accuracy: 0.5862 - val_loss: 0.6709 - val_precision: 0.5370 - val_recall: 0.3502 - val_accuracy: 0.5901\n",
      "Epoch 30/300\n",
      "166/166 [==============================] - 3s 19ms/step - loss: 0.6664 - precision: 0.5101 - recall: 0.3780 - accuracy: 0.5756 - val_loss: 0.6692 - val_precision: 0.5551 - val_recall: 0.3041 - val_accuracy: 0.5953\n",
      "Epoch 31/300\n",
      "166/166 [==============================] - 3s 19ms/step - loss: 0.6608 - precision: 0.5488 - recall: 0.3358 - accuracy: 0.5949 - val_loss: 0.6747 - val_precision: 0.5495 - val_recall: 0.1218 - val_accuracy: 0.5787\n",
      "Epoch 32/300\n",
      "166/166 [==============================] - 3s 18ms/step - loss: 0.6622 - precision: 0.5525 - recall: 0.2888 - accuracy: 0.5928 - val_loss: 0.6712 - val_precision: 0.5476 - val_recall: 0.2020 - val_accuracy: 0.5844\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.6746 - precision: 0.5183 - recall: 0.4901 - accuracy: 0.5840\n",
      "[[526 277]\n",
      " [310 298]]\n",
      "Training took 107 seconds to complete.\n",
      "Remove 1862 rows on WBA124 out of 8385 as it has no data\n",
      "Remove 1791 rows on WBA124 out of 8377 as it has no data\n",
      "Remove 1459 rows on WBA124 out of 8377 as it has no data\n",
      "Remove 1595 rows on WBA124 out of 8385 as it has no data\n",
      "Remove 1481 rows on WBA124 out of 8385 as it has no data\n",
      "29570 33721\n",
      "4151\n",
      "Epoch 1/300\n",
      "189/189 [==============================] - 11s 36ms/step - loss: 0.6778 - precision_1: 0.4888 - recall_1: 0.2108 - accuracy: 0.5835 - val_loss: 0.6725 - val_precision_1: 0.4739 - val_recall_1: 0.4016 - val_accuracy: 0.5694\n",
      "Epoch 2/300\n",
      "189/189 [==============================] - 6s 31ms/step - loss: 0.6709 - precision_1: 0.5097 - recall_1: 0.1995 - accuracy: 0.5907 - val_loss: 0.6711 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - val_accuracy: 0.5876\n",
      "Epoch 3/300\n",
      "189/189 [==============================] - 6s 31ms/step - loss: 0.6718 - precision_1: 0.4845 - recall_1: 0.1566 - accuracy: 0.5834 - val_loss: 0.6761 - val_precision_1: 0.4710 - val_recall_1: 0.4317 - val_accuracy: 0.5656\n",
      "Epoch 4/300\n",
      "189/189 [==============================] - 6s 31ms/step - loss: 0.6713 - precision_1: 0.4811 - recall_1: 0.1634 - accuracy: 0.5822 - val_loss: 0.6728 - val_precision_1: 0.4743 - val_recall_1: 0.3986 - val_accuracy: 0.5698\n",
      "Epoch 5/300\n",
      "189/189 [==============================] - 6s 31ms/step - loss: 0.6702 - precision_1: 0.4818 - recall_1: 0.1273 - accuracy: 0.5835 - val_loss: 0.6733 - val_precision_1: 0.4718 - val_recall_1: 0.4287 - val_accuracy: 0.5665\n",
      "Epoch 6/300\n",
      "189/189 [==============================] - 6s 31ms/step - loss: 0.6701 - precision_1: 0.5011 - recall_1: 0.1839 - accuracy: 0.5878 - val_loss: 0.6739 - val_precision_1: 0.4805 - val_recall_1: 0.3213 - val_accuracy: 0.5768\n",
      "Epoch 7/300\n",
      "189/189 [==============================] - 6s 31ms/step - loss: 0.6690 - precision_1: 0.4972 - recall_1: 0.1754 - accuracy: 0.5867 - val_loss: 0.6744 - val_precision_1: 0.4785 - val_recall_1: 0.3695 - val_accuracy: 0.5739\n",
      "Epoch 8/300\n",
      "189/189 [==============================] - 6s 31ms/step - loss: 0.6691 - precision_1: 0.4831 - recall_1: 0.1317 - accuracy: 0.5837 - val_loss: 0.6724 - val_precision_1: 0.4798 - val_recall_1: 0.1074 - val_accuracy: 0.5839\n",
      "Epoch 9/300\n",
      "189/189 [==============================] - 6s 31ms/step - loss: 0.6695 - precision_1: 0.4752 - recall_1: 0.1542 - accuracy: 0.5809 - val_loss: 0.6734 - val_precision_1: 0.4747 - val_recall_1: 0.4137 - val_accuracy: 0.5694\n",
      "Epoch 10/300\n",
      "189/189 [==============================] - 6s 31ms/step - loss: 0.6687 - precision_1: 0.4962 - recall_1: 0.1827 - accuracy: 0.5864 - val_loss: 0.6733 - val_precision_1: 0.5055 - val_recall_1: 0.2299 - val_accuracy: 0.5896\n",
      "Epoch 11/300\n",
      "189/189 [==============================] - 6s 31ms/step - loss: 0.6681 - precision_1: 0.5124 - recall_1: 0.1413 - accuracy: 0.5903 - val_loss: 0.6746 - val_precision_1: 0.4712 - val_recall_1: 0.4277 - val_accuracy: 0.5660\n",
      "Epoch 12/300\n",
      "189/189 [==============================] - 6s 31ms/step - loss: 0.6689 - precision_1: 0.4853 - recall_1: 0.2445 - accuracy: 0.5814 - val_loss: 0.6725 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00 - val_accuracy: 0.5876\n",
      "Epoch 13/300\n",
      "189/189 [==============================] - 6s 30ms/step - loss: 0.6680 - precision_1: 0.4988 - recall_1: 0.1658 - accuracy: 0.5872 - val_loss: 0.6732 - val_precision_1: 0.4717 - val_recall_1: 0.4267 - val_accuracy: 0.5665\n",
      "Epoch 14/300\n",
      "189/189 [==============================] - 6s 31ms/step - loss: 0.6681 - precision_1: 0.4815 - recall_1: 0.1357 - accuracy: 0.5832 - val_loss: 0.6737 - val_precision_1: 0.4712 - val_recall_1: 0.4267 - val_accuracy: 0.5660\n",
      "Epoch 15/300\n",
      "189/189 [==============================] - 6s 31ms/step - loss: 0.6686 - precision_1: 0.4963 - recall_1: 0.2670 - accuracy: 0.5859 - val_loss: 0.6737 - val_precision_1: 0.4713 - val_recall_1: 0.4287 - val_accuracy: 0.5660\n",
      "Epoch 16/300\n",
      "189/189 [==============================] - 6s 31ms/step - loss: 0.6688 - precision_1: 0.5330 - recall_1: 0.1883 - accuracy: 0.5971 - val_loss: 0.6744 - val_precision_1: 0.4714 - val_recall_1: 0.4297 - val_accuracy: 0.5660\n",
      "Epoch 17/300\n",
      "189/189 [==============================] - 6s 31ms/step - loss: 0.6687 - precision_1: 0.5064 - recall_1: 0.2393 - accuracy: 0.5900 - val_loss: 0.6733 - val_precision_1: 0.4741 - val_recall_1: 0.3865 - val_accuracy: 0.5702\n",
      "Epoch 18/300\n",
      "189/189 [==============================] - 6s 31ms/step - loss: 0.6670 - precision_1: 0.5033 - recall_1: 0.2144 - accuracy: 0.5887 - val_loss: 0.6717 - val_precision_1: 0.4810 - val_recall_1: 0.1014 - val_accuracy: 0.5843\n",
      "Epoch 19/300\n",
      "189/189 [==============================] - 6s 32ms/step - loss: 0.6676 - precision_1: 0.5122 - recall_1: 0.2100 - accuracy: 0.5917 - val_loss: 0.6750 - val_precision_1: 0.4770 - val_recall_1: 0.4267 - val_accuracy: 0.5706\n",
      "Epoch 20/300\n",
      "189/189 [==============================] - 6s 31ms/step - loss: 0.6669 - precision_1: 0.5172 - recall_1: 0.1690 - accuracy: 0.5922 - val_loss: 0.6723 - val_precision_1: 0.4804 - val_recall_1: 0.4066 - val_accuracy: 0.5739\n",
      "Epoch 21/300\n",
      "189/189 [==============================] - 6s 31ms/step - loss: 0.6672 - precision_1: 0.5095 - recall_1: 0.1722 - accuracy: 0.5902 - val_loss: 0.6723 - val_precision_1: 0.4833 - val_recall_1: 0.4066 - val_accuracy: 0.5760\n",
      "Epoch 22/300\n",
      "189/189 [==============================] - 6s 31ms/step - loss: 0.6655 - precision_1: 0.5185 - recall_1: 0.2706 - accuracy: 0.5955 - val_loss: 0.6717 - val_precision_1: 0.4868 - val_recall_1: 0.2229 - val_accuracy: 0.5826\n",
      "Epoch 23/300\n",
      "189/189 [==============================] - 6s 31ms/step - loss: 0.6658 - precision_1: 0.5326 - recall_1: 0.1574 - accuracy: 0.5955 - val_loss: 0.6720 - val_precision_1: 0.4868 - val_recall_1: 0.3695 - val_accuracy: 0.5793\n",
      "Epoch 24/300\n",
      "189/189 [==============================] - 6s 31ms/step - loss: 0.6651 - precision_1: 0.5320 - recall_1: 0.2437 - accuracy: 0.5996 - val_loss: 0.6744 - val_precision_1: 0.4809 - val_recall_1: 0.4036 - val_accuracy: 0.5743\n",
      "Epoch 25/300\n",
      "189/189 [==============================] - 6s 32ms/step - loss: 0.6651 - precision_1: 0.5120 - recall_1: 0.1798 - accuracy: 0.5910 - val_loss: 0.6727 - val_precision_1: 0.4898 - val_recall_1: 0.2892 - val_accuracy: 0.5826\n",
      "Epoch 26/300\n",
      "189/189 [==============================] - 6s 31ms/step - loss: 0.6632 - precision_1: 0.5285 - recall_1: 0.2417 - accuracy: 0.5983 - val_loss: 0.6754 - val_precision_1: 0.5139 - val_recall_1: 0.1114 - val_accuracy: 0.5901\n",
      "Epoch 27/300\n",
      "189/189 [==============================] - 6s 31ms/step - loss: 0.6639 - precision_1: 0.5286 - recall_1: 0.1778 - accuracy: 0.5955 - val_loss: 0.6747 - val_precision_1: 0.5519 - val_recall_1: 0.1014 - val_accuracy: 0.5954\n",
      "Epoch 28/300\n",
      "189/189 [==============================] - 6s 31ms/step - loss: 0.6615 - precision_1: 0.5652 - recall_1: 0.1722 - accuracy: 0.6039 - val_loss: 0.6733 - val_precision_1: 0.4167 - val_recall_1: 0.0100 - val_accuracy: 0.5859\n",
      "Epoch 29/300\n",
      "189/189 [==============================] - 6s 31ms/step - loss: 0.6617 - precision_1: 0.5217 - recall_1: 0.1690 - accuracy: 0.5933 - val_loss: 0.6814 - val_precision_1: 0.5523 - val_recall_1: 0.0954 - val_accuracy: 0.5950\n",
      "Epoch 30/300\n",
      "189/189 [==============================] - 6s 31ms/step - loss: 0.6614 - precision_1: 0.5431 - recall_1: 0.1240 - accuracy: 0.5956 - val_loss: 0.6724 - val_precision_1: 0.5234 - val_recall_1: 0.0562 - val_accuracy: 0.5896\n",
      "Epoch 31/300\n",
      "189/189 [==============================] - 6s 31ms/step - loss: 0.6596 - precision_1: 0.5704 - recall_1: 0.1658 - accuracy: 0.6044 - val_loss: 0.6892 - val_precision_1: 0.4959 - val_recall_1: 0.3072 - val_accuracy: 0.5855\n",
      "Epoch 32/300\n",
      "189/189 [==============================] - 6s 31ms/step - loss: 0.6573 - precision_1: 0.5561 - recall_1: 0.1971 - accuracy: 0.6039 - val_loss: 0.6802 - val_precision_1: 0.5075 - val_recall_1: 0.2028 - val_accuracy: 0.5901\n",
      "Epoch 33/300\n",
      "189/189 [==============================] - 6s 31ms/step - loss: 0.6603 - precision_1: 0.5171 - recall_1: 0.2180 - accuracy: 0.5935 - val_loss: 0.6743 - val_precision_1: 0.5396 - val_recall_1: 0.1094 - val_accuracy: 0.5942\n",
      "51/51 [==============================] - 0s 9ms/step - loss: 0.6764 - precision_1: 0.4762 - recall_1: 0.4367 - accuracy: 0.5698\n",
      "[[628 319]\n",
      " [374 290]]\n",
      "Training took 201 seconds to complete.\n",
      "Remove 1158 rows on WBA124 out of 8377 as it has no data\n",
      "Remove 1016 rows on WBA124 out of 8369 as it has no data\n",
      "Remove 1009 rows on WBA124 out of 8369 as it has no data\n",
      "Remove 882 rows on WBA124 out of 8377 as it has no data\n",
      "Remove 858 rows on WBA124 out of 8377 as it has no data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32610 36946\n",
      "4336\n",
      "Epoch 1/300\n",
      "204/204 [==============================] - 16s 59ms/step - loss: 0.6750 - precision_2: 0.4416 - recall_2: 0.1511 - accuracy: 0.5847 - val_loss: 0.6680 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00 - val_accuracy: 0.6005\n",
      "Epoch 2/300\n",
      "204/204 [==============================] - 11s 53ms/step - loss: 0.6687 - precision_2: 0.4645 - recall_2: 0.0930 - accuracy: 0.5950 - val_loss: 0.6708 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00 - val_accuracy: 0.6005\n",
      "Epoch 3/300\n",
      "204/204 [==============================] - 11s 53ms/step - loss: 0.6685 - precision_2: 0.4350 - recall_2: 0.0411 - accuracy: 0.5958 - val_loss: 0.6709 - val_precision_2: 0.4555 - val_recall_2: 0.3987 - val_accuracy: 0.5695\n",
      "Epoch 4/300\n",
      "204/204 [==============================] - 11s 53ms/step - loss: 0.6676 - precision_2: 0.4533 - recall_2: 0.0373 - accuracy: 0.5976 - val_loss: 0.6682 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00 - val_accuracy: 0.6005\n",
      "Epoch 5/300\n",
      "204/204 [==============================] - 11s 53ms/step - loss: 0.6666 - precision_2: 0.4474 - recall_2: 0.0523 - accuracy: 0.5958 - val_loss: 0.6681 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00 - val_accuracy: 0.6005\n",
      "Epoch 6/300\n",
      "204/204 [==============================] - 11s 53ms/step - loss: 0.6658 - precision_2: 0.5272 - recall_2: 0.0484 - accuracy: 0.6027 - val_loss: 0.6695 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00 - val_accuracy: 0.6005\n",
      "Epoch 7/300\n",
      "204/204 [==============================] - 11s 53ms/step - loss: 0.6671 - precision_2: 0.4903 - recall_2: 0.0292 - accuracy: 0.6002 - val_loss: 0.6690 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00 - val_accuracy: 0.6005\n",
      "Epoch 8/300\n",
      "204/204 [==============================] - 11s 53ms/step - loss: 0.6657 - precision_2: 0.4140 - recall_2: 0.0342 - accuracy: 0.5950 - val_loss: 0.6674 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00 - val_accuracy: 0.6005\n",
      "Epoch 9/300\n",
      "204/204 [==============================] - 11s 53ms/step - loss: 0.6659 - precision_2: 0.4684 - recall_2: 0.0142 - accuracy: 0.5999 - val_loss: 0.6694 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00 - val_accuracy: 0.6005\n",
      "Epoch 10/300\n",
      "204/204 [==============================] - 11s 53ms/step - loss: 0.6652 - precision_2: 0.5125 - recall_2: 0.0158 - accuracy: 0.6010 - val_loss: 0.6684 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00 - val_accuracy: 0.6005\n",
      "Epoch 11/300\n",
      "204/204 [==============================] - 11s 54ms/step - loss: 0.6656 - precision_2: 0.4908 - recall_2: 0.0308 - accuracy: 0.6002 - val_loss: 0.6682 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00 - val_accuracy: 0.6005\n",
      "Epoch 12/300\n",
      "204/204 [==============================] - 11s 53ms/step - loss: 0.6660 - precision_2: 0.3750 - recall_2: 0.0023 - accuracy: 0.6001 - val_loss: 0.6677 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00 - val_accuracy: 0.6005\n",
      "Epoch 13/300\n",
      "204/204 [==============================] - 11s 53ms/step - loss: 0.6651 - precision_2: 0.4608 - recall_2: 0.0384 - accuracy: 0.5981 - val_loss: 0.6672 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00 - val_accuracy: 0.6005\n",
      "Epoch 14/300\n",
      "204/204 [==============================] - 11s 53ms/step - loss: 0.6659 - precision_2: 0.4645 - recall_2: 0.0277 - accuracy: 0.5990 - val_loss: 0.6678 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00 - val_accuracy: 0.6005\n",
      "Epoch 15/300\n",
      "204/204 [==============================] - 11s 53ms/step - loss: 0.6642 - precision_2: 0.2857 - recall_2: 7.6894e-04 - accuracy: 0.6002 - val_loss: 0.6676 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00 - val_accuracy: 0.6005\n",
      "Epoch 16/300\n",
      "204/204 [==============================] - 11s 53ms/step - loss: 0.6646 - precision_2: 0.5032 - recall_2: 0.0304 - accuracy: 0.6009 - val_loss: 0.6668 - val_precision_2: 0.6667 - val_recall_2: 0.0019 - val_accuracy: 0.6009\n",
      "Epoch 17/300\n",
      "204/204 [==============================] - 11s 53ms/step - loss: 0.6643 - precision_2: 0.4344 - recall_2: 0.0204 - accuracy: 0.5982 - val_loss: 0.6663 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00 - val_accuracy: 0.6005\n",
      "Epoch 18/300\n",
      "204/204 [==============================] - 11s 53ms/step - loss: 0.6635 - precision_2: 0.4074 - recall_2: 0.0042 - accuracy: 0.5999 - val_loss: 0.6670 - val_precision_2: 0.6667 - val_recall_2: 0.0019 - val_accuracy: 0.6009\n",
      "Epoch 19/300\n",
      "204/204 [==============================] - 11s 53ms/step - loss: 0.6631 - precision_2: 0.5789 - recall_2: 0.0042 - accuracy: 0.6012 - val_loss: 0.6686 - val_precision_2: 0.6000 - val_recall_2: 0.0029 - val_accuracy: 0.6009\n",
      "Epoch 20/300\n",
      "204/204 [==============================] - 11s 52ms/step - loss: 0.6625 - precision_2: 0.4405 - recall_2: 0.0142 - accuracy: 0.5992 - val_loss: 0.6687 - val_precision_2: 0.4231 - val_recall_2: 0.0317 - val_accuracy: 0.5959\n",
      "Epoch 21/300\n",
      "204/204 [==============================] - 11s 53ms/step - loss: 0.6673 - precision_2: 0.4660 - recall_2: 0.0342 - accuracy: 0.5987 - val_loss: 0.6702 - val_precision_2: 0.6000 - val_recall_2: 0.0029 - val_accuracy: 0.6009\n",
      "Epoch 22/300\n",
      "204/204 [==============================] - 11s 53ms/step - loss: 0.6657 - precision_2: 0.5500 - recall_2: 0.0127 - accuracy: 0.6016 - val_loss: 0.6714 - val_precision_2: 0.3864 - val_recall_2: 0.0327 - val_accuracy: 0.5929\n",
      "Epoch 23/300\n",
      "204/204 [==============================] - 10s 51ms/step - loss: 0.6662 - precision_2: 0.4963 - recall_2: 0.0519 - accuracy: 0.6004 - val_loss: 0.6692 - val_precision_2: 0.6667 - val_recall_2: 0.0038 - val_accuracy: 0.6013\n",
      "Epoch 24/300\n",
      "204/204 [==============================] - 11s 52ms/step - loss: 0.6641 - precision_2: 0.4733 - recall_2: 0.0511 - accuracy: 0.5984 - val_loss: 0.6673 - val_precision_2: 0.5714 - val_recall_2: 0.0038 - val_accuracy: 0.6009\n",
      "Epoch 25/300\n",
      "204/204 [==============================] - 11s 53ms/step - loss: 0.6643 - precision_2: 0.5000 - recall_2: 0.0300 - accuracy: 0.6007 - val_loss: 0.6671 - val_precision_2: 0.4444 - val_recall_2: 0.0038 - val_accuracy: 0.6002\n",
      "Epoch 26/300\n",
      "204/204 [==============================] - 11s 52ms/step - loss: 0.6636 - precision_2: 0.5000 - recall_2: 0.0381 - accuracy: 0.6007 - val_loss: 0.6675 - val_precision_2: 0.4167 - val_recall_2: 0.0096 - val_accuracy: 0.5990\n",
      "Epoch 27/300\n",
      "204/204 [==============================] - 11s 52ms/step - loss: 0.6638 - precision_2: 0.5109 - recall_2: 0.0450 - accuracy: 0.6015 - val_loss: 0.6680 - val_precision_2: 0.4000 - val_recall_2: 0.0038 - val_accuracy: 0.5998\n",
      "Epoch 28/300\n",
      "204/204 [==============================] - 11s 52ms/step - loss: 0.6636 - precision_2: 0.6486 - recall_2: 0.0092 - accuracy: 0.6024 - val_loss: 0.6696 - val_precision_2: 0.4286 - val_recall_2: 0.0086 - val_accuracy: 0.5994\n",
      "Epoch 29/300\n",
      "204/204 [==============================] - 11s 52ms/step - loss: 0.6613 - precision_2: 0.5980 - recall_2: 0.0235 - accuracy: 0.6038 - val_loss: 0.6714 - val_precision_2: 0.4138 - val_recall_2: 0.0115 - val_accuracy: 0.5986\n",
      "Epoch 30/300\n",
      "204/204 [==============================] - 11s 53ms/step - loss: 0.6622 - precision_2: 0.5600 - recall_2: 0.0323 - accuracy: 0.6035 - val_loss: 0.6709 - val_precision_2: 0.4500 - val_recall_2: 0.0086 - val_accuracy: 0.5998\n",
      "Epoch 31/300\n",
      "204/204 [==============================] - 11s 52ms/step - loss: 0.6612 - precision_2: 0.5417 - recall_2: 0.0450 - accuracy: 0.6035 - val_loss: 0.6733 - val_precision_2: 0.4571 - val_recall_2: 0.0154 - val_accuracy: 0.5994\n",
      "Epoch 32/300\n",
      "204/204 [==============================] - 11s 52ms/step - loss: 0.6629 - precision_2: 0.4906 - recall_2: 0.0300 - accuracy: 0.6002 - val_loss: 0.6677 - val_precision_2: 0.5000 - val_recall_2: 0.0086 - val_accuracy: 0.6005\n",
      "Epoch 33/300\n",
      "204/204 [==============================] - 11s 52ms/step - loss: 0.6622 - precision_2: 0.5238 - recall_2: 0.0550 - accuracy: 0.6027 - val_loss: 0.6682 - val_precision_2: 0.3774 - val_recall_2: 0.0192 - val_accuracy: 0.5955\n",
      "55/55 [==============================] - 1s 14ms/step - loss: 0.6651 - precision_2: 0.4798 - recall_2: 0.4280 - accuracy: 0.5863\n",
      "[[722 322]\n",
      " [397 297]]\n",
      "Training took 365 seconds to complete.\n",
      "Remove 810 rows on WBA124 out of 8369 as it has no data\n",
      "Remove 660 rows on WBA124 out of 8361 as it has no data\n",
      "Remove 777 rows on WBA124 out of 8361 as it has no data\n",
      "Remove 585 rows on WBA124 out of 8369 as it has no data\n",
      "Remove 548 rows on WBA124 out of 8369 as it has no data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34045 38449\n",
      "4404\n",
      "Epoch 1/300\n",
      "211/211 [==============================] - 21s 82ms/step - loss: 0.6713 - precision_3: 0.4637 - recall_3: 0.1355 - accuracy: 0.5989 - val_loss: 0.6655 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_accuracy: 0.6072\n",
      "Epoch 2/300\n",
      "211/211 [==============================] - 16s 74ms/step - loss: 0.6676 - precision_3: 0.4435 - recall_3: 0.0594 - accuracy: 0.6013 - val_loss: 0.6666 - val_precision_3: 0.4502 - val_recall_3: 0.4106 - val_accuracy: 0.5715\n",
      "Epoch 3/300\n",
      "211/211 [==============================] - 15s 73ms/step - loss: 0.6654 - precision_3: 0.4873 - recall_3: 0.1014 - accuracy: 0.6052 - val_loss: 0.6651 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_accuracy: 0.6072\n",
      "Epoch 4/300\n",
      "211/211 [==============================] - 15s 73ms/step - loss: 0.6646 - precision_3: 0.5000 - recall_3: 0.0337 - accuracy: 0.6073 - val_loss: 0.6649 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_accuracy: 0.6072\n",
      "Epoch 5/300\n",
      "211/211 [==============================] - 16s 74ms/step - loss: 0.6657 - precision_3: 0.4444 - recall_3: 0.0091 - accuracy: 0.6064 - val_loss: 0.6642 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_accuracy: 0.6072\n",
      "Epoch 6/300\n",
      "211/211 [==============================] - 16s 74ms/step - loss: 0.6644 - precision_3: 0.5000 - recall_3: 0.0473 - accuracy: 0.6073 - val_loss: 0.6652 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_accuracy: 0.6072\n",
      "Epoch 7/300\n",
      "211/211 [==============================] - 16s 74ms/step - loss: 0.6645 - precision_3: 0.4043 - recall_3: 0.0072 - accuracy: 0.6059 - val_loss: 0.6644 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_accuracy: 0.6072\n",
      "Epoch 8/300\n",
      "211/211 [==============================] - 16s 75ms/step - loss: 0.6638 - precision_3: 0.4154 - recall_3: 0.0307 - accuracy: 0.6023 - val_loss: 0.6651 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_accuracy: 0.6072\n",
      "Epoch 9/300\n",
      "211/211 [==============================] - 16s 74ms/step - loss: 0.6631 - precision_3: 0.2500 - recall_3: 0.0011 - accuracy: 0.6064 - val_loss: 0.6641 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_accuracy: 0.6072\n",
      "Epoch 10/300\n",
      "211/211 [==============================] - 16s 74ms/step - loss: 0.6645 - precision_3: 0.5714 - recall_3: 0.0015 - accuracy: 0.6074 - val_loss: 0.6640 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_accuracy: 0.6072\n",
      "Epoch 11/300\n",
      "211/211 [==============================] - 16s 75ms/step - loss: 0.6644 - precision_3: 0.3000 - recall_3: 0.0011 - accuracy: 0.6067 - val_loss: 0.6638 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_accuracy: 0.6072\n",
      "Epoch 12/300\n",
      "211/211 [==============================] - 16s 76ms/step - loss: 0.6634 - precision_3: 0.4583 - recall_3: 0.0042 - accuracy: 0.6070 - val_loss: 0.6638 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_accuracy: 0.6072\n",
      "Epoch 13/300\n",
      "211/211 [==============================] - 16s 76ms/step - loss: 0.6629 - precision_3: 0.1765 - recall_3: 0.0011 - accuracy: 0.6056 - val_loss: 0.6643 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_accuracy: 0.6072\n",
      "Epoch 14/300\n",
      "211/211 [==============================] - 16s 75ms/step - loss: 0.6637 - precision_3: 0.5000 - recall_3: 3.7850e-04 - accuracy: 0.6073 - val_loss: 0.6639 - val_precision_3: 1.0000 - val_recall_3: 0.0028 - val_accuracy: 0.6083\n",
      "Epoch 15/300\n",
      "211/211 [==============================] - 16s 75ms/step - loss: 0.6638 - precision_3: 0.5102 - recall_3: 0.0189 - accuracy: 0.6076 - val_loss: 0.6634 - val_precision_3: 0.6667 - val_recall_3: 0.0057 - val_accuracy: 0.6083\n",
      "Epoch 16/300\n",
      "211/211 [==============================] - 16s 74ms/step - loss: 0.6632 - precision_3: 0.5714 - recall_3: 0.0091 - accuracy: 0.6081 - val_loss: 0.6635 - val_precision_3: 0.7500 - val_recall_3: 0.0057 - val_accuracy: 0.6087\n",
      "Epoch 17/300\n",
      "211/211 [==============================] - 16s 75ms/step - loss: 0.6629 - precision_3: 0.5238 - recall_3: 0.0125 - accuracy: 0.6077 - val_loss: 0.6637 - val_precision_3: 0.6667 - val_recall_3: 0.0057 - val_accuracy: 0.6083\n",
      "Epoch 18/300\n",
      "211/211 [==============================] - 16s 75ms/step - loss: 0.6635 - precision_3: 0.6275 - recall_3: 0.0121 - accuracy: 0.6092 - val_loss: 0.6635 - val_precision_3: 0.7500 - val_recall_3: 0.0057 - val_accuracy: 0.6087\n",
      "Epoch 19/300\n",
      "211/211 [==============================] - 16s 75ms/step - loss: 0.6620 - precision_3: 0.6970 - recall_3: 0.0087 - accuracy: 0.6092 - val_loss: 0.6642 - val_precision_3: 0.6000 - val_recall_3: 0.0028 - val_accuracy: 0.6076\n",
      "Epoch 20/300\n",
      "211/211 [==============================] - 16s 75ms/step - loss: 0.6617 - precision_3: 0.7188 - recall_3: 0.0087 - accuracy: 0.6093 - val_loss: 0.6638 - val_precision_3: 0.6667 - val_recall_3: 0.0057 - val_accuracy: 0.6083\n",
      "Epoch 21/300\n",
      "211/211 [==============================] - 16s 75ms/step - loss: 0.6612 - precision_3: 0.7600 - recall_3: 0.0072 - accuracy: 0.6092 - val_loss: 0.6639 - val_precision_3: 0.6000 - val_recall_3: 0.0028 - val_accuracy: 0.6076\n",
      "Epoch 22/300\n",
      "211/211 [==============================] - 16s 74ms/step - loss: 0.6614 - precision_3: 0.5600 - recall_3: 0.0106 - accuracy: 0.6081 - val_loss: 0.6653 - val_precision_3: 0.4545 - val_recall_3: 0.0047 - val_accuracy: 0.6068\n",
      "Epoch 23/300\n",
      "211/211 [==============================] - 16s 75ms/step - loss: 0.6605 - precision_3: 0.4876 - recall_3: 0.0371 - accuracy: 0.6065 - val_loss: 0.6644 - val_precision_3: 0.5000 - val_recall_3: 9.4607e-04 - val_accuracy: 0.6072\n",
      "Epoch 24/300\n",
      "211/211 [==============================] - 15s 73ms/step - loss: 0.6619 - precision_3: 0.7368 - recall_3: 0.0053 - accuracy: 0.6086 - val_loss: 0.6628 - val_precision_3: 0.6667 - val_recall_3: 0.0019 - val_accuracy: 0.6076\n",
      "Epoch 25/300\n",
      "211/211 [==============================] - 16s 75ms/step - loss: 0.6615 - precision_3: 0.4970 - recall_3: 0.0625 - accuracy: 0.6070 - val_loss: 0.6649 - val_precision_3: 0.7500 - val_recall_3: 0.0057 - val_accuracy: 0.6087\n",
      "Epoch 26/300\n",
      "211/211 [==============================] - 16s 74ms/step - loss: 0.6601 - precision_3: 0.5269 - recall_3: 0.0333 - accuracy: 0.6086 - val_loss: 0.6642 - val_precision_3: 0.6000 - val_recall_3: 0.0028 - val_accuracy: 0.6076\n",
      "Epoch 27/300\n",
      "211/211 [==============================] - 16s 74ms/step - loss: 0.6605 - precision_3: 0.6415 - recall_3: 0.0129 - accuracy: 0.6095 - val_loss: 0.6634 - val_precision_3: 0.4000 - val_recall_3: 0.0019 - val_accuracy: 0.6068\n",
      "Epoch 28/300\n",
      "211/211 [==============================] - 15s 73ms/step - loss: 0.6588 - precision_3: 0.5346 - recall_3: 0.0322 - accuracy: 0.6089 - val_loss: 0.6635 - val_precision_3: 0.6667 - val_recall_3: 0.0076 - val_accuracy: 0.6087\n",
      "Epoch 29/300\n",
      "211/211 [==============================] - 16s 74ms/step - loss: 0.6600 - precision_3: 0.5372 - recall_3: 0.0492 - accuracy: 0.6099 - val_loss: 0.6643 - val_precision_3: 0.6364 - val_recall_3: 0.0066 - val_accuracy: 0.6083\n",
      "Epoch 30/300\n",
      "211/211 [==============================] - 16s 75ms/step - loss: 0.6593 - precision_3: 0.6032 - recall_3: 0.0144 - accuracy: 0.6092 - val_loss: 0.6640 - val_precision_3: 0.3333 - val_recall_3: 0.0028 - val_accuracy: 0.6061\n",
      "Epoch 31/300\n",
      "211/211 [==============================] - 15s 73ms/step - loss: 0.6587 - precision_3: 0.4943 - recall_3: 0.0496 - accuracy: 0.6068 - val_loss: 0.6643 - val_precision_3: 0.6667 - val_recall_3: 0.0019 - val_accuracy: 0.6076\n",
      "Epoch 32/300\n",
      "211/211 [==============================] - 16s 74ms/step - loss: 0.6569 - precision_3: 0.6545 - recall_3: 0.0136 - accuracy: 0.6098 - val_loss: 0.6632 - val_precision_3: 0.8889 - val_recall_3: 0.0076 - val_accuracy: 0.6098\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.6695 - precision_3: 0.4403 - recall_3: 0.4184 - accuracy: 0.5627\n",
      "[[715 375]\n",
      " [410 295]]\n",
      "Training took 511 seconds to complete.\n"
     ]
    }
   ],
   "source": [
    "seq_result = {}\n",
    "lookback = [12, 24, 48, 72]\n",
    "count = 0\n",
    "mode = \"max\"\n",
    "for hour in lookback:\n",
    "    if count  == 0:\n",
    "        monitor = \"val_recall\"\n",
    "    else:\n",
    "        monitor = f\"val_recall_{count}\"\n",
    "    \n",
    "    count += 1\n",
    "    #NEW MACHINES WITH LITTLE DATA: 120, 121 122, 125\n",
    "    #CAN USE 123, 124, 126, 127, 128, 129, 130, 131, 132, 133\n",
    "    wba123 = status_LSTM(\"WBA123\", hour, 3)\n",
    "    wba124 = status_LSTM(\"WBA124\", hour, 3)\n",
    "    wba126 = status_LSTM(\"WBA126\", hour, 3)\n",
    "    wba127 = status_LSTM(\"WBA127\", hour, 3)\n",
    "    wba128 = status_LSTM(\"WBA128\", hour, 3)\n",
    "    \n",
    "    #remove data points\n",
    "    wba123.impt_state_seq, wba123.impt_duration_seq, wba123.major_down_arr = remove_empty_datapoint(wba123.impt_state_seq, wba123.impt_duration_seq, wba123.major_down_arr)\n",
    "    wba124.impt_state_seq, wba124.impt_duration_seq, wba124.major_down_arr = remove_empty_datapoint(wba124.impt_state_seq, wba124.impt_duration_seq, wba124.major_down_arr)\n",
    "    wba126.impt_state_seq, wba126.impt_duration_seq, wba126.major_down_arr = remove_empty_datapoint(wba126.impt_state_seq, wba126.impt_duration_seq, wba126.major_down_arr)\n",
    "    wba127.impt_state_seq, wba127.impt_duration_seq, wba127.major_down_arr = remove_empty_datapoint(wba127.impt_state_seq, wba127.impt_duration_seq, wba127.major_down_arr)\n",
    "    wba128.impt_state_seq, wba128.impt_duration_seq, wba128.major_down_arr = remove_empty_datapoint(wba128.impt_state_seq, wba128.impt_duration_seq, wba128.major_down_arr)\n",
    "\n",
    "    start = datetime.now()\n",
    "    tmp1 = np.concatenate((wba123.impt_state_seq, wba124.impt_state_seq, wba126.impt_state_seq, wba127.impt_state_seq, wba128.impt_state_seq))\n",
    "    encoded_X_seq, n_statename = label_encode(tmp1)\n",
    "    numerical_X_seq = np.concatenate((wba123.impt_duration_seq, wba124.impt_duration_seq, wba126.impt_duration_seq, wba127.impt_duration_seq, wba128.impt_duration_seq,)) # one numerical variable (duration associated with the statename)\n",
    "    target = np.concatenate((wba123.major_down_arr, wba124.major_down_arr, wba126.major_down_arr, wba127.major_down_arr, wba128.major_down_arr, ))\n",
    "    \n",
    "    # downsample negative data\n",
    "    encoded_X_seq, numerical_X_seq, target = randomly_select_negative(encoded_X_seq, numerical_X_seq,  target, 0.2)\n",
    "    \n",
    "    #shuffle the X and Y values to make generalize better\n",
    "    shuffled = shuffle(encoded_X_seq, numerical_X_seq, target)\n",
    "    encoded_X_seq = shuffled[0]\n",
    "    numerical_X_seq = shuffled[1]\n",
    "    target = shuffled[2]\n",
    "    \n",
    "    # padding to average length\n",
    "    mean_length = int(np.mean([len(x) for x in encoded_X_seq]))\n",
    "    padded_statename = np.zeros([len(encoded_X_seq), mean_length])\n",
    "    padded_duration = np.zeros([len(encoded_X_seq), mean_length])\n",
    "    for i,j in enumerate(encoded_X_seq):\n",
    "        padded_statename[i][0:len(j)] = j[:mean_length]\n",
    "        padded_duration[i][0:len(j)] = numerical_X_seq[i][:mean_length]\n",
    "\n",
    "    #train_val_test split\n",
    "    X_train_statename_seq, X_val_statename_seq, X_train_duration_seq, X_val_duration_seq, y_train_seq, y_val_seq =  train_test_split(padded_statename, padded_duration, target, test_size=0.4, random_state=42, stratify=target)\n",
    "    X_val_statename_seq, X_test_statename_seq, X_val_duration_seq, X_test_duration_seq, y_val_seq, y_test_seq =  train_test_split(X_val_statename_seq, X_val_duration_seq, y_val_seq, test_size=0.4, random_state=42, stratify=y_val_seq)    \n",
    "    \n",
    "    # embed the categorical variable using Keras Functional API\n",
    "    in_layer = Input(shape=(X_train_statename_seq.shape[1],), name=\"statename\")\n",
    "    em_layer = Embedding(n_statename, 128, mask_zero=True)(in_layer)\n",
    "    em_layer = Reshape((X_train_statename_seq.shape[1], -1))(em_layer)\n",
    "\n",
    "    # input layer for numerical variable\n",
    "    in_num = Input(shape=(X_train_duration_seq.shape[1], 1), name=\"duration\")\n",
    "\n",
    "    merge = concatenate([em_layer, in_num])\n",
    "    lstm_layer1 = Bidirectional(LSTM(128, return_sequences=True))(merge)\n",
    "    dropout1 = Dropout(0.5)(lstm_layer1)\n",
    "    lstm_layer2 = LSTM(64)(dropout1)\n",
    "    dropout2 = Dropout(0.5)(lstm_layer2)\n",
    "    dense = Dense(64, activation='relu')(dropout2)\n",
    "    classifier = Dense(1, activation='sigmoid')(dense)\n",
    "\n",
    "    all_inputs = [in_layer, in_num]\n",
    "    model = Model(inputs=all_inputs, outputs=classifier)\n",
    "\n",
    "    # model seems to be overfitting, try to reduce overfitting by reduce LR, but model should take longer to converge so use a larger EPOCH\n",
    "    callbacks = [ReduceLROnPlateau(monitor=monitor, factor=0.2, patience=5, min_lr=0.001), \\\n",
    "                EarlyStopping(monitor=monitor, mode=mode, patience=30, restore_best_weights=True)]\n",
    "\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), 'accuracy'])\n",
    "\n",
    "    model.fit([X_train_statename_seq, X_train_duration_seq], y_train_seq,\n",
    "             validation_data=([X_val_statename_seq, X_val_duration_seq], y_val_seq),\n",
    "             callbacks=callbacks, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "\n",
    "    evaluate = model.evaluate([X_test_statename_seq, X_test_duration_seq], y_test_seq) #loss, mse\n",
    "    \n",
    "    pred = model.predict([X_test_statename_seq, X_test_duration_seq])\n",
    "    classes = []\n",
    "    for ele in pred:\n",
    "        classes.append(int((ele>0.5)[0]))\n",
    "\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "    cm = confusion_matrix(y_test_seq, classes)\n",
    "    print(cm)\n",
    "    \n",
    "    seq_result[hour] = [evaluate, cm]\n",
    "\n",
    "    end = datetime.now()\n",
    "    time = end - start\n",
    "    print(f\"Training took {time.seconds} seconds to complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10f43971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{12: [[0.6746309399604797,\n",
       "   0.5182608962059021,\n",
       "   0.49013158679008484,\n",
       "   0.5839830040931702],\n",
       "  array([[526, 277],\n",
       "         [310, 298]])],\n",
       " 24: [[0.6764256358146667,\n",
       "   0.4761904776096344,\n",
       "   0.4367469847202301,\n",
       "   0.5698323845863342],\n",
       "  array([[628, 319],\n",
       "         [374, 290]])],\n",
       " 48: [[0.6651474833488464,\n",
       "   0.479806125164032,\n",
       "   0.42795389890670776,\n",
       "   0.586306095123291],\n",
       "  array([[722, 322],\n",
       "         [397, 297]])],\n",
       " 72: [[0.6695340275764465,\n",
       "   0.44029849767684937,\n",
       "   0.41843971610069275,\n",
       "   0.5626741051673889],\n",
       "  array([[715, 375],\n",
       "         [410, 295]])]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_result # monitor val recall bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd95d4bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{84: [[1.2854076623916626,\n",
       "   0.5663600564002991,\n",
       "   0.6096181273460388,\n",
       "   0.6653782725334167],\n",
       "  array([[774, 330],\n",
       "         [276, 431]])],\n",
       " 96: [[1.0571415424346924,\n",
       "   0.5884892344474792,\n",
       "   0.5768688321113586,\n",
       "   0.6787280440330505],\n",
       "  array([[829, 286],\n",
       "         [300, 409]])],\n",
       " 120: [[1.2010926008224487,\n",
       "   0.5956112742424011,\n",
       "   0.5344585180282593,\n",
       "   0.6804123520851135],\n",
       "  array([[874, 258],\n",
       "         [331, 380]])]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "079f4def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{12: [[0.6739060282707214,\n",
       "   0.49929675459861755,\n",
       "   0.5838815569877625,\n",
       "   0.5686968564987183],\n",
       "  array([[448, 356],\n",
       "         [253, 355]])],\n",
       " 24: [[0.6818949580192566,\n",
       "   0.4321766495704651,\n",
       "   0.4114114046096802,\n",
       "   0.5337879657745361],\n",
       "  array([[587, 360],\n",
       "         [392, 274]])],\n",
       " 48: [[0.6943953633308411,\n",
       "   0.4988763928413391,\n",
       "   0.319884717464447,\n",
       "   0.6001150608062744],\n",
       "  array([[821, 223],\n",
       "         [472, 222]])],\n",
       " 72: [[1.1131274700164795,\n",
       "   0.5409638285636902,\n",
       "   0.6386913061141968,\n",
       "   0.6458449363708496],\n",
       "  array([[709, 381],\n",
       "         [254, 449]])]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_result # downsample negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a559e1af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{5: [[0.4488883316516876,\n",
       "   0.27272728085517883,\n",
       "   0.3744680881500244,\n",
       "   0.814032793045044],\n",
       "  array([[4748,  704],\n",
       "         [ 441,  264]]),\n",
       "  '              precision    recall  f1-score   support\\n\\n           0       0.92      0.87      0.89      5452\\n           1       0.27      0.37      0.32       705\\n\\n    accuracy                           0.81      6157\\n   macro avg       0.59      0.62      0.60      6157\\nweighted avg       0.84      0.81      0.83      6157\\n'],\n",
       " 10: [[0.5950702428817749,\n",
       "   0.18969346582889557,\n",
       "   0.6056737303733826,\n",
       "   0.6585999727249146],\n",
       "  array([[3628, 1824],\n",
       "         [ 278,  427]]),\n",
       "  '              precision    recall  f1-score   support\\n\\n           0       0.93      0.67      0.78      5452\\n           1       0.19      0.61      0.29       705\\n\\n    accuracy                           0.66      6157\\n   macro avg       0.56      0.64      0.53      6157\\nweighted avg       0.84      0.66      0.72      6157\\n'],\n",
       " 15: [[0.6356426477432251,\n",
       "   0.20301418006420135,\n",
       "   0.6505681872367859,\n",
       "   0.6678043007850647],\n",
       "  array([[3651, 1798],\n",
       "         [ 246,  458]]),\n",
       "  '              precision    recall  f1-score   support\\n\\n           0       0.94      0.67      0.78      5449\\n           1       0.20      0.65      0.31       704\\n\\n    accuracy                           0.67      6153\\n   macro avg       0.57      0.66      0.55      6153\\nweighted avg       0.85      0.67      0.73      6153\\n'],\n",
       " 20: [[1.0185438394546509,\n",
       "   0.11843039840459824,\n",
       "   0.9474431872367859,\n",
       "   0.18706321716308594],\n",
       "  array([[ 484, 4965],\n",
       "         [  37,  667]]),\n",
       "  '              precision    recall  f1-score   support\\n\\n           0       0.93      0.09      0.16      5449\\n           1       0.12      0.95      0.21       704\\n\\n    accuracy                           0.19      6153\\n   macro avg       0.52      0.52      0.19      6153\\nweighted avg       0.84      0.19      0.17      6153\\n']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_result # Bidirectional, leaving class 1 weight as variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f37f8617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{12: [0.6014404296875,\n",
       "  0.1650485396385193,\n",
       "  0.44810542464256287,\n",
       "  0.6304535865783691],\n",
       " 24: [0.5659424066543579,\n",
       "  0.2021428644657135,\n",
       "  0.4262048304080963,\n",
       "  0.7228491902351379],\n",
       " 48: [0.532018780708313,\n",
       "  0.21800947189331055,\n",
       "  0.530259370803833,\n",
       "  0.7219125032424927],\n",
       " 72: [0.4936830699443817, 0.23517654836177826, 0.5, 0.7561728358268738]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_result #Bidirecitonal, 5 EQ under same EQ family, remove empty datapoints, shuffle+masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "984bf4da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{12: [0.6382883191108704,\n",
       "  0.16345123946666718,\n",
       "  0.5016447305679321,\n",
       "  0.5972342491149902],\n",
       " 24: [0.6094536185264587,\n",
       "  0.1734505146741867,\n",
       "  0.5639097690582275,\n",
       "  0.6156550645828247],\n",
       " 48: [0.5899127125740051, 0.0, 0.0, 0.8825616836547852],\n",
       " 72: [0.5766468048095703,\n",
       "  0.16018734872341156,\n",
       "  0.48510637879371643,\n",
       "  0.649772584438324]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_result # everything above, except bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7df30bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENV-MAL-VE",
   "language": "python",
   "name": "env-mal-ve_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
