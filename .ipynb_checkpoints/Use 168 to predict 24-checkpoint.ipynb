{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef385019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Input, \\\n",
    "                                        BatchNormalization, Embedding, Masking, Bidirectional, Conv1D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 300\n",
    "\n",
    "from datetime import datetime, timedelta, date\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cx_Oracle\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import joblib\n",
    "import shap\n",
    "\n",
    "class Aggregate_helper:\n",
    "    def __init__(self, eq_id, lookback_window, major_down_hour, alarm_table):\n",
    "        self.eq_id = eq_id\n",
    "        self.lookback_window = lookback_window\n",
    "        self.alarm_table = alarm_table\n",
    "        self.status_table = self.query_status()\n",
    "        self.major_down_hour = major_down_hour\n",
    "        \n",
    "        ## include all status instead since further preprocessing would be performed\n",
    "        ## make sure that the timeframe table is a subset of both the alarm and status table to compute major down correctly\n",
    "        time1 = self.alarm_table.iloc[0][\"DT_SET\"]\n",
    "        time2 = self.status_table.iloc[0][\"TIMESTAMP_START\"]\n",
    "        timeend1 = self.alarm_table.iloc[len(self.alarm_table)-1][\"DT_SET\"]\n",
    "        timeend2 = self.status_table.iloc[len(self.status_table)-1][\"TIMESTAMP_START\"]\n",
    "        \n",
    "        # give a 3 days window to ensure that the alarm and status are captured fully\n",
    "        start = (max(time1, time2) + timedelta(days=3)).strftime(\"%d/%m/%Y\") \n",
    "        end = min(timeend1, timeend2).strftime(\"%d/%m/%Y\")\n",
    "        \n",
    "        self.timeframe_table = self.generate_time(start, end, 24)\n",
    "        self.major_down_arr = self.major_down(self.timeframe_table, self.status_table, self.major_down_hour, 3600)\n",
    "        self.aggregated = self.aggregate(self.timeframe_table, self.lookback_window, self.alarm_table, self.status_table)\n",
    "        self.aggregated_table = pd.concat([self.timeframe_table.reset_index(drop=True), self.aggregated.reset_index(drop=True)], axis=1)\n",
    "        \n",
    "    def generate_time(self, start_date:str, end_date:str, hour:int):\n",
    "        start = datetime.strptime(start_date, '%d/%m/%Y')\n",
    "        end = datetime.strptime(end_date, '%d/%m/%Y')\n",
    "\n",
    "        dates = []\n",
    "        while start<=end:\n",
    "            row = [start]\n",
    "            dates.append(row)\n",
    "            start += timedelta(hours=hour)\n",
    "\n",
    "        return pd.DataFrame(dates, columns=['TIMESTAMP'])\n",
    "    \n",
    "    def query_status(self):\n",
    "        try:\n",
    "            oracle_string = \"oracle+cx_oracle://{username}:{password}@{hostname}:{port}/{database}\"\n",
    "            engine = create_engine(\n",
    "                oracle_string.format(\n",
    "                    username = 'TFM4CEBERUS',\n",
    "                    password = 'TFM4CEBERUS',\n",
    "                    hostname = 'ome-db.bth.infineon.com',\n",
    "                    port = '1538',\n",
    "                    database = 'ome'\n",
    "                    )\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "\n",
    "        query = f\"\"\"select EQ_ID, TIMESTAMP_START, TIMESTAMP_END, DURATION, STATE_NAME, LEVEL3_NAME, LEVEL3 \n",
    "                from (SELECT\n",
    "                  eq.eq_id, eq.name, eq.eq_type_ident\n",
    "                , data.timestamp_start,data.timestamp_end\n",
    "                , ROUND((data.timestamp_end - data.timestamp_start)*24*60*60,0) AS Duration\n",
    "                , data.tr25_3_status,data.tr25_4_status,data.tr25_5_status,data.eq_status\n",
    "                , level5s.state_name\n",
    "                , level5.state_name Level5_Name, level5.state_sign Level5\n",
    "                , level4.state_name Level4_Name, level4.state_sign Level4\n",
    "                , level3.state_name Level3_Name, level3.state_sign Level3\n",
    "                ,mh.device\n",
    "                ,mh.package,\n",
    "                mh.lotid as lot,\n",
    "                mh.product,\n",
    "                mh.operation\n",
    "\n",
    "                FROM OMEDATA.EQUIPMENT_STATE_HISTORY data\n",
    "                , OMEADMIN.EQUIPMENT_INSTANCES eq\n",
    "                , V_EQ_STATES level5s\n",
    "                , OMEADMIN.DEF_STANDARD_STATEMODEL level5\n",
    "                , OMEADMIN.DEF_STANDARD_STATEMODEL level4\n",
    "                , OMEADMIN.DEF_STANDARD_STATEMODEL level3\n",
    "                , OMEDATA.METAKEY_HISTORY mh\n",
    "\n",
    "                WHERE data.eq_ident  = eq.eq_ident\n",
    "                AND  data.eq_status = level5s.state_ident(+)\n",
    "                AND level5.state_ident = data.tr25_5_status\n",
    "                AND level4.state_ident = data.tr25_4_status\n",
    "                AND level3.state_ident = data.tr25_3_status\n",
    "                AND  data.metakey_ident =mh.ident(+)\n",
    "                and data.timestamp_start > sysdate - 1050)\n",
    "                where eq_id = '{self.eq_id}'\n",
    "                ORDER BY TIMESTAMP_START\"\"\"\n",
    "\n",
    "        status = pd.read_sql(query, engine)\n",
    "        status.columns = map(lambda x: str(x).upper(), status.columns) \n",
    "\n",
    "        return status\n",
    "    \n",
    "    def aggregate(self, timeframe_table, lookback_window, alarm_table, status_table):\n",
    "        alarm_df = pd.DataFrame()\n",
    "        statename_df = pd.DataFrame()\n",
    "\n",
    "        for idx, row in timeframe_table.iterrows():\n",
    "            end = row[\"TIMESTAMP\"]\n",
    "            start = end - timedelta(hours=lookback_window)\n",
    "\n",
    "            ## count the frequencies of each alarm\n",
    "            filtered_alarm = alarm_table.loc[(alarm_table[\"DT_SET\"] >= start) & (alarm_table[\"DT_SET\"] <= end)]\n",
    "            alarm_freq_table = filtered_alarm[\"Alarm ID\"].value_counts().to_frame().T.reset_index(drop=True)\n",
    "            alarm_df = pd.concat([alarm_df, alarm_freq_table], axis=0)\n",
    "\n",
    "            ## count the frequencies of each statename, include everything since feature engineering would be performed\n",
    "            filtered_statename = status_table.loc[(status_table[\"TIMESTAMP_START\"] >= start) & (status_table[\"TIMESTAMP_START\"] <= end)]\n",
    "            status_freq_table = filtered_statename[\"STATE_NAME\"].value_counts().to_frame().T.reset_index(drop=True)\n",
    "            statename_df = pd.concat([statename_df, status_freq_table], axis=0)\n",
    "        \n",
    "        df = pd.concat([alarm_df, statename_df], axis=1) # current dataframe\n",
    "        df = df.fillna(0)\n",
    "\n",
    "        ## convert all columns from float to int\n",
    "        cols = df.columns\n",
    "        df[cols] = df[cols].astype(int)\n",
    "        df[\"EQUIPMENT\"] = self.eq_id\n",
    "        return df\n",
    "        \n",
    "    def major_down(self, input_df, status_table, hour, threshold): \n",
    "        hour = pd.Timedelta(hours=hour)\n",
    "        major_down = []\n",
    "\n",
    "        for idx, row in input_df.iterrows():\n",
    "            start = row['TIMESTAMP']\n",
    "            end = start+hour\n",
    "            frame = status_table[(status_table['TIMESTAMP_START']>start) & (status_table['TIMESTAMP_START']<end)]\n",
    "            UD = frame.loc[frame['LEVEL3']=='UDT']\n",
    "\n",
    "            if len(UD) == 0: #no record within this 6 hours:\n",
    "                major_down.append(0)\n",
    "            else:\n",
    "                time_diff = (UD['TIMESTAMP_END']-UD['TIMESTAMP_START']).dt.seconds\n",
    "                if any(time_diff>=threshold): #threshold = 3600s\n",
    "                    major_down.append(1)\n",
    "                else:\n",
    "                    major_down.append(0)\n",
    "        return np.array(major_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d80118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "all_tables = []\n",
    "all_target = []\n",
    "\n",
    "start = datetime.now()\n",
    "for _, path, filename in os.walk(\"Data\"):\n",
    "    for ele in filename:\n",
    "        alarm_file = ele\n",
    "        eq_id = ele[:6]\n",
    "        print(eq_id)\n",
    "\n",
    "        full_alarm = pd.read_excel(f\"Data/{alarm_file}\", engine='openpyxl', usecols = \"B,C,D,F,M\")\n",
    "        equipment = Aggregate_helper(eq_id, 168, 24, full_alarm)\n",
    "\n",
    "        all_tables.append(equipment.aggregated_table)\n",
    "        all_target.append(equipment.major_down_arr)\n",
    "\n",
    "end = datetime.now()\n",
    "print(f\"Data collection took {(end-start).seconds} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87328572",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_training = pd.concat(all_tables, axis=0)\n",
    "new_training = new_training.fillna(0)\n",
    "\n",
    "## convert all columns from float to int\n",
    "df_float = new_training.select_dtypes(include=[np.float])\n",
    "cols = df_float.columns\n",
    "new_training[cols] = new_training[cols].astype(int)\n",
    "\n",
    "new_target = np.concatenate(all_target)\n",
    "\n",
    "col_order = new_training.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03107f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_training[\"TARGET\"] = new_target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0225f667",
   "metadata": {},
   "source": [
    "# Check Prediction if train_val_split by index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedf349c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = new_training.drop([\"TIMESTAMP\", \"EQUIPMENT\", \"TARGET\"], axis=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(training_df.values, new_target, test_size=0.2, stratify=new_target)\n",
    "print(Counter(y_train), Counter(y_val))\n",
    "\n",
    "# balanced dataset\n",
    "# 24 hours: 93% testing accuracy\n",
    "# 168 hours: 95% RFM 92% DT\n",
    "\n",
    "# unbalanced dataset but with class weight\n",
    "# 168 hours = 94% RF, 92% DT\n",
    "\n",
    "# unbalanced\n",
    "# 168 hours = 94% RF, 92% DT\n",
    "\n",
    "# shuffle\n",
    "# 168 hours = 95% RF, 92% DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45770ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = new_training.drop([\"TIMESTAMP\", \"EQUIPMENT\"], axis=1)\n",
    "train_pct = int(0.8*len(training_df))\n",
    "train = training_df[:train_pct]\n",
    "val = training_df[train_pct:]\n",
    "\n",
    "X_train = train.drop([\"TARGET\"], axis=1).values\n",
    "y_train = train[\"TARGET\"].values\n",
    "\n",
    "X_val = val.drop([\"TARGET\"], axis=1).values\n",
    "y_val = val[\"TARGET\"].values\n",
    "print(Counter(y_train), Counter(y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2092e6d2",
   "metadata": {},
   "source": [
    "# Check prediction results when test set is collected from the latest dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ed122f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect test set by date\n",
    "aug = datetime(2021, 8, 15)\n",
    "test_set = new_training[new_training[\"TIMESTAMP\"]>=aug].drop([\"TIMESTAMP\", \"EQUIPMENT\"], axis=1)\n",
    "training_set = new_training[new_training[\"TIMESTAMP\"]<aug].drop([\"TIMESTAMP\", \"EQUIPMENT\"], axis=1)\n",
    "\n",
    "X_train = training_set.drop([\"TARGET\"], axis=1).values\n",
    "y_train = np.array(training_set[\"TARGET\"].values)\n",
    "\n",
    "X_val = test_set.drop([\"TARGET\"], axis=1).values\n",
    "y_val = np.array(test_set[\"TARGET\"].values)\n",
    "\n",
    "# best accuracy: 81% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff4c3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(y_train), Counter(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680f04b8",
   "metadata": {},
   "source": [
    "# Check prediction results when test set is one specific equipment after a date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9ff203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect test set by EQ\n",
    "# predict aug results for that EQ\n",
    "test_set = new_training[(new_training[\"TIMESTAMP\"]>=aug) &\n",
    "                        (new_training[\"EQUIPMENT\"]==\"WBA127\")].drop([\"TIMESTAMP\", \"EQUIPMENT\"], axis=1)\n",
    "training_set = new_training[~((new_training[\"TIMESTAMP\"]>=aug) &\n",
    "                        (new_training[\"EQUIPMENT\"]==\"WBA127\"))].drop([\"TIMESTAMP\", \"EQUIPMENT\"], axis=1)\n",
    "\n",
    "X_train = training_set.drop([\"TARGET\"], axis=1).values\n",
    "y_train = np.array(training_set[\"TARGET\"].values)\n",
    "\n",
    "X_val = test_set.drop([\"TARGET\"], axis=1).values\n",
    "y_val = np.array(test_set[\"TARGET\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaa23e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(y_train), Counter(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a9d06b",
   "metadata": {},
   "source": [
    "# Check prediction when it is only trained on one equipment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5914b868",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = datetime(2021, 8, 25)\n",
    "test_set = new_training[(new_training[\"TIMESTAMP\"]>=aug) &\n",
    "                        (new_training[\"EQUIPMENT\"]==\"WBA128\")].drop([\"TIMESTAMP\", \"EQUIPMENT\"], axis=1)\n",
    "training_set = new_training[(new_training[\"TIMESTAMP\"]<=aug) &\n",
    "                        (new_training[\"EQUIPMENT\"]==\"WBA128\")].drop([\"TIMESTAMP\", \"EQUIPMENT\"], axis=1)\n",
    "\n",
    "X_train = training_set.drop([\"TARGET\"], axis=1).values\n",
    "y_train = np.array(training_set[\"TARGET\"].values)\n",
    "\n",
    "X_val = test_set.drop([\"TARGET\"], axis=1).values\n",
    "y_val = np.array(test_set[\"TARGET\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dcefdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(y_train), Counter(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d23b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_val, y_train, y_val = train_test_split(new_training.values, new_target, test_size=0.2, stratify=new_target)\n",
    "# print(Counter(y_train), Counter(y_val))\n",
    "\n",
    "# oversample = SMOTE()\n",
    "# smote_X, smote_y = oversample.fit_resample(X_train, y_train)\n",
    "# print(Counter(smote_y))\n",
    "\n",
    "# class_weights = class_weight.compute_class_weight('balanced',\n",
    "#                                                  np.unique(y_train),\n",
    "#                                                  y_train)\n",
    "# weights ={}\n",
    "# for idx, val in enumerate(class_weights):\n",
    "#     weights[idx] = val\n",
    "# weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2006e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model = DecisionTreeClassifier(random_state=42) # decision tree has no n_jobs parameter\n",
    "# xgb_model = XGBClassifier(random_state=42, n_jobs=-1)\n",
    "rf_model = RandomForestClassifier(random_state=42, n_jobs=-1, oob_score=True)\n",
    "\n",
    "def to_labels(y_scores, threshold):\n",
    "    return (y_scores >= threshold).astype('int')\n",
    "\n",
    "models = [rf_model]\n",
    "for model in models:\n",
    "    start = datetime.now()\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_val)\n",
    "    y_score = model.predict_proba(X_val)[:,1]\n",
    "\n",
    "#     end = datetime.now()\n",
    "#     print(f\"Training took {(end-start).seconds} seconds\")\n",
    "#     print(confusion_matrix(y_val, pred))\n",
    "#     print(f\"Prediction Accuracy for {type(model).__name__} is {accuracy_score(y_val, pred)}\")\n",
    "print(confusion_matrix(y_val, pred), accuracy_score(y_val, pred))\n",
    "print(\"Model out of bag score = \", rf_model.oob_score_)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_score)\n",
    "J = tpr - fpr\n",
    "ix = np.argmax(J)\n",
    "best_thresh = thresholds[ix]\n",
    "print('Best Threshold=%f' % (best_thresh))\n",
    "\n",
    "testest = to_labels(y_score, best_thresh) # best thresh optimizes recall\n",
    "confusion_matrix(y_val, testest), accuracy_score(y_val, testest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd445ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "pred = rf_model.predict(X_val)\n",
    "\n",
    "print(confusion_matrix(y_val, pred))\n",
    "print(f\"Prediction Accuracy for {type(rf_model).__name__} is {accuracy_score(y_val, pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3177506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ca0126",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'RF_168Input24Output_95Acc.pkl'\n",
    "joblib.dump(rf_model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6462bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model_path = 'DT_168Input24Output_92Acc.pkl'\n",
    "joblib.dump(dt_model, dt_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a12a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = permutation_importance(rf_model, X_test, y_test, n_repeats=5, n_jobs=-1, random_state=0, scoring='f1')\n",
    "# for i in r.importances_mean.argsort()[::-1]:\n",
    "#     if r.importances_mean[i] - 2 * r.importances_std[i] > 0:\n",
    "#         print(f\"{new_training.columns[i]:<8}\"\n",
    "#               f\"{r.importances_mean[i]:.3f}\"\n",
    "#               f\" +/- {r.importances_std[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9b3048",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "randomlist = random.sample(range(1, len(X_val)), 144)\n",
    "abc_train = X_val[randomlist]\n",
    "abc_target = y_val[randomlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f4c9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "abc_pred = rf_model.predict(abc_train)\n",
    "print(confusion_matrix(abc_target, abc_pred))\n",
    "print(f\"Prediction Accuracy for {type(rf_model).__name__} is {accuracy_score(abc_target, abc_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ae3b96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENV-MAL-VE",
   "language": "python",
   "name": "env-mal-ve_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
