{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e516e46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Input, Embedding, Masking, Bidirectional, Conv1D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 300\n",
    "\n",
    "from datetime import datetime, timedelta, date\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cx_Oracle\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "class seven_days_LSTM:\n",
    "    def __init__(self, eq_id, alarm_table, hour_horizontal, hour_vertical):\n",
    "        self.eq_id = eq_id\n",
    "        self.alarm_table = alarm_table\n",
    "        self.status_table = self.query_status()\n",
    "        self.hour_horizontal = hour_horizontal\n",
    "        self.hour_vertical = hour_vertical\n",
    "        \n",
    "        status_start = self.status_table.iloc[0][\"TIMESTAMP_START\"].date() + timedelta(days=1)\n",
    "        status_end = self.status_table.iloc[len(self.status_table)-1][\"TIMESTAMP_START\"].date()\n",
    "        alarm_start = self.alarm_table.iloc[0][\"DT_SET\"].date() + timedelta(days=1) # add one day to make it start from 00:00:00\n",
    "        alarm_end = self.alarm_table.iloc[len(self.alarm_table)-1][\"DT_SET\"].date()\n",
    "        self.start_date = max(status_start, alarm_start)\n",
    "        self.end_date = min(status_end, alarm_end)\n",
    "        \n",
    "        self.timeframe_table = self.generate_time(self.start_date.strftime(\"%d/%m/%Y\"), self.end_date.strftime(\"%d/%m/%Y\"), \\\n",
    "                                                  self.hour_horizontal, self.hour_vertical)\n",
    "        self.alarm_freq_table = self.alarm_freq(self.timeframe_table, self.alarm_table)\n",
    "        \n",
    "        self.major_down_arr = self.major_down(self.timeframe_table, self.status_table, 6, 3600)\n",
    "\n",
    "        self.X_seq = self.alarm_breakdown_pattern(self.timeframe_table, self.alarm_table, self.status_table, self.hour_horizontal)\n",
    "        \n",
    "    def generate_time(self, start_date:str, end_date:str, hours_row:int, hour:int):\n",
    "        start = datetime.strptime(start_date, '%d/%m/%Y')\n",
    "        end = datetime.strptime(end_date, '%d/%m/%Y')\n",
    "\n",
    "        dates = []\n",
    "        while start+timedelta(hours=hours_row)<=end:\n",
    "            row = [start, start+timedelta(hours=hours_row)]\n",
    "            dates.append(row)\n",
    "            start += timedelta(hours=hour)\n",
    "\n",
    "        return pd.DataFrame(dates, columns=['TIMESTAMP_START', 'TIMESTAMP_END'])\n",
    "    \n",
    "    \n",
    "    def alarm_freq(self, input_table, alarm_table):\n",
    "        #create an empty alarm table\n",
    "        seq = alarm_table['Alarm ID'].value_counts().to_dict()\n",
    "        empty_seq = dict.fromkeys(seq, 0)\n",
    "        alarm_id_df = pd.DataFrame(columns = list(empty_seq.keys()))\n",
    "\n",
    "        for idx, row in input_table.iterrows():\n",
    "            alarm_dict = empty_seq\n",
    "            start_date = row['TIMESTAMP_START']\n",
    "            end_date = row['TIMESTAMP_END']\n",
    "            table = alarm_table[(alarm_table['DT_SET']>=start_date) &(alarm_table['DT_SET']<=end_date)]\n",
    "            alarm_seq = table['Alarm ID'].value_counts().to_dict()\n",
    "            for key, val in alarm_seq.items():\n",
    "                if key not in alarm_dict.keys():\n",
    "                    return alarm_seq, key\n",
    "                alarm_dict[key] = int(val)\n",
    "            alarm_id_df = alarm_id_df.append(alarm_seq, ignore_index=True)\n",
    "            \n",
    "        alarm_id_df = alarm_id_df.fillna(0)\n",
    "        float_col = alarm_id_df.columns[alarm_id_df.dtypes.eq('float64')]\n",
    "        for col in float_col:\n",
    "            alarm_id_df[col] = alarm_id_df[col].astype('int64')\n",
    "        return alarm_id_df\n",
    "    \n",
    "    def alarm_breakdown_pattern(self, datetime_table, alarm_table, status_table, hour):\n",
    "        ORIG_ALARMS = []\n",
    "        \n",
    "        #validate alarm table date\n",
    "        if alarm_table.iloc[0]['DT_SET'] < status_table.iloc[0]['TIMESTAMP_START'] or \\\n",
    "            alarm_table.iloc[len(alarm_table)-1]['DT_SET'] > status_table.iloc[len(status_table)-1]['TIMESTAMP_START']:\n",
    "            raise ValueError(\"Alarm table date must be within the range of status table date\")\n",
    "\n",
    "        for idx, row in datetime_table.iterrows():\n",
    "            start = row['TIMESTAMP_START']\n",
    "            end = row['TIMESTAMP_END']\n",
    "\n",
    "            table = alarm_table[(alarm_table['DT_SET']>=start) & (alarm_table['DT_SET']<=end)]\n",
    "            new_table = table[[\"Alarm ID\"]]\n",
    "            \n",
    "            tmp2 = []\n",
    "            for n in new_table.values: # this part is needed to achieve the data structure in X_seq, else it would fail\n",
    "                tmp2.append(n[0])\n",
    "            ORIG_ALARMS.append(tmp2)\n",
    "\n",
    "        return np.array(ORIG_ALARMS)\n",
    "    \n",
    "    def query_status(self):\n",
    "        try:\n",
    "            oracle_string = \"oracle+cx_oracle://{username}:{password}@{hostname}:{port}/{database}\"\n",
    "            engine = create_engine(\n",
    "                oracle_string.format(\n",
    "                    username = 'TFM4CEBERUS',\n",
    "                    password = 'TFM4CEBERUS',\n",
    "                    hostname = 'ome-db.bth.infineon.com',\n",
    "                    port = '1538',\n",
    "                    database = 'ome'\n",
    "                    )\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "\n",
    "        query = f\"\"\"select EQ_ID, TIMESTAMP_START, TIMESTAMP_END, DURATION, LEVEL3_NAME, LEVEL3 \n",
    "                from (SELECT\n",
    "                  eq.eq_id, eq.name, eq.eq_type_ident\n",
    "                , data.timestamp_start,data.timestamp_end\n",
    "                , ROUND((data.timestamp_end - data.timestamp_start)*24*60*60,0) AS Duration\n",
    "                , data.tr25_3_status,data.tr25_4_status,data.tr25_5_status,data.eq_status\n",
    "                , level5s.state_name\n",
    "                , level5.state_name Level5_Name, level5.state_sign Level5\n",
    "                , level4.state_name Level4_Name, level4.state_sign Level4\n",
    "                , level3.state_name Level3_Name, level3.state_sign Level3\n",
    "                ,mh.device\n",
    "                ,mh.package,\n",
    "                mh.lotid as lot,\n",
    "                mh.product,\n",
    "                mh.operation\n",
    "\n",
    "                FROM OMEDATA.EQUIPMENT_STATE_HISTORY data\n",
    "                , OMEADMIN.EQUIPMENT_INSTANCES eq\n",
    "                , V_EQ_STATES level5s\n",
    "                , OMEADMIN.DEF_STANDARD_STATEMODEL level5\n",
    "                , OMEADMIN.DEF_STANDARD_STATEMODEL level4\n",
    "                , OMEADMIN.DEF_STANDARD_STATEMODEL level3\n",
    "                , OMEDATA.METAKEY_HISTORY mh\n",
    "\n",
    "                WHERE data.eq_ident  = eq.eq_ident\n",
    "                AND  data.eq_status = level5s.state_ident(+)\n",
    "                AND level5.state_ident = data.tr25_5_status\n",
    "                AND level4.state_ident = data.tr25_4_status\n",
    "                AND level3.state_ident = data.tr25_3_status\n",
    "                AND  data.metakey_ident =mh.ident(+)\n",
    "                and data.timestamp_start > sysdate - 1500)\n",
    "                where eq_id = '{self.eq_id}'\n",
    "                ORDER BY TIMESTAMP_START\"\"\"\n",
    "\n",
    "        status = pd.read_sql(query, engine)\n",
    "        status.columns = map(lambda x: str(x).upper(), status.columns) \n",
    "\n",
    "        return status\n",
    "\n",
    "    def major_down(self, input_table, status_table, hour, threshold):\n",
    "        hour = pd.Timedelta(hours=hour)\n",
    "        major_down = []\n",
    "        \n",
    "        # timeframe table must be a subset of the status table to correctly determine major down\n",
    "        if status_table.iloc[0][\"TIMESTAMP_START\"] > input_table.iloc[0][\"TIMESTAMP_START\"]:\n",
    "            raise Exception(\"Timeframe table must be a subset of the status table\")\n",
    "        if status_table.iloc[len(status_table)-1][\"TIMESTAMP_START\"] <= input_table.iloc[len(input_table)-1][\"TIMESTAMP_START\"]:\n",
    "            raise Exception(\"Timeframe table must be a subset of the status table\")   \n",
    "            \n",
    "        for idx, row in input_table.iterrows():\n",
    "            start = row['TIMESTAMP_END']\n",
    "            end = start+hour\n",
    "            frame = status_table[(status_table['TIMESTAMP_START']>start) & (status_table['TIMESTAMP_START']<end)]\n",
    "            UD = frame.loc[frame['LEVEL3']=='UDT']\n",
    "\n",
    "            if len(UD) == 0: #no record within this 6 hours:\n",
    "                major_down.append(0)\n",
    "            else:\n",
    "                time_diff = (UD['TIMESTAMP_END']-UD['TIMESTAMP_START']).dt.seconds\n",
    "                if any(time_diff>threshold):\n",
    "                    major_down.append(1)\n",
    "                else:\n",
    "                    major_down.append(0)\n",
    "        return major_down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baa2d728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_full_alarm(eq_id, apc_alarm_table_path):\n",
    "    \n",
    "    alarm_apc = pd.read_excel(apc_alarm_table_path, engine=\"openpyxl\", usecols = \"B,C,D,F\")\n",
    "    start_date = sorted(alarm_apc[\"DT_SET\"].dt.date)[0]\n",
    "    start_date_STR = start_date.strftime(\"%d/%m/%Y\")\n",
    "    alarm_apc_new = alarm_apc.rename(columns={\"Equipment\": \"EQ_ID\", \"Alarm ID\": \"ALARM_ID\", \"DT_SET\": \"TIMESTAMP_START\", \"DT_CLEAR\":\"TIMESTAMP_END\"})\n",
    "    \n",
    "    try:\n",
    "        oracle_string = \"oracle+cx_oracle://{username}:{password}@{hostname}:{port}/{database}\"\n",
    "        engine = create_engine(\n",
    "            oracle_string.format(\n",
    "                username = 'TFM4CEBERUS',\n",
    "                password = 'TFM4CEBERUS',\n",
    "                hostname = 'ome-db.bth.infineon.com',\n",
    "                port = '1538',\n",
    "                database = 'ome'\n",
    "            )\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "    query = f\"\"\"SELECT * FROM (select ei.eq_id, ea.alarm_id, ac.name as alarm_class, ah.timestamp_start,ah.timestamp_end\n",
    "                from OMEADMIN.equipment_instances ei\n",
    "                join OMEADMIN.equipment_alarms ea on (ei.eq_type_ident(+) = ea.eq_type_ident)\n",
    "                join OMEDATA.ALARM_HISTORY ah on (ea.alarm_id = ah.alarm_id and ah.eq_ident = ei.eq_ident)\n",
    "                join OMEDATA.METAKEY_HISTORY mh on (ah.metakey_ident = mh.ident)\n",
    "                join OMEADMIN.EQUIPMENT_ALARM_CLASSES ac on (ac.IDENT = ea.ALARM_CLASS_IDENT and ac.eq_type_ident = ea.eq_type_ident)\n",
    "                where ah.timestamp_start > sysdate - 365\n",
    "                and ah.timestamp_start < sysdate -1)\n",
    "                WHERE EQ_ID = '{eq_id}'\n",
    "                ORDER BY TIMESTAMP_START\n",
    "                \"\"\"\n",
    "\n",
    "    alarm = pd.read_sql(query, engine)\n",
    "    alarm.columns = map(lambda x: str(x).upper(), alarm.columns)\n",
    "    \n",
    "    #map the alarm class\n",
    "    all_alarm_id = alarm[\"ALARM_ID\"].unique().tolist()\n",
    "    all_alarm_dict = dict.fromkeys(all_alarm_id, None)\n",
    "    \n",
    "    for key in all_alarm_dict.keys():\n",
    "        alarm_class = alarm[alarm.ALARM_ID ==key].iloc[0][\"ALARM_CLASS\"]\n",
    "        all_alarm_dict[key] = alarm_class\n",
    "    \n",
    "    alarm_apc_new[\"ALARM_CLASS\"] = alarm_apc_new[\"ALARM_ID\"].map(all_alarm_dict)\n",
    "    alarm_apc_new[\"ALARM_CLASS\"] = alarm_apc_new[\"ALARM_CLASS\"].fillna(value=\"Important Alarms\") #those alarms cannot be found in TFM must be the important alarms\n",
    "    \n",
    "    filtered_alarm = alarm.loc[alarm[\"TIMESTAMP_START\"].dt.date < start_date] # in case TFM have more data than APC, for earlier data, take from TFM\n",
    "    return pd.concat([filtered_alarm, alarm_apc_new], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7329595b",
   "metadata": {},
   "source": [
    "# Padding the alarm sequence to max length to help achieve consistent input length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d0c4a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wba124_fullalarm = find_full_alarm(\"WBA124\", \"Data/WBA124_FullAlarm.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7be09286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training by looking back 12 hours of alarm data\n",
      "Epoch 1/150\n",
      "189/189 [==============================] - 96s 490ms/step - loss: 0.6948 - accuracy: 0.5970 - val_loss: 0.6766 - val_accuracy: 0.8396\n",
      "Epoch 2/150\n",
      "189/189 [==============================] - 92s 487ms/step - loss: 0.6943 - accuracy: 0.5607 - val_loss: 0.7032 - val_accuracy: 0.1604\n",
      "Epoch 3/150\n",
      "189/189 [==============================] - 91s 483ms/step - loss: 0.6951 - accuracy: 0.6291 - val_loss: 0.6885 - val_accuracy: 0.8396\n",
      "Epoch 4/150\n",
      "189/189 [==============================] - 91s 482ms/step - loss: 0.6944 - accuracy: 0.5687 - val_loss: 0.7131 - val_accuracy: 0.1604\n",
      "Epoch 5/150\n",
      "189/189 [==============================] - 91s 482ms/step - loss: 0.6936 - accuracy: 0.3570 - val_loss: 0.6986 - val_accuracy: 0.1604\n",
      "Epoch 6/150\n",
      "189/189 [==============================] - 91s 483ms/step - loss: 0.6941 - accuracy: 0.2842 - val_loss: 0.6927 - val_accuracy: 0.8396\n",
      "Epoch 7/150\n",
      "189/189 [==============================] - 91s 481ms/step - loss: 0.6931 - accuracy: 0.4571 - val_loss: 0.6966 - val_accuracy: 0.1604\n",
      "Epoch 8/150\n",
      "189/189 [==============================] - 91s 482ms/step - loss: 0.6931 - accuracy: 0.2829 - val_loss: 0.6941 - val_accuracy: 0.1604\n",
      "Epoch 9/150\n",
      "189/189 [==============================] - 91s 483ms/step - loss: 0.6931 - accuracy: 0.2304 - val_loss: 0.6932 - val_accuracy: 0.1604\n",
      "Epoch 10/150\n",
      "189/189 [==============================] - 91s 480ms/step - loss: 0.6930 - accuracy: 0.7405 - val_loss: 0.6917 - val_accuracy: 0.8396\n",
      "Epoch 11/150\n",
      "189/189 [==============================] - 91s 480ms/step - loss: 0.6931 - accuracy: 0.7194 - val_loss: 0.6949 - val_accuracy: 0.1604\n",
      "Epoch 12/150\n",
      "189/189 [==============================] - 91s 484ms/step - loss: 0.6930 - accuracy: 0.2590 - val_loss: 0.6847 - val_accuracy: 0.8396\n",
      "Epoch 13/150\n",
      "189/189 [==============================] - 91s 481ms/step - loss: 0.6933 - accuracy: 0.7432 - val_loss: 0.6940 - val_accuracy: 0.1604\n",
      "Epoch 14/150\n",
      "189/189 [==============================] - 91s 481ms/step - loss: 0.6933 - accuracy: 0.2464 - val_loss: 0.6852 - val_accuracy: 0.8396\n",
      "Epoch 15/150\n",
      "189/189 [==============================] - 91s 483ms/step - loss: 0.6945 - accuracy: 0.4197 - val_loss: 0.6936 - val_accuracy: 0.1604\n",
      "Epoch 16/150\n",
      "189/189 [==============================] - 92s 487ms/step - loss: 0.6931 - accuracy: 0.3020 - val_loss: 0.6976 - val_accuracy: 0.1604\n",
      "Epoch 17/150\n",
      "189/189 [==============================] - 91s 484ms/step - loss: 0.6933 - accuracy: 0.4893 - val_loss: 0.6910 - val_accuracy: 0.8396\n",
      "Epoch 18/150\n",
      "189/189 [==============================] - 91s 483ms/step - loss: 0.6939 - accuracy: 0.5954 - val_loss: 0.7039 - val_accuracy: 0.1604\n",
      "Epoch 19/150\n",
      "189/189 [==============================] - 91s 483ms/step - loss: 0.6933 - accuracy: 0.2120 - val_loss: 0.6038 - val_accuracy: 0.8396\n",
      "Epoch 20/150\n",
      "189/189 [==============================] - 91s 483ms/step - loss: 0.6972 - accuracy: 0.5257 - val_loss: 0.6965 - val_accuracy: 0.1604\n",
      "Epoch 21/150\n",
      "189/189 [==============================] - 91s 483ms/step - loss: 0.6934 - accuracy: 0.6394 - val_loss: 0.6867 - val_accuracy: 0.8396\n",
      "Epoch 22/150\n",
      "189/189 [==============================] - 91s 484ms/step - loss: 0.6933 - accuracy: 0.7689 - val_loss: 0.6873 - val_accuracy: 0.8396\n",
      "Epoch 23/150\n",
      "189/189 [==============================] - 91s 482ms/step - loss: 0.6932 - accuracy: 0.3487 - val_loss: 0.6927 - val_accuracy: 0.8396\n",
      "Epoch 24/150\n",
      "189/189 [==============================] - 91s 482ms/step - loss: 0.6930 - accuracy: 0.5102 - val_loss: 0.6915 - val_accuracy: 0.8396\n",
      "Epoch 25/150\n",
      "189/189 [==============================] - 91s 482ms/step - loss: 0.6931 - accuracy: 0.2241 - val_loss: 0.6911 - val_accuracy: 0.8396\n",
      "Epoch 26/150\n",
      "189/189 [==============================] - 91s 483ms/step - loss: 0.6930 - accuracy: 0.8604 - val_loss: 0.6928 - val_accuracy: 0.8396\n",
      "Epoch 27/150\n",
      "189/189 [==============================] - 91s 484ms/step - loss: 0.6931 - accuracy: 0.3952 - val_loss: 0.6956 - val_accuracy: 0.1604\n",
      "Epoch 28/150\n",
      "189/189 [==============================] - 91s 483ms/step - loss: 0.6930 - accuracy: 0.5917 - val_loss: 0.6947 - val_accuracy: 0.1604\n",
      "Epoch 29/150\n",
      "189/189 [==============================] - 91s 484ms/step - loss: 0.6931 - accuracy: 0.1672 - val_loss: 0.6946 - val_accuracy: 0.1604\n",
      "Epoch 30/150\n",
      "189/189 [==============================] - 91s 483ms/step - loss: 0.6931 - accuracy: 0.4383 - val_loss: 0.6936 - val_accuracy: 0.1604\n",
      "Epoch 31/150\n",
      "189/189 [==============================] - 92s 485ms/step - loss: 0.6931 - accuracy: 0.2085 - val_loss: 0.6930 - val_accuracy: 0.8396\n",
      "Epoch 32/150\n",
      "189/189 [==============================] - 92s 488ms/step - loss: 0.6931 - accuracy: 0.7166 - val_loss: 0.6936 - val_accuracy: 0.1604\n",
      "Epoch 33/150\n",
      "189/189 [==============================] - 92s 487ms/step - loss: 0.6933 - accuracy: 0.8614 - val_loss: 0.6927 - val_accuracy: 0.8396\n",
      "Epoch 34/150\n",
      "189/189 [==============================] - 91s 483ms/step - loss: 0.6930 - accuracy: 0.3140 - val_loss: 0.6945 - val_accuracy: 0.1604\n",
      "Epoch 35/150\n",
      "189/189 [==============================] - 92s 485ms/step - loss: 0.6930 - accuracy: 0.4976 - val_loss: 0.6929 - val_accuracy: 0.8396\n",
      "Epoch 36/150\n",
      "189/189 [==============================] - 92s 484ms/step - loss: 0.6930 - accuracy: 0.1650 - val_loss: 0.6927 - val_accuracy: 0.8396\n",
      "Epoch 37/150\n",
      "189/189 [==============================] - 91s 481ms/step - loss: 0.6931 - accuracy: 0.7840 - val_loss: 0.6932 - val_accuracy: 0.1604\n",
      "Epoch 38/150\n",
      "189/189 [==============================] - 91s 481ms/step - loss: 0.6930 - accuracy: 0.8713 - val_loss: 0.6925 - val_accuracy: 0.8396\n",
      "Epoch 39/150\n",
      "189/189 [==============================] - 91s 482ms/step - loss: 0.6933 - accuracy: 0.3367 - val_loss: 0.6704 - val_accuracy: 0.8396\n",
      "Epoch 40/150\n",
      "189/189 [==============================] - 91s 482ms/step - loss: 0.6938 - accuracy: 0.5109 - val_loss: 0.6890 - val_accuracy: 0.8396\n",
      "Epoch 41/150\n",
      "189/189 [==============================] - 91s 483ms/step - loss: 0.6932 - accuracy: 0.7933 - val_loss: 0.6901 - val_accuracy: 0.8396\n",
      "Epoch 42/150\n",
      "189/189 [==============================] - 91s 480ms/step - loss: 0.6930 - accuracy: 0.2283 - val_loss: 0.6934 - val_accuracy: 0.1604\n",
      "Epoch 43/150\n",
      "189/189 [==============================] - 91s 481ms/step - loss: 0.6935 - accuracy: 0.7606 - val_loss: 0.6924 - val_accuracy: 0.8396\n",
      "Epoch 44/150\n",
      "189/189 [==============================] - 91s 481ms/step - loss: 0.6930 - accuracy: 0.2092 - val_loss: 0.6932 - val_accuracy: 0.1604\n",
      "Epoch 45/150\n",
      "189/189 [==============================] - 91s 482ms/step - loss: 0.6934 - accuracy: 0.2525 - val_loss: 0.6879 - val_accuracy: 0.8396\n",
      "Epoch 46/150\n",
      "189/189 [==============================] - 91s 484ms/step - loss: 0.6933 - accuracy: 0.4561 - val_loss: 0.6916 - val_accuracy: 0.8396\n",
      "Epoch 47/150\n",
      "189/189 [==============================] - 91s 481ms/step - loss: 0.6940 - accuracy: 0.7081 - val_loss: 0.6952 - val_accuracy: 0.1604\n",
      "Epoch 48/150\n",
      "189/189 [==============================] - 91s 481ms/step - loss: 0.6931 - accuracy: 0.4297 - val_loss: 0.6947 - val_accuracy: 0.1604\n",
      "Epoch 49/150\n",
      "189/189 [==============================] - 91s 479ms/step - loss: 0.6930 - accuracy: 0.3360 - val_loss: 0.6951 - val_accuracy: 0.1604\n",
      "Epoch 50/150\n",
      "189/189 [==============================] - 92s 488ms/step - loss: 0.6930 - accuracy: 0.1147 - val_loss: 0.6975 - val_accuracy: 0.1604\n",
      "Epoch 51/150\n",
      "189/189 [==============================] - 91s 482ms/step - loss: 0.6930 - accuracy: 0.0933 - val_loss: 0.6943 - val_accuracy: 0.1604\n",
      "Epoch 52/150\n",
      "189/189 [==============================] - 91s 483ms/step - loss: 0.6930 - accuracy: 0.3651 - val_loss: 0.6946 - val_accuracy: 0.1604\n",
      "Epoch 53/150\n",
      "189/189 [==============================] - 91s 483ms/step - loss: 0.6930 - accuracy: 0.6445 - val_loss: 0.6954 - val_accuracy: 0.1604\n",
      "Epoch 54/150\n",
      "189/189 [==============================] - 91s 481ms/step - loss: 0.6930 - accuracy: 0.2482 - val_loss: 0.6945 - val_accuracy: 0.1604\n",
      "Epoch 55/150\n",
      "189/189 [==============================] - 91s 481ms/step - loss: 0.6931 - accuracy: 0.6354 - val_loss: 0.6937 - val_accuracy: 0.1604\n",
      "Epoch 56/150\n",
      "189/189 [==============================] - 91s 484ms/step - loss: 0.6930 - accuracy: 0.1953 - val_loss: 0.6935 - val_accuracy: 0.1604\n",
      "Epoch 57/150\n",
      "189/189 [==============================] - 92s 485ms/step - loss: 0.6930 - accuracy: 0.2597 - val_loss: 0.6936 - val_accuracy: 0.1604\n",
      "Epoch 58/150\n",
      "189/189 [==============================] - 91s 480ms/step - loss: 0.6931 - accuracy: 0.8582 - val_loss: 0.6939 - val_accuracy: 0.1604\n",
      "Epoch 59/150\n",
      "189/189 [==============================] - 91s 484ms/step - loss: 0.6930 - accuracy: 0.0936 - val_loss: 0.6947 - val_accuracy: 0.1604\n",
      "Epoch 60/150\n",
      "189/189 [==============================] - 92s 486ms/step - loss: 0.6930 - accuracy: 0.0931 - val_loss: 0.6937 - val_accuracy: 0.1604\n",
      "Epoch 61/150\n",
      "189/189 [==============================] - 91s 481ms/step - loss: 0.6931 - accuracy: 0.5258 - val_loss: 0.6912 - val_accuracy: 0.8396\n",
      "Epoch 62/150\n",
      "189/189 [==============================] - 91s 482ms/step - loss: 0.7305 - accuracy: 0.7251 - val_loss: 0.6830 - val_accuracy: 0.8396\n",
      "Epoch 63/150\n",
      "189/189 [==============================] - 91s 480ms/step - loss: 0.6971 - accuracy: 0.3648 - val_loss: 0.7019 - val_accuracy: 0.1604\n",
      "Epoch 64/150\n",
      "189/189 [==============================] - 91s 481ms/step - loss: 0.6936 - accuracy: 0.2396 - val_loss: 0.6940 - val_accuracy: 0.1604\n",
      "Epoch 65/150\n",
      "189/189 [==============================] - 91s 483ms/step - loss: 0.6935 - accuracy: 0.5569 - val_loss: 0.6920 - val_accuracy: 0.8396\n",
      "Epoch 66/150\n",
      "189/189 [==============================] - 91s 482ms/step - loss: 0.6933 - accuracy: 0.4900 - val_loss: 0.6873 - val_accuracy: 0.8396\n",
      "Epoch 67/150\n",
      "189/189 [==============================] - 91s 482ms/step - loss: 0.6930 - accuracy: 0.4991 - val_loss: 0.6893 - val_accuracy: 0.8396\n",
      "Epoch 68/150\n",
      "189/189 [==============================] - 92s 485ms/step - loss: 0.6939 - accuracy: 0.5472 - val_loss: 0.6887 - val_accuracy: 0.8396\n",
      "Epoch 69/150\n",
      "189/189 [==============================] - 92s 485ms/step - loss: 0.6935 - accuracy: 0.7226 - val_loss: 0.6892 - val_accuracy: 0.8396\n",
      "Epoch 70/150\n",
      "189/189 [==============================] - 92s 485ms/step - loss: 0.6939 - accuracy: 0.4621 - val_loss: 0.6931 - val_accuracy: 0.1673\n",
      "Epoch 71/150\n",
      "189/189 [==============================] - 91s 482ms/step - loss: 0.6935 - accuracy: 0.4413 - val_loss: 0.6919 - val_accuracy: 0.8396\n",
      "Epoch 72/150\n",
      "189/189 [==============================] - 91s 484ms/step - loss: 0.6941 - accuracy: 0.5081 - val_loss: 0.6889 - val_accuracy: 0.8396\n",
      "Epoch 73/150\n",
      "189/189 [==============================] - 91s 483ms/step - loss: 0.6932 - accuracy: 0.5514 - val_loss: 0.7058 - val_accuracy: 0.1604\n",
      "Epoch 74/150\n",
      "189/189 [==============================] - 91s 483ms/step - loss: 0.6942 - accuracy: 0.5280 - val_loss: 0.6962 - val_accuracy: 0.1604\n",
      "Epoch 75/150\n",
      "189/189 [==============================] - 91s 482ms/step - loss: 0.6936 - accuracy: 0.3631 - val_loss: 0.6920 - val_accuracy: 0.8396\n",
      "Epoch 76/150\n",
      "189/189 [==============================] - 91s 483ms/step - loss: 0.6940 - accuracy: 0.4315 - val_loss: 0.6999 - val_accuracy: 0.1604\n",
      "Epoch 77/150\n",
      "189/189 [==============================] - 92s 487ms/step - loss: 0.6928 - accuracy: 0.3696 - val_loss: 0.6675 - val_accuracy: 0.8396\n",
      "Epoch 78/150\n",
      "189/189 [==============================] - 92s 485ms/step - loss: 0.6944 - accuracy: 0.3721 - val_loss: 0.7025 - val_accuracy: 0.1604\n",
      "Epoch 79/150\n",
      "189/189 [==============================] - 92s 487ms/step - loss: 0.6936 - accuracy: 0.3712 - val_loss: 0.6944 - val_accuracy: 0.1604\n",
      "Epoch 80/150\n",
      "189/189 [==============================] - 92s 485ms/step - loss: 0.6933 - accuracy: 0.2344 - val_loss: 0.6963 - val_accuracy: 0.1604\n",
      "Epoch 81/150\n",
      "189/189 [==============================] - 92s 485ms/step - loss: 0.6932 - accuracy: 0.5633 - val_loss: 0.6979 - val_accuracy: 0.1604\n",
      "Epoch 82/150\n",
      "189/189 [==============================] - 92s 485ms/step - loss: 0.6934 - accuracy: 0.1275 - val_loss: 0.6948 - val_accuracy: 0.1604\n",
      "Epoch 83/150\n",
      "189/189 [==============================] - 92s 485ms/step - loss: 0.6931 - accuracy: 0.5816 - val_loss: 0.6953 - val_accuracy: 0.1604\n",
      "Epoch 84/150\n",
      "189/189 [==============================] - 92s 485ms/step - loss: 0.6935 - accuracy: 0.5618 - val_loss: 0.6933 - val_accuracy: 0.1604\n",
      "Epoch 85/150\n",
      "189/189 [==============================] - 91s 484ms/step - loss: 0.6933 - accuracy: 0.5555 - val_loss: 0.6973 - val_accuracy: 0.1604\n",
      "Epoch 86/150\n",
      "189/189 [==============================] - 92s 486ms/step - loss: 0.6938 - accuracy: 0.2090 - val_loss: 0.6969 - val_accuracy: 0.1604\n",
      "Epoch 87/150\n",
      "189/189 [==============================] - 91s 484ms/step - loss: 0.6930 - accuracy: 0.3583 - val_loss: 0.6985 - val_accuracy: 0.1604\n",
      "Epoch 88/150\n",
      "189/189 [==============================] - 91s 480ms/step - loss: 0.6937 - accuracy: 0.3741 - val_loss: 0.6968 - val_accuracy: 0.1604\n",
      "Epoch 89/150\n",
      "189/189 [==============================] - 92s 485ms/step - loss: 0.6930 - accuracy: 0.1971 - val_loss: 0.6959 - val_accuracy: 0.1604\n",
      "Epoch 90/150\n",
      "189/189 [==============================] - 91s 483ms/step - loss: 0.6932 - accuracy: 0.2339 - val_loss: 0.6941 - val_accuracy: 0.1604\n",
      "Epoch 91/150\n",
      "189/189 [==============================] - 92s 488ms/step - loss: 0.6934 - accuracy: 0.3628 - val_loss: 0.6890 - val_accuracy: 0.8396\n",
      "Epoch 92/150\n",
      "189/189 [==============================] - 92s 489ms/step - loss: 0.6930 - accuracy: 0.5283 - val_loss: 0.6918 - val_accuracy: 0.8396\n",
      "Epoch 93/150\n",
      "189/189 [==============================] - 92s 489ms/step - loss: 0.6930 - accuracy: 0.8614 - val_loss: 0.6930 - val_accuracy: 0.8396\n",
      "Epoch 94/150\n",
      "189/189 [==============================] - 91s 481ms/step - loss: 0.6931 - accuracy: 0.5021 - val_loss: 0.6914 - val_accuracy: 0.8396\n",
      "Epoch 95/150\n",
      "189/189 [==============================] - 91s 484ms/step - loss: 0.6932 - accuracy: 0.6020 - val_loss: 0.6939 - val_accuracy: 0.1604\n",
      "Epoch 96/150\n",
      "189/189 [==============================] - 93s 490ms/step - loss: 0.6930 - accuracy: 0.7163 - val_loss: 0.6941 - val_accuracy: 0.1604\n",
      "Epoch 97/150\n",
      "189/189 [==============================] - 92s 487ms/step - loss: 0.6938 - accuracy: 0.2459 - val_loss: 0.6960 - val_accuracy: 0.1604\n",
      "Epoch 98/150\n",
      "189/189 [==============================] - 91s 484ms/step - loss: 0.6931 - accuracy: 0.4777 - val_loss: 0.6953 - val_accuracy: 0.1604\n",
      "Epoch 99/150\n",
      "189/189 [==============================] - 91s 484ms/step - loss: 0.6935 - accuracy: 0.2424 - val_loss: 0.6960 - val_accuracy: 0.1604\n",
      "Epoch 100/150\n",
      "189/189 [==============================] - 92s 488ms/step - loss: 0.6933 - accuracy: 0.6298 - val_loss: 0.6926 - val_accuracy: 0.8396\n",
      "Epoch 101/150\n",
      "189/189 [==============================] - 92s 487ms/step - loss: 0.6928 - accuracy: 0.2401 - val_loss: 0.6987 - val_accuracy: 0.1604\n",
      "Epoch 102/150\n",
      "189/189 [==============================] - 92s 488ms/step - loss: 0.6942 - accuracy: 0.2490 - val_loss: 0.6446 - val_accuracy: 0.8396\n",
      "Epoch 103/150\n",
      "189/189 [==============================] - 92s 487ms/step - loss: 0.6934 - accuracy: 0.1997 - val_loss: 0.7028 - val_accuracy: 0.1604\n",
      "Epoch 104/150\n",
      "189/189 [==============================] - 92s 485ms/step - loss: 0.6932 - accuracy: 0.1569 - val_loss: 0.6973 - val_accuracy: 0.1604\n",
      "Epoch 105/150\n",
      "189/189 [==============================] - 91s 484ms/step - loss: 0.6928 - accuracy: 0.3621 - val_loss: 0.7018 - val_accuracy: 0.1604\n",
      "Epoch 106/150\n",
      "189/189 [==============================] - 92s 485ms/step - loss: 0.6928 - accuracy: 0.3940 - val_loss: 0.7122 - val_accuracy: 0.1604\n",
      "Epoch 107/150\n",
      "189/189 [==============================] - 92s 488ms/step - loss: 0.6932 - accuracy: 0.2175 - val_loss: 0.7059 - val_accuracy: 0.1604\n",
      "Epoch 108/150\n",
      "189/189 [==============================] - 92s 487ms/step - loss: 0.6939 - accuracy: 0.2202 - val_loss: 0.7017 - val_accuracy: 0.1604\n",
      "Epoch 109/150\n",
      "189/189 [==============================] - 92s 485ms/step - loss: 0.6936 - accuracy: 0.3023 - val_loss: 0.6980 - val_accuracy: 0.1604\n",
      "Epoch 110/150\n",
      "189/189 [==============================] - 92s 486ms/step - loss: 0.6922 - accuracy: 0.2457 - val_loss: 0.7008 - val_accuracy: 0.1604\n",
      "Epoch 111/150\n",
      "189/189 [==============================] - 93s 495ms/step - loss: 0.6943 - accuracy: 0.3624 - val_loss: 0.7027 - val_accuracy: 0.1604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/150\n",
      "189/189 [==============================] - 91s 484ms/step - loss: 0.6934 - accuracy: 0.3350 - val_loss: 0.7025 - val_accuracy: 0.1604\n",
      "Epoch 113/150\n",
      "189/189 [==============================] - 90s 479ms/step - loss: 0.6930 - accuracy: 0.2720 - val_loss: 0.7009 - val_accuracy: 0.1604\n",
      "Epoch 114/150\n",
      "189/189 [==============================] - 78s 412ms/step - loss: 0.6931 - accuracy: 0.3246 - val_loss: 0.6988 - val_accuracy: 0.1604\n",
      "Epoch 115/150\n",
      "189/189 [==============================] - 78s 414ms/step - loss: 0.6932 - accuracy: 0.1602 - val_loss: 0.6963 - val_accuracy: 0.1604\n",
      "Epoch 116/150\n",
      "189/189 [==============================] - 87s 461ms/step - loss: 0.6931 - accuracy: 0.1753 - val_loss: 0.6938 - val_accuracy: 0.1604\n",
      "Epoch 117/150\n",
      "189/189 [==============================] - 92s 486ms/step - loss: 0.6930 - accuracy: 0.6674 - val_loss: 0.6929 - val_accuracy: 0.8396\n",
      "Epoch 118/150\n",
      "189/189 [==============================] - 92s 485ms/step - loss: 0.6936 - accuracy: 0.5743 - val_loss: 0.6954 - val_accuracy: 0.1604\n",
      "Epoch 119/150\n",
      "189/189 [==============================] - 91s 482ms/step - loss: 0.6930 - accuracy: 0.4011 - val_loss: 0.6949 - val_accuracy: 0.1604\n",
      "Epoch 120/150\n",
      "189/189 [==============================] - 91s 480ms/step - loss: 0.6935 - accuracy: 0.7534 - val_loss: 0.6965 - val_accuracy: 0.1604\n",
      "Epoch 121/150\n",
      "189/189 [==============================] - 92s 487ms/step - loss: 0.6931 - accuracy: 0.2796 - val_loss: 0.6985 - val_accuracy: 0.1604\n",
      "Epoch 122/150\n",
      "189/189 [==============================] - 92s 484ms/step - loss: 0.6932 - accuracy: 0.3849 - val_loss: 0.6955 - val_accuracy: 0.1604\n",
      "Epoch 123/150\n",
      "189/189 [==============================] - 91s 484ms/step - loss: 0.6929 - accuracy: 0.1091 - val_loss: 0.6945 - val_accuracy: 0.1604\n",
      "Epoch 124/150\n",
      "189/189 [==============================] - 92s 486ms/step - loss: 0.6933 - accuracy: 0.3045 - val_loss: 0.6925 - val_accuracy: 0.8396\n",
      "Epoch 125/150\n",
      "189/189 [==============================] - 92s 485ms/step - loss: 0.6933 - accuracy: 0.8094 - val_loss: 0.6940 - val_accuracy: 0.1604\n",
      "Epoch 126/150\n",
      "189/189 [==============================] - 92s 486ms/step - loss: 0.6935 - accuracy: 0.2824 - val_loss: 0.6985 - val_accuracy: 0.1604\n",
      "Epoch 127/150\n",
      "189/189 [==============================] - 91s 479ms/step - loss: 0.6942 - accuracy: 0.4513 - val_loss: 0.7017 - val_accuracy: 0.1604\n",
      "Epoch 128/150\n",
      "189/189 [==============================] - 91s 483ms/step - loss: 0.6936 - accuracy: 0.1698 - val_loss: 0.7065 - val_accuracy: 0.1604\n",
      "Epoch 129/150\n",
      "189/189 [==============================] - 91s 482ms/step - loss: 0.6933 - accuracy: 0.2258 - val_loss: 0.6961 - val_accuracy: 0.1604\n",
      "Epoch 130/150\n",
      "189/189 [==============================] - 91s 482ms/step - loss: 0.6933 - accuracy: 0.6361 - val_loss: 0.6981 - val_accuracy: 0.1604\n",
      "Epoch 131/150\n",
      "189/189 [==============================] - 92s 486ms/step - loss: 0.6931 - accuracy: 0.1511 - val_loss: 0.6980 - val_accuracy: 0.1604\n",
      "Epoch 132/150\n",
      "189/189 [==============================] - 91s 484ms/step - loss: 0.6922 - accuracy: 0.6429 - val_loss: 0.6939 - val_accuracy: 0.1604\n",
      "Epoch 133/150\n",
      "189/189 [==============================] - 91s 482ms/step - loss: 0.6939 - accuracy: 0.3678 - val_loss: 0.7025 - val_accuracy: 0.1604\n",
      "Epoch 134/150\n",
      "189/189 [==============================] - 92s 486ms/step - loss: 0.6947 - accuracy: 0.2248 - val_loss: 0.7023 - val_accuracy: 0.1604\n",
      "Epoch 135/150\n",
      "189/189 [==============================] - 91s 481ms/step - loss: 0.6934 - accuracy: 0.1325 - val_loss: 0.6967 - val_accuracy: 0.1604\n",
      "Epoch 136/150\n",
      "189/189 [==============================] - 92s 485ms/step - loss: 0.6933 - accuracy: 0.3349 - val_loss: 0.6952 - val_accuracy: 0.1604\n",
      "Epoch 137/150\n",
      "189/189 [==============================] - 91s 484ms/step - loss: 0.6930 - accuracy: 0.1171 - val_loss: 0.6948 - val_accuracy: 0.1604\n",
      "Epoch 138/150\n",
      "189/189 [==============================] - 92s 485ms/step - loss: 0.6930 - accuracy: 0.3734 - val_loss: 0.6949 - val_accuracy: 0.1604\n",
      "Epoch 139/150\n",
      "189/189 [==============================] - 91s 481ms/step - loss: 0.6930 - accuracy: 0.3151 - val_loss: 0.6966 - val_accuracy: 0.1604\n",
      "Epoch 140/150\n",
      "189/189 [==============================] - 92s 487ms/step - loss: 0.6936 - accuracy: 0.3121 - val_loss: 0.6959 - val_accuracy: 0.1604\n",
      "Epoch 141/150\n",
      "189/189 [==============================] - 92s 486ms/step - loss: 0.6931 - accuracy: 0.1290 - val_loss: 0.6942 - val_accuracy: 0.1604\n",
      "Epoch 142/150\n",
      "189/189 [==============================] - 91s 484ms/step - loss: 0.6930 - accuracy: 0.7242 - val_loss: 0.6940 - val_accuracy: 0.1604\n",
      "Epoch 143/150\n",
      "189/189 [==============================] - 92s 485ms/step - loss: 0.6931 - accuracy: 0.7637 - val_loss: 0.6952 - val_accuracy: 0.1604\n",
      "Epoch 144/150\n",
      "189/189 [==============================] - 91s 482ms/step - loss: 0.6932 - accuracy: 0.1491 - val_loss: 0.6951 - val_accuracy: 0.1604\n",
      "Epoch 145/150\n",
      "189/189 [==============================] - 91s 484ms/step - loss: 0.6933 - accuracy: 0.2735 - val_loss: 0.6934 - val_accuracy: 0.1604\n",
      "Epoch 146/150\n",
      "189/189 [==============================] - 91s 482ms/step - loss: 0.6931 - accuracy: 0.3701 - val_loss: 0.6921 - val_accuracy: 0.8396\n",
      "Epoch 147/150\n",
      "189/189 [==============================] - 92s 485ms/step - loss: 0.6929 - accuracy: 0.7163 - val_loss: 0.6957 - val_accuracy: 0.1604\n",
      "Epoch 148/150\n",
      "189/189 [==============================] - 92s 485ms/step - loss: 0.6933 - accuracy: 0.2984 - val_loss: 0.6950 - val_accuracy: 0.1604\n",
      "Epoch 149/150\n",
      "189/189 [==============================] - 91s 483ms/step - loss: 0.6931 - accuracy: 0.7008 - val_loss: 0.6935 - val_accuracy: 0.1604\n",
      "Epoch 150/150\n",
      "189/189 [==============================] - 92s 486ms/step - loss: 0.6931 - accuracy: 0.6631 - val_loss: 0.6956 - val_accuracy: 0.1604\n",
      "27/27 [==============================] - 4s 141ms/step - loss: 0.6961 - accuracy: 0.0894\n",
      "Training took a total of 13819 seconds\n"
     ]
    }
   ],
   "source": [
    "# seq_result = {}\n",
    "# lookback = [6,12,18,24,48,72]\n",
    "# for hour in lookback:\n",
    "hour = 12\n",
    "start = datetime.now()\n",
    "print(f\"Training by looking back {hour} hours of alarm data\")\n",
    "wba124 = seven_days_LSTM(\"WBA124\", wba124_fullalarm, hour, 3)\n",
    "\n",
    "# pad the alarm to train on LSTM\n",
    "unpadded_arr = wba124.X_seq\n",
    "padded_alarm = np.zeros([len(unpadded_arr),len(max(unpadded_arr,key = lambda x: len(x)))])\n",
    "for i,j in enumerate(unpadded_arr):\n",
    "     padded_alarm[i][0:len(j)] = j\n",
    "\n",
    "# scale training data for the model to learn faster, because max length is taking the model too long to train per epoch\n",
    "scaler = StandardScaler()\n",
    "scaled_X_seq = scaler.fit_transform(padded_alarm)\n",
    "\n",
    "#train_val_test split\n",
    "val_percentage = 0.2\n",
    "test_percentage = 0.1\n",
    "\n",
    "test_index = int(len(scaled_X_seq) * (1-test_percentage))\n",
    "val_index = int(len(scaled_X_seq) * (1- val_percentage - test_percentage))\n",
    "\n",
    "X_train_seq, X_val_seq, X_test_seq = scaled_X_seq[:val_index], scaled_X_seq[val_index:test_index], scaled_X_seq[test_index:]\n",
    "y_train_seq, y_val_seq, y_test_seq = wba124.y[:val_index], wba124.y[val_index:test_index], wba124.y[test_index:]\n",
    "\n",
    "X_train_seq = X_train_seq.reshape(X_train_seq.shape[0], X_train_seq.shape[1], 1)\n",
    "X_val_seq = X_val_seq.reshape(X_val_seq.shape[0], X_val_seq.shape[1], 1)\n",
    "X_test_seq = X_test_seq.reshape(X_test_seq.shape[0], X_test_seq.shape[1], 1)\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                             np.unique(y_train_seq),\n",
    "                                             y_train_seq)\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "#need to reinitialize the model because x_train_seq changes in shape\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(X_train_seq.shape[1:]), return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(LSTM(64, input_shape=(X_train_seq.shape[1:])))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(32, activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train_seq, y_train_seq, \n",
    "                batch_size=BATCH_SIZE, epochs=EPOCHS, \n",
    "                validation_data=(X_val_seq, y_val_seq), \n",
    "                class_weight=class_weights_dict)\n",
    "evaluate = model.evaluate(X_test_seq, y_test_seq) #loss, mse\n",
    "\n",
    "end = datetime.now()\n",
    "time = end - start\n",
    "print(f\"Training took a total of {time.seconds} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a9bed4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, [0.6960725784301758, 0.08943089097738266])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hour, evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28605e22",
   "metadata": {},
   "source": [
    "## Padding the sequence with the average length\n",
    "#### https://towardsdatascience.com/using-tensorflow-ragged-tensors-2af07849a7bd\n",
    "#### Apparently can also greatly help boost accracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb3be33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wba124_fullalarm = find_full_alarm(\"WBA124\", \"Data/WBA124_FullAlarm.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70942ac3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training by looking back 6 hours of alarm data\n",
      "Epoch 1/150\n",
      "189/189 [==============================] - 7s 18ms/step - loss: 0.6948 - accuracy: 0.6301 - val_loss: 0.6975 - val_accuracy: 0.1649\n",
      "Epoch 2/150\n",
      "189/189 [==============================] - 2s 13ms/step - loss: 0.6955 - accuracy: 0.4774 - val_loss: 0.6836 - val_accuracy: 0.7718\n",
      "Epoch 3/150\n",
      "189/189 [==============================] - 2s 13ms/step - loss: 0.6945 - accuracy: 0.4411 - val_loss: 0.6766 - val_accuracy: 0.7816\n",
      "Epoch 4/150\n",
      "189/189 [==============================] - 2s 13ms/step - loss: 0.6948 - accuracy: 0.4905 - val_loss: 0.6793 - val_accuracy: 0.7729\n",
      "Epoch 5/150\n",
      "189/189 [==============================] - 2s 13ms/step - loss: 0.6951 - accuracy: 0.3723 - val_loss: 0.6520 - val_accuracy: 0.7898\n",
      "Epoch 6/150\n",
      "189/189 [==============================] - 2s 13ms/step - loss: 0.6938 - accuracy: 0.3476 - val_loss: 0.7000 - val_accuracy: 0.1626\n",
      "Epoch 7/150\n",
      "189/189 [==============================] - 2s 13ms/step - loss: 0.6943 - accuracy: 0.4866 - val_loss: 0.7106 - val_accuracy: 0.1603\n",
      "Epoch 8/150\n",
      "189/189 [==============================] - 2s 13ms/step - loss: 0.6932 - accuracy: 0.3473 - val_loss: 0.6687 - val_accuracy: 0.8043\n",
      "Epoch 9/150\n",
      "189/189 [==============================] - 3s 13ms/step - loss: 0.6926 - accuracy: 0.5315 - val_loss: 0.6927 - val_accuracy: 0.7660\n",
      "Epoch 10/150\n",
      "189/189 [==============================] - 2s 12ms/step - loss: 0.6938 - accuracy: 0.4047 - val_loss: 0.7122 - val_accuracy: 0.1597\n",
      "Epoch 11/150\n",
      "189/189 [==============================] - 2s 12ms/step - loss: 0.6925 - accuracy: 0.4226 - val_loss: 0.6741 - val_accuracy: 0.7706\n",
      "Epoch 12/150\n",
      "189/189 [==============================] - 2s 12ms/step - loss: 0.6936 - accuracy: 0.4879 - val_loss: 0.6809 - val_accuracy: 0.7741\n",
      "Epoch 13/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.6926 - accuracy: 0.4748 - val_loss: 0.6616 - val_accuracy: 0.8130\n",
      "Epoch 14/150\n",
      "189/189 [==============================] - 2s 12ms/step - loss: 0.6934 - accuracy: 0.4992 - val_loss: 0.6717 - val_accuracy: 0.7956\n",
      "Epoch 15/150\n",
      "189/189 [==============================] - 2s 12ms/step - loss: 0.6929 - accuracy: 0.5110 - val_loss: 0.6948 - val_accuracy: 0.7747\n",
      "Epoch 16/150\n",
      "189/189 [==============================] - 2s 11ms/step - loss: 0.6950 - accuracy: 0.4482 - val_loss: 0.7011 - val_accuracy: 0.1597\n",
      "Epoch 17/150\n",
      "189/189 [==============================] - 2s 13ms/step - loss: 0.6923 - accuracy: 0.3984 - val_loss: 0.6710 - val_accuracy: 0.7915\n",
      "Epoch 18/150\n",
      "189/189 [==============================] - 2s 13ms/step - loss: 0.6932 - accuracy: 0.5045 - val_loss: 0.6752 - val_accuracy: 0.7863\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6826 - accuracy: 0.6365\n",
      "Training took a total of 170 seconds\n",
      "Training by looking back 12 hours of alarm data\n",
      "Epoch 1/150\n",
      "189/189 [==============================] - 9s 31ms/step - loss: 0.6986 - accuracy: 0.4835 - val_loss: 0.7120 - val_accuracy: 0.2138\n",
      "Epoch 2/150\n",
      "189/189 [==============================] - 5s 26ms/step - loss: 0.6953 - accuracy: 0.5132 - val_loss: 0.7598 - val_accuracy: 0.1702\n",
      "Epoch 3/150\n",
      "189/189 [==============================] - 5s 26ms/step - loss: 0.6933 - accuracy: 0.5811 - val_loss: 0.6939 - val_accuracy: 0.1993\n",
      "Epoch 4/150\n",
      "189/189 [==============================] - 5s 26ms/step - loss: 0.6942 - accuracy: 0.4911 - val_loss: 0.6556 - val_accuracy: 0.8013\n",
      "Epoch 5/150\n",
      "189/189 [==============================] - 5s 25ms/step - loss: 0.6939 - accuracy: 0.6153 - val_loss: 0.6857 - val_accuracy: 0.7879\n",
      "Epoch 6/150\n",
      "189/189 [==============================] - 4s 23ms/step - loss: 0.6936 - accuracy: 0.5034 - val_loss: 0.7119 - val_accuracy: 0.1604\n",
      "Epoch 7/150\n",
      "189/189 [==============================] - 4s 23ms/step - loss: 0.6930 - accuracy: 0.4053 - val_loss: 0.6773 - val_accuracy: 0.8013\n",
      "Epoch 8/150\n",
      "189/189 [==============================] - 5s 24ms/step - loss: 0.6922 - accuracy: 0.6399 - val_loss: 0.6969 - val_accuracy: 0.7699\n",
      "Epoch 9/150\n",
      "189/189 [==============================] - 4s 23ms/step - loss: 0.6924 - accuracy: 0.5408 - val_loss: 0.6778 - val_accuracy: 0.7908\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.6906 - accuracy: 0.6156\n",
      "Training took a total of 181 seconds\n",
      "Training by looking back 18 hours of alarm data\n",
      "Epoch 1/150\n",
      "189/189 [==============================] - 11s 39ms/step - loss: 0.6987 - accuracy: 0.5272 - val_loss: 0.6318 - val_accuracy: 0.7936\n",
      "Epoch 2/150\n",
      "189/189 [==============================] - 6s 33ms/step - loss: 0.6946 - accuracy: 0.5586 - val_loss: 0.7075 - val_accuracy: 0.1605\n",
      "Epoch 3/150\n",
      "189/189 [==============================] - 6s 32ms/step - loss: 0.6932 - accuracy: 0.4993 - val_loss: 0.7159 - val_accuracy: 0.1622\n",
      "Epoch 4/150\n",
      "189/189 [==============================] - 6s 32ms/step - loss: 0.6935 - accuracy: 0.4824 - val_loss: 0.6240 - val_accuracy: 0.7890\n",
      "Epoch 5/150\n",
      "189/189 [==============================] - 6s 33ms/step - loss: 0.6924 - accuracy: 0.4560 - val_loss: 0.6927 - val_accuracy: 0.7698\n",
      "Epoch 6/150\n",
      "189/189 [==============================] - 6s 32ms/step - loss: 0.6929 - accuracy: 0.4777 - val_loss: 0.7067 - val_accuracy: 0.1640\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.7177 - accuracy: 0.0964\n",
      "Training took a total of 186 seconds\n",
      "Training by looking back 24 hours of alarm data\n",
      "Epoch 1/150\n",
      "189/189 [==============================] - 12s 48ms/step - loss: 0.6980 - accuracy: 0.6038 - val_loss: 0.7011 - val_accuracy: 0.1605\n",
      "Epoch 2/150\n",
      "189/189 [==============================] - 8s 44ms/step - loss: 0.6949 - accuracy: 0.4505 - val_loss: 0.6871 - val_accuracy: 0.7756\n",
      "Epoch 3/150\n",
      "189/189 [==============================] - 9s 46ms/step - loss: 0.6935 - accuracy: 0.5121 - val_loss: 0.6926 - val_accuracy: 0.2157\n",
      "Epoch 4/150\n",
      "189/189 [==============================] - 9s 46ms/step - loss: 0.6950 - accuracy: 0.5334 - val_loss: 0.7098 - val_accuracy: 0.1605\n",
      "Epoch 5/150\n",
      "189/189 [==============================] - 9s 48ms/step - loss: 0.6937 - accuracy: 0.2520 - val_loss: 0.6483 - val_accuracy: 0.7971\n",
      "Epoch 6/150\n",
      "189/189 [==============================] - 9s 48ms/step - loss: 0.6951 - accuracy: 0.5070 - val_loss: 0.6827 - val_accuracy: 0.8174\n",
      "Epoch 7/150\n",
      "189/189 [==============================] - 9s 48ms/step - loss: 0.6940 - accuracy: 0.5154 - val_loss: 0.6744 - val_accuracy: 0.7733\n",
      "Epoch 8/150\n",
      "189/189 [==============================] - 9s 48ms/step - loss: 0.6930 - accuracy: 0.5493 - val_loss: 0.6749 - val_accuracy: 0.7750\n",
      "Epoch 9/150\n",
      "189/189 [==============================] - 9s 48ms/step - loss: 0.6934 - accuracy: 0.4654 - val_loss: 0.7070 - val_accuracy: 0.1605\n",
      "Epoch 10/150\n",
      "189/189 [==============================] - 9s 48ms/step - loss: 0.6983 - accuracy: 0.2895 - val_loss: 0.6706 - val_accuracy: 0.7872\n",
      "Epoch 11/150\n",
      "189/189 [==============================] - 9s 47ms/step - loss: 0.6943 - accuracy: 0.4953 - val_loss: 0.7087 - val_accuracy: 0.1872\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.7061 - accuracy: 0.2602\n",
      "Training took a total of 250 seconds\n",
      "Training by looking back 48 hours of alarm data\n",
      "Epoch 1/150\n",
      "188/188 [==============================] - 21s 93ms/step - loss: 0.6977 - accuracy: 0.4881 - val_loss: 0.7178 - val_accuracy: 0.1595\n",
      "Epoch 2/150\n",
      "188/188 [==============================] - 17s 90ms/step - loss: 0.6938 - accuracy: 0.5109 - val_loss: 0.6980 - val_accuracy: 0.2002\n",
      "Epoch 3/150\n",
      "188/188 [==============================] - 16s 88ms/step - loss: 0.6942 - accuracy: 0.4994 - val_loss: 0.6886 - val_accuracy: 0.8399\n",
      "Epoch 4/150\n",
      "188/188 [==============================] - 16s 85ms/step - loss: 0.6938 - accuracy: 0.5832 - val_loss: 0.7020 - val_accuracy: 0.2381\n",
      "Epoch 5/150\n",
      "188/188 [==============================] - 16s 85ms/step - loss: 0.6951 - accuracy: 0.6264 - val_loss: 0.6949 - val_accuracy: 0.2340\n",
      "Epoch 6/150\n",
      "188/188 [==============================] - 16s 83ms/step - loss: 0.6941 - accuracy: 0.5012 - val_loss: 0.6832 - val_accuracy: 0.8405\n",
      "Epoch 7/150\n",
      "188/188 [==============================] - 16s 83ms/step - loss: 0.6932 - accuracy: 0.6663 - val_loss: 0.6892 - val_accuracy: 0.8405\n",
      "Epoch 8/150\n",
      "188/188 [==============================] - 16s 86ms/step - loss: 0.6951 - accuracy: 0.5628 - val_loss: 0.7082 - val_accuracy: 0.1601\n",
      "Epoch 9/150\n",
      "188/188 [==============================] - 17s 90ms/step - loss: 0.6937 - accuracy: 0.3852 - val_loss: 0.7015 - val_accuracy: 0.2194\n",
      "Epoch 10/150\n",
      "188/188 [==============================] - 17s 91ms/step - loss: 0.6926 - accuracy: 0.4746 - val_loss: 0.6675 - val_accuracy: 0.8376\n",
      "Epoch 11/150\n",
      "188/188 [==============================] - 17s 91ms/step - loss: 0.6941 - accuracy: 0.5338 - val_loss: 0.6857 - val_accuracy: 0.8306\n",
      "27/27 [==============================] - 1s 26ms/step - loss: 0.6757 - accuracy: 0.8802\n",
      "Training took a total of 371 seconds\n",
      "Training by looking back 72 hours of alarm data\n",
      "Epoch 1/150\n",
      "188/188 [==============================] - 25s 116ms/step - loss: 0.6975 - accuracy: 0.5463 - val_loss: 0.7069 - val_accuracy: 0.1590\n",
      "Epoch 2/150\n",
      "188/188 [==============================] - 21s 112ms/step - loss: 0.6960 - accuracy: 0.4468 - val_loss: 0.6872 - val_accuracy: 0.8218\n",
      "Epoch 3/150\n",
      "188/188 [==============================] - 21s 112ms/step - loss: 0.6960 - accuracy: 0.4152 - val_loss: 0.7148 - val_accuracy: 0.1584\n",
      "Epoch 4/150\n",
      "188/188 [==============================] - 21s 112ms/step - loss: 0.6948 - accuracy: 0.4342 - val_loss: 0.7036 - val_accuracy: 0.1584\n",
      "Epoch 5/150\n",
      "188/188 [==============================] - 21s 111ms/step - loss: 0.6948 - accuracy: 0.3824 - val_loss: 0.7017 - val_accuracy: 0.1590\n",
      "Epoch 6/150\n",
      "188/188 [==============================] - 21s 113ms/step - loss: 0.6939 - accuracy: 0.3550 - val_loss: 0.7132 - val_accuracy: 0.1578\n",
      "Epoch 7/150\n",
      "188/188 [==============================] - 21s 112ms/step - loss: 0.6943 - accuracy: 0.3784 - val_loss: 0.6925 - val_accuracy: 0.2330\n",
      "27/27 [==============================] - 1s 32ms/step - loss: 0.6837 - accuracy: 0.5308\n",
      "Training took a total of 397 seconds\n"
     ]
    }
   ],
   "source": [
    "seq_result = {}\n",
    "lookback = [6,12,18,24,48,72]\n",
    "for hour in lookback:\n",
    "    start = datetime.now()\n",
    "    print(f\"Training by looking back {hour} hours of alarm data\")\n",
    "    wba124 = seven_days_LSTM(\"WBA124\", wba124_fullalarm, hour, 3)\n",
    "\n",
    "    # pad the alarm to train on LSTM\n",
    "    unpadded_arr = wba124.X_seq\n",
    "    mean_length = int(np.mean([len(x) for x in unpadded_arr]))\n",
    "    padded_alarm = np.zeros([len(unpadded_arr), mean_length])\n",
    "    for i,j in enumerate(unpadded_arr):\n",
    "        padded_alarm[i][0:len(j)] = j[:mean_length]\n",
    "\n",
    "    # scale training data for the model to learn faster\n",
    "    scaler = StandardScaler()\n",
    "    scaled_X_seq = scaler.fit_transform(padded_alarm)\n",
    "\n",
    "    #train_val_test split\n",
    "    val_percentage = 0.2\n",
    "    test_percentage = 0.1\n",
    "\n",
    "    test_index = int(len(scaled_X_seq) * (1-test_percentage))\n",
    "    val_index = int(len(scaled_X_seq) * (1- val_percentage - test_percentage))\n",
    "\n",
    "    X_train_seq, X_val_seq, X_test_seq = scaled_X_seq[:val_index], scaled_X_seq[val_index:test_index], scaled_X_seq[test_index:]\n",
    "    y_train_seq, y_val_seq, y_test_seq = wba124.y[:val_index], wba124.y[val_index:test_index], wba124.y[test_index:]\n",
    "\n",
    "    X_train_seq = X_train_seq.reshape(X_train_seq.shape[0], X_train_seq.shape[1], 1)\n",
    "    X_val_seq = X_val_seq.reshape(X_val_seq.shape[0], X_val_seq.shape[1], 1)\n",
    "    X_test_seq = X_test_seq.reshape(X_test_seq.shape[0], X_test_seq.shape[1], 1)\n",
    "\n",
    "    class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train_seq),\n",
    "                                                 y_train_seq)\n",
    "    class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "    #need to reinitialize the model because x_train_seq changes in shape\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=(X_train_seq.shape[1:]), return_sequences=True))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(LSTM(64, input_shape=(X_train_seq.shape[1:])))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(32, activation = 'relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(2, activation = 'softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    callbacks = [EarlyStopping(monitor='val_accuracy', mode='max', patience=5)]\n",
    "\n",
    "    history = model.fit(X_train_seq, y_train_seq, \n",
    "                    batch_size=BATCH_SIZE, epochs=EPOCHS, \n",
    "                    validation_data=(X_val_seq, y_val_seq), \n",
    "                    class_weight=class_weights_dict, callbacks=callbacks)\n",
    "    \n",
    "    evaluate = model.evaluate(X_test_seq, y_test_seq) #loss, mse\n",
    "\n",
    "    end = datetime.now()\n",
    "    time = end - start\n",
    "    seq_result[hour, mean_length] = evaluate\n",
    "    print(f\"Training took a total of {time.seconds} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e68372e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(6, 9): [0.6825759410858154, 0.6364692449569702],\n",
       " (12, 19): [0.6905587315559387, 0.6155632734298706],\n",
       " (18, 28): [0.7176927924156189, 0.09639953821897507],\n",
       " (24, 38): [0.7060851454734802, 0.2601625919342041],\n",
       " (48, 76): [0.6757280826568604, 0.880232572555542],\n",
       " (72, 114): [0.6836994290351868, 0.530849814414978]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6aad2f7",
   "metadata": {},
   "source": [
    "# Training with Embedding layer\n",
    "### LabelEncode the alarm id to determine n_vocab in Embedding layer\n",
    "### Since max_vocab measures the maximum number of unique vocab in our input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc33a2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode(X_seq): # do this the manual way as we are not certain if sklearn LabelEncoder can handle 3D array\n",
    "    all_unique_alarms = [set(ele) for ele in X_seq]\n",
    "    unique_alarms = set()\n",
    "    for ele in all_unique_alarms:\n",
    "        unique_alarms |= ele\n",
    "    \n",
    "    enc_label = 1  #start encoding from 1 as we have to pad the sequence with 0\n",
    "    mapping_dict = {}\n",
    "    for ele in unique_alarms:\n",
    "        mapping_dict[ele] = enc_label\n",
    "        enc_label += 1\n",
    "\n",
    "        enc_array = []\n",
    "        \n",
    "    #X_seq is a 3D array\n",
    "    for timestamp in X_seq:\n",
    "        tmp_arr = []\n",
    "        for ele in timestamp:\n",
    "            tmp_arr.append(mapping_dict[ele])\n",
    "        enc_array.append(np.array(tmp_arr))\n",
    "\n",
    "    return np.array(enc_array), len(unique_alarms)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28197e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(padded_alarm, n_alarms):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(padded_alarm.shape[1])))\n",
    "    model.add(Embedding(n_alarms, 128, mask_zero=True))\n",
    "    model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(Bidirectional(LSTM(10)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), 'accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a6bf92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomly_select_negative(X_seq, major_down_arr, ratio):\n",
    "    bool_arr = [ele==0 for ele in major_down_arr]\n",
    "    major_arr = np.array(major_down_arr)[np.where(bool_arr)[0]]\n",
    "    print(len(major_arr), len(major_down_arr))\n",
    "    negative = X_seq[np.where(bool_arr)[0]]\n",
    "    \n",
    "    positive_bool_arr = [ele==1 for ele in major_down_arr]\n",
    "    positive_major_arr = np.array(major_down_arr)[np.where(positive_bool_arr)[0]]\n",
    "    print(len(positive_major_arr))\n",
    "    positive = X_seq[np.where(positive_bool_arr)[0]]\n",
    "    \n",
    "    discard, keep, target_discard, target_keep = train_test_split(negative, major_arr, test_size=ratio)\n",
    "    \n",
    "    handpicked = np.concatenate((positive, keep))\n",
    "    target = np.concatenate((positive_major_arr, target_keep))\n",
    "    \n",
    "    if len(handpicked) != len(target):\n",
    "        raise Exception(\"Length of training inputs are different\")\n",
    "    \n",
    "    return handpicked, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40286b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_datapoint(X_seq, major_down_arr):\n",
    "    bool_arr = [len(ele)==0 for ele in X_seq] #this is to find the index to remove for bot y array and X_seq\n",
    "    idx_remove = np.where(bool_arr)[0]\n",
    "    major_down_arr = np.delete(np.array(major_down_arr), idx_remove) # remove the corresponding y value as well\n",
    "    X_seq = np.delete(X_seq, idx_remove) #remove rows with no alarms\n",
    "    return X_seq, major_down_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb9cc591",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38437 43033\n",
      "4596\n",
      "12284 12284\n",
      "Epoch 1/300\n",
      "231/231 [==============================] - 8s 20ms/step - loss: 0.6473 - precision: 0.4180 - recall: 0.0185 - accuracy: 0.6232 - val_loss: 0.6317 - val_precision: 0.5686 - val_recall: 0.1840 - val_accuracy: 0.6425\n",
      "Epoch 2/300\n",
      "231/231 [==============================] - 4s 16ms/step - loss: 0.6412 - precision: 0.5339 - recall: 0.2314 - accuracy: 0.6369 - val_loss: 0.6291 - val_precision: 0.5545 - val_recall: 0.3826 - val_accuracy: 0.6540\n",
      "Epoch 3/300\n",
      "231/231 [==============================] - 4s 16ms/step - loss: 0.6382 - precision: 0.5284 - recall: 0.2771 - accuracy: 0.6370 - val_loss: 0.6304 - val_precision: 0.5377 - val_recall: 0.3681 - val_accuracy: 0.6452\n",
      "Epoch 4/300\n",
      "231/231 [==============================] - 4s 16ms/step - loss: 0.6380 - precision: 0.5341 - recall: 0.3036 - accuracy: 0.6404 - val_loss: 0.6302 - val_precision: 0.5442 - val_recall: 0.4016 - val_accuracy: 0.6503\n",
      "Epoch 5/300\n",
      "231/231 [==============================] - 4s 16ms/step - loss: 0.6358 - precision: 0.5226 - recall: 0.2815 - accuracy: 0.6350 - val_loss: 0.6294 - val_precision: 0.5330 - val_recall: 0.4325 - val_accuracy: 0.6459\n",
      "Epoch 6/300\n",
      "231/231 [==============================] - 4s 16ms/step - loss: 0.6353 - precision: 0.5313 - recall: 0.3261 - accuracy: 0.6403 - val_loss: 0.6275 - val_precision: 0.5375 - val_recall: 0.3772 - val_accuracy: 0.6455\n",
      "Epoch 7/300\n",
      "231/231 [==============================] - 4s 17ms/step - loss: 0.6329 - precision: 0.5398 - recall: 0.3348 - accuracy: 0.6444 - val_loss: 0.6272 - val_precision: 0.5463 - val_recall: 0.3908 - val_accuracy: 0.6506\n",
      "Epoch 8/300\n",
      "231/231 [==============================] - 4s 16ms/step - loss: 0.6304 - precision: 0.5584 - recall: 0.3261 - accuracy: 0.6514 - val_loss: 0.6265 - val_precision: 0.5597 - val_recall: 0.3654 - val_accuracy: 0.6550\n",
      "Epoch 9/300\n",
      "231/231 [==============================] - 4s 16ms/step - loss: 0.6312 - precision: 0.5478 - recall: 0.3116 - accuracy: 0.6463 - val_loss: 0.6268 - val_precision: 0.5473 - val_recall: 0.3989 - val_accuracy: 0.6516\n",
      "Epoch 10/300\n",
      "231/231 [==============================] - 4s 16ms/step - loss: 0.6282 - precision: 0.5481 - recall: 0.3598 - accuracy: 0.6495 - val_loss: 0.6292 - val_precision: 0.5582 - val_recall: 0.3345 - val_accuracy: 0.6520\n",
      "Epoch 11/300\n",
      "231/231 [==============================] - 4s 16ms/step - loss: 0.6264 - precision: 0.5616 - recall: 0.3653 - accuracy: 0.6559 - val_loss: 0.6267 - val_precision: 0.5402 - val_recall: 0.3898 - val_accuracy: 0.6476\n",
      "Epoch 12/300\n",
      "231/231 [==============================] - 4s 16ms/step - loss: 0.6256 - precision: 0.5648 - recall: 0.3794 - accuracy: 0.6585 - val_loss: 0.6284 - val_precision: 0.5656 - val_recall: 0.2892 - val_accuracy: 0.6509\n",
      "Epoch 13/300\n",
      "231/231 [==============================] - 4s 16ms/step - loss: 0.6231 - precision: 0.5702 - recall: 0.3489 - accuracy: 0.6581 - val_loss: 0.6290 - val_precision: 0.5487 - val_recall: 0.3726 - val_accuracy: 0.6506\n",
      "Epoch 14/300\n",
      "231/231 [==============================] - 4s 16ms/step - loss: 0.6216 - precision: 0.5680 - recall: 0.3725 - accuracy: 0.6593 - val_loss: 0.6270 - val_precision: 0.5613 - val_recall: 0.3654 - val_accuracy: 0.6557\n",
      "Epoch 15/300\n",
      "231/231 [==============================] - 4s 17ms/step - loss: 0.6193 - precision: 0.5731 - recall: 0.3653 - accuracy: 0.6608 - val_loss: 0.6297 - val_precision: 0.5594 - val_recall: 0.3844 - val_accuracy: 0.6564\n",
      "Epoch 16/300\n",
      "231/231 [==============================] - 4s 17ms/step - loss: 0.6174 - precision: 0.5713 - recall: 0.3631 - accuracy: 0.6598 - val_loss: 0.6306 - val_precision: 0.5557 - val_recall: 0.3209 - val_accuracy: 0.6499\n",
      "Epoch 17/300\n",
      "231/231 [==============================] - 4s 17ms/step - loss: 0.6152 - precision: 0.5781 - recall: 0.3841 - accuracy: 0.6647 - val_loss: 0.6358 - val_precision: 0.5608 - val_recall: 0.2593 - val_accuracy: 0.6469\n",
      "Epoch 18/300\n",
      "231/231 [==============================] - 4s 17ms/step - loss: 0.6125 - precision: 0.5877 - recall: 0.3805 - accuracy: 0.6684 - val_loss: 0.6318 - val_precision: 0.5565 - val_recall: 0.3083 - val_accuracy: 0.6493\n",
      "Epoch 19/300\n",
      "231/231 [==============================] - 4s 17ms/step - loss: 0.6083 - precision: 0.6035 - recall: 0.3754 - accuracy: 0.6741 - val_loss: 0.6346 - val_precision: 0.5472 - val_recall: 0.3835 - val_accuracy: 0.6506\n",
      "Epoch 20/300\n",
      "231/231 [==============================] - 4s 17ms/step - loss: 0.6065 - precision: 0.6047 - recall: 0.3950 - accuracy: 0.6771 - val_loss: 0.6369 - val_precision: 0.5528 - val_recall: 0.3228 - val_accuracy: 0.6489\n",
      "Epoch 21/300\n",
      "231/231 [==============================] - 4s 17ms/step - loss: 0.6037 - precision: 0.6038 - recall: 0.4102 - accuracy: 0.6787 - val_loss: 0.6373 - val_precision: 0.5472 - val_recall: 0.3155 - val_accuracy: 0.6462\n",
      "Epoch 22/300\n",
      "231/231 [==============================] - 4s 17ms/step - loss: 0.5985 - precision: 0.6107 - recall: 0.4102 - accuracy: 0.6815 - val_loss: 0.6381 - val_precision: 0.5437 - val_recall: 0.3835 - val_accuracy: 0.6489\n",
      "Epoch 23/300\n",
      "231/231 [==============================] - 4s 17ms/step - loss: 0.5960 - precision: 0.6057 - recall: 0.4207 - accuracy: 0.6809 - val_loss: 0.6424 - val_precision: 0.5277 - val_recall: 0.3536 - val_accuracy: 0.6398\n",
      "Epoch 24/300\n",
      "231/231 [==============================] - 4s 17ms/step - loss: 0.5956 - precision: 0.6164 - recall: 0.4273 - accuracy: 0.6863 - val_loss: 0.6426 - val_precision: 0.5444 - val_recall: 0.4116 - val_accuracy: 0.6509\n",
      "Epoch 25/300\n",
      "231/231 [==============================] - 4s 17ms/step - loss: 0.5937 - precision: 0.6173 - recall: 0.4124 - accuracy: 0.6845 - val_loss: 0.6447 - val_precision: 0.5317 - val_recall: 0.2738 - val_accuracy: 0.6381\n",
      "Epoch 26/300\n",
      "231/231 [==============================] - 4s 17ms/step - loss: 0.5894 - precision: 0.6281 - recall: 0.4233 - accuracy: 0.6905 - val_loss: 0.6453 - val_precision: 0.5483 - val_recall: 0.3708 - val_accuracy: 0.6503\n",
      "Epoch 27/300\n",
      "231/231 [==============================] - 4s 17ms/step - loss: 0.5881 - precision: 0.6220 - recall: 0.4363 - accuracy: 0.6900 - val_loss: 0.6431 - val_precision: 0.5373 - val_recall: 0.3726 - val_accuracy: 0.6452\n",
      "Epoch 28/300\n",
      "231/231 [==============================] - 4s 17ms/step - loss: 0.5841 - precision: 0.6261 - recall: 0.4204 - accuracy: 0.6893 - val_loss: 0.6493 - val_precision: 0.5366 - val_recall: 0.3255 - val_accuracy: 0.6425\n",
      "Epoch 29/300\n",
      "231/231 [==============================] - 4s 17ms/step - loss: 0.5825 - precision: 0.6240 - recall: 0.4298 - accuracy: 0.6898 - val_loss: 0.6505 - val_precision: 0.5508 - val_recall: 0.3980 - val_accuracy: 0.6533\n",
      "Epoch 30/300\n",
      "231/231 [==============================] - 4s 17ms/step - loss: 0.5776 - precision: 0.6381 - recall: 0.4534 - accuracy: 0.6993 - val_loss: 0.6498 - val_precision: 0.5320 - val_recall: 0.3998 - val_accuracy: 0.6438\n",
      "Epoch 31/300\n",
      "231/231 [==============================] - 4s 17ms/step - loss: 0.5734 - precision: 0.6466 - recall: 0.4487 - accuracy: 0.7020 - val_loss: 0.6565 - val_precision: 0.5412 - val_recall: 0.4053 - val_accuracy: 0.6489\n",
      "Epoch 32/300\n",
      "231/231 [==============================] - 4s 17ms/step - loss: 0.5705 - precision: 0.6485 - recall: 0.4650 - accuracy: 0.7056 - val_loss: 0.6588 - val_precision: 0.5511 - val_recall: 0.3762 - val_accuracy: 0.6520\n",
      "Epoch 33/300\n",
      "231/231 [==============================] - 4s 17ms/step - loss: 0.5696 - precision: 0.6467 - recall: 0.4694 - accuracy: 0.7056 - val_loss: 0.6607 - val_precision: 0.5374 - val_recall: 0.3908 - val_accuracy: 0.6462\n",
      "Epoch 34/300\n",
      "231/231 [==============================] - 4s 17ms/step - loss: 0.5636 - precision: 0.6376 - recall: 0.4664 - accuracy: 0.7012 - val_loss: 0.6567 - val_precision: 0.5492 - val_recall: 0.3645 - val_accuracy: 0.6503\n",
      "Epoch 35/300\n",
      "231/231 [==============================] - 4s 17ms/step - loss: 0.5620 - precision: 0.6461 - recall: 0.4668 - accuracy: 0.7049 - val_loss: 0.6614 - val_precision: 0.5389 - val_recall: 0.3645 - val_accuracy: 0.6455\n",
      "62/62 [==============================] - 0s 3ms/step - loss: 0.6287 - precision: 0.5336 - recall: 0.4633 - accuracy: 0.6475\n",
      "Training took 905 seconds to complete.\n",
      "38421 43013\n",
      "4592\n",
      "12277 12277\n",
      "Epoch 1/300\n",
      "231/231 [==============================] - 11s 33ms/step - loss: 0.6493 - precision_1: 0.5064 - recall_1: 0.1005 - accuracy: 0.6269 - val_loss: 0.6480 - val_precision_1: 0.4459 - val_recall_1: 0.0299 - val_accuracy: 0.6232\n",
      "Epoch 2/300\n",
      "231/231 [==============================] - 7s 30ms/step - loss: 0.6409 - precision_1: 0.5295 - recall_1: 0.1826 - accuracy: 0.6336 - val_loss: 0.6456 - val_precision_1: 0.5000 - val_recall_1: 0.1751 - val_accuracy: 0.6259\n",
      "Epoch 3/300\n",
      "231/231 [==============================] - 7s 30ms/step - loss: 0.6394 - precision_1: 0.5095 - recall_1: 0.2345 - accuracy: 0.6292 - val_loss: 0.6445 - val_precision_1: 0.5282 - val_recall_1: 0.0935 - val_accuracy: 0.6297\n",
      "Epoch 4/300\n",
      "231/231 [==============================] - 7s 30ms/step - loss: 0.6361 - precision_1: 0.5261 - recall_1: 0.2301 - accuracy: 0.6345 - val_loss: 0.6424 - val_precision_1: 0.5223 - val_recall_1: 0.2868 - val_accuracy: 0.6351\n",
      "Epoch 5/300\n",
      "231/231 [==============================] - 7s 30ms/step - loss: 0.6331 - precision_1: 0.5179 - recall_1: 0.2472 - accuracy: 0.6324 - val_loss: 0.6427 - val_precision_1: 0.5217 - val_recall_1: 0.3276 - val_accuracy: 0.6361\n",
      "Epoch 6/300\n",
      "231/231 [==============================] - 7s 30ms/step - loss: 0.6311 - precision_1: 0.5327 - recall_1: 0.2628 - accuracy: 0.6381 - val_loss: 0.6402 - val_precision_1: 0.5343 - val_recall_1: 0.2895 - val_accuracy: 0.6399\n",
      "Epoch 7/300\n",
      "231/231 [==============================] - 7s 30ms/step - loss: 0.6281 - precision_1: 0.5269 - recall_1: 0.3307 - accuracy: 0.6386 - val_loss: 0.6406 - val_precision_1: 0.5243 - val_recall_1: 0.1370 - val_accuracy: 0.6307\n",
      "Epoch 8/300\n",
      "231/231 [==============================] - 7s 30ms/step - loss: 0.6283 - precision_1: 0.5328 - recall_1: 0.2566 - accuracy: 0.6378 - val_loss: 0.6401 - val_precision_1: 0.5123 - val_recall_1: 0.2831 - val_accuracy: 0.6310\n",
      "Epoch 9/300\n",
      "231/231 [==============================] - 7s 30ms/step - loss: 0.6229 - precision_1: 0.5459 - recall_1: 0.3151 - accuracy: 0.6458 - val_loss: 0.6405 - val_precision_1: 0.5125 - val_recall_1: 0.2595 - val_accuracy: 0.6307\n",
      "Epoch 10/300\n",
      "231/231 [==============================] - 7s 30ms/step - loss: 0.6223 - precision_1: 0.5566 - recall_1: 0.2907 - accuracy: 0.6481 - val_loss: 0.6401 - val_precision_1: 0.5099 - val_recall_1: 0.3739 - val_accuracy: 0.6314\n",
      "Epoch 11/300\n",
      "231/231 [==============================] - 7s 30ms/step - loss: 0.6210 - precision_1: 0.5379 - recall_1: 0.3350 - accuracy: 0.6436 - val_loss: 0.6386 - val_precision_1: 0.5505 - val_recall_1: 0.1978 - val_accuracy: 0.6395\n",
      "Epoch 12/300\n",
      "231/231 [==============================] - 7s 30ms/step - loss: 0.6193 - precision_1: 0.5592 - recall_1: 0.3412 - accuracy: 0.6530 - val_loss: 0.6434 - val_precision_1: 0.5416 - val_recall_1: 0.2187 - val_accuracy: 0.6385\n",
      "Epoch 13/300\n",
      "231/231 [==============================] - 7s 30ms/step - loss: 0.6161 - precision_1: 0.5477 - recall_1: 0.3394 - accuracy: 0.6481 - val_loss: 0.6431 - val_precision_1: 0.5009 - val_recall_1: 0.4809 - val_accuracy: 0.6266\n",
      "Epoch 14/300\n",
      "231/231 [==============================] - 7s 30ms/step - loss: 0.6135 - precision_1: 0.5713 - recall_1: 0.3604 - accuracy: 0.6597 - val_loss: 0.6436 - val_precision_1: 0.5257 - val_recall_1: 0.3711 - val_accuracy: 0.6395\n",
      "Epoch 15/300\n",
      "231/231 [==============================] - 7s 31ms/step - loss: 0.6100 - precision_1: 0.5648 - recall_1: 0.3858 - accuracy: 0.6591 - val_loss: 0.6387 - val_precision_1: 0.5132 - val_recall_1: 0.3884 - val_accuracy: 0.6334\n",
      "Epoch 16/300\n",
      "231/231 [==============================] - 7s 30ms/step - loss: 0.6074 - precision_1: 0.5693 - recall_1: 0.3699 - accuracy: 0.6597 - val_loss: 0.6428 - val_precision_1: 0.5241 - val_recall_1: 0.2858 - val_accuracy: 0.6358\n",
      "Epoch 17/300\n",
      "231/231 [==============================] - 7s 31ms/step - loss: 0.6055 - precision_1: 0.5816 - recall_1: 0.3931 - accuracy: 0.6673 - val_loss: 0.6397 - val_precision_1: 0.5087 - val_recall_1: 0.3721 - val_accuracy: 0.6307\n",
      "Epoch 18/300\n",
      "231/231 [==============================] - 7s 30ms/step - loss: 0.5976 - precision_1: 0.5953 - recall_1: 0.4363 - accuracy: 0.6783 - val_loss: 0.6439 - val_precision_1: 0.5255 - val_recall_1: 0.3176 - val_accuracy: 0.6375\n",
      "Epoch 19/300\n",
      "231/231 [==============================] - 7s 30ms/step - loss: 0.5975 - precision_1: 0.5909 - recall_1: 0.4069 - accuracy: 0.6728 - val_loss: 0.6482 - val_precision_1: 0.5162 - val_recall_1: 0.4047 - val_accuracy: 0.6354\n",
      "Epoch 20/300\n",
      "231/231 [==============================] - 7s 30ms/step - loss: 0.5934 - precision_1: 0.6017 - recall_1: 0.4403 - accuracy: 0.6816 - val_loss: 0.6467 - val_precision_1: 0.5084 - val_recall_1: 0.3848 - val_accuracy: 0.6307\n",
      "Epoch 21/300\n",
      "231/231 [==============================] - 7s 30ms/step - loss: 0.5936 - precision_1: 0.5980 - recall_1: 0.4254 - accuracy: 0.6781 - val_loss: 0.6465 - val_precision_1: 0.5194 - val_recall_1: 0.3530 - val_accuracy: 0.6358\n",
      "Epoch 22/300\n",
      "231/231 [==============================] - 7s 30ms/step - loss: 0.5867 - precision_1: 0.6027 - recall_1: 0.4472 - accuracy: 0.6830 - val_loss: 0.6465 - val_precision_1: 0.5146 - val_recall_1: 0.3367 - val_accuracy: 0.6331\n",
      "Epoch 23/300\n",
      "231/231 [==============================] - 7s 30ms/step - loss: 0.5843 - precision_1: 0.6101 - recall_1: 0.4396 - accuracy: 0.6853 - val_loss: 0.6563 - val_precision_1: 0.4892 - val_recall_1: 0.4093 - val_accuracy: 0.6191\n",
      "Epoch 24/300\n",
      "231/231 [==============================] - 7s 30ms/step - loss: 0.5810 - precision_1: 0.6250 - recall_1: 0.4465 - accuracy: 0.6928 - val_loss: 0.6569 - val_precision_1: 0.4896 - val_recall_1: 0.4074 - val_accuracy: 0.6195\n",
      "Epoch 25/300\n",
      "231/231 [==============================] - 7s 30ms/step - loss: 0.5753 - precision_1: 0.6306 - recall_1: 0.4653 - accuracy: 0.6981 - val_loss: 0.6591 - val_precision_1: 0.4918 - val_recall_1: 0.3530 - val_accuracy: 0.6215\n",
      "Epoch 26/300\n",
      "231/231 [==============================] - 7s 30ms/step - loss: 0.5717 - precision_1: 0.6349 - recall_1: 0.4817 - accuracy: 0.7026 - val_loss: 0.6542 - val_precision_1: 0.5085 - val_recall_1: 0.3539 - val_accuracy: 0.6303\n",
      "Epoch 27/300\n",
      "231/231 [==============================] - 7s 30ms/step - loss: 0.5665 - precision_1: 0.6400 - recall_1: 0.4828 - accuracy: 0.7050 - val_loss: 0.6514 - val_precision_1: 0.5012 - val_recall_1: 0.3784 - val_accuracy: 0.6266\n",
      "Epoch 28/300\n",
      "231/231 [==============================] - 7s 30ms/step - loss: 0.5629 - precision_1: 0.6448 - recall_1: 0.4897 - accuracy: 0.7083 - val_loss: 0.6559 - val_precision_1: 0.4923 - val_recall_1: 0.4374 - val_accuracy: 0.6208\n",
      "Epoch 29/300\n",
      "231/231 [==============================] - 7s 30ms/step - loss: 0.5607 - precision_1: 0.6373 - recall_1: 0.5045 - accuracy: 0.7073 - val_loss: 0.6555 - val_precision_1: 0.5006 - val_recall_1: 0.3730 - val_accuracy: 0.6263\n",
      "Epoch 30/300\n",
      "231/231 [==============================] - 7s 30ms/step - loss: 0.5567 - precision_1: 0.6464 - recall_1: 0.4864 - accuracy: 0.7084 - val_loss: 0.6733 - val_precision_1: 0.5118 - val_recall_1: 0.3348 - val_accuracy: 0.6317\n",
      "Epoch 31/300\n",
      "231/231 [==============================] - 7s 30ms/step - loss: 0.5498 - precision_1: 0.6580 - recall_1: 0.4980 - accuracy: 0.7154 - val_loss: 0.6635 - val_precision_1: 0.5045 - val_recall_1: 0.3593 - val_accuracy: 0.6283\n",
      "Epoch 32/300\n",
      "231/231 [==============================] - 7s 30ms/step - loss: 0.5463 - precision_1: 0.6706 - recall_1: 0.5129 - accuracy: 0.7236 - val_loss: 0.6763 - val_precision_1: 0.5048 - val_recall_1: 0.3793 - val_accuracy: 0.6286\n",
      "Epoch 33/300\n",
      "231/231 [==============================] - 7s 30ms/step - loss: 0.5396 - precision_1: 0.6704 - recall_1: 0.5183 - accuracy: 0.7245 - val_loss: 0.6868 - val_precision_1: 0.5066 - val_recall_1: 0.3485 - val_accuracy: 0.6293\n",
      "Epoch 34/300\n",
      "231/231 [==============================] - 7s 31ms/step - loss: 0.5387 - precision_1: 0.6816 - recall_1: 0.5314 - accuracy: 0.7319 - val_loss: 0.6871 - val_precision_1: 0.5026 - val_recall_1: 0.4374 - val_accuracy: 0.6276\n",
      "Epoch 35/300\n",
      "231/231 [==============================] - 7s 31ms/step - loss: 0.5361 - precision_1: 0.6807 - recall_1: 0.5209 - accuracy: 0.7294 - val_loss: 0.6865 - val_precision_1: 0.4906 - val_recall_1: 0.4020 - val_accuracy: 0.6202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/300\n",
      "231/231 [==============================] - 7s 31ms/step - loss: 0.5261 - precision_1: 0.6827 - recall_1: 0.5372 - accuracy: 0.7335 - val_loss: 0.6959 - val_precision_1: 0.4968 - val_recall_1: 0.4220 - val_accuracy: 0.6239\n",
      "Epoch 37/300\n",
      "231/231 [==============================] - 7s 30ms/step - loss: 0.5204 - precision_1: 0.6896 - recall_1: 0.5452 - accuracy: 0.7381 - val_loss: 0.6924 - val_precision_1: 0.5106 - val_recall_1: 0.3929 - val_accuracy: 0.6320\n",
      "Epoch 38/300\n",
      "231/231 [==============================] - 7s 30ms/step - loss: 0.5226 - precision_1: 0.6943 - recall_1: 0.5441 - accuracy: 0.7399 - val_loss: 0.7060 - val_precision_1: 0.5163 - val_recall_1: 0.3439 - val_accuracy: 0.6341\n",
      "Epoch 39/300\n",
      "231/231 [==============================] - 7s 30ms/step - loss: 0.5169 - precision_1: 0.6964 - recall_1: 0.5528 - accuracy: 0.7426 - val_loss: 0.7099 - val_precision_1: 0.5030 - val_recall_1: 0.3757 - val_accuracy: 0.6276\n",
      "Epoch 40/300\n",
      "231/231 [==============================] - 7s 30ms/step - loss: 0.5185 - precision_1: 0.6963 - recall_1: 0.5575 - accuracy: 0.7436 - val_loss: 0.7037 - val_precision_1: 0.5011 - val_recall_1: 0.4002 - val_accuracy: 0.6266\n",
      "Epoch 41/300\n",
      "231/231 [==============================] - 7s 30ms/step - loss: 0.5090 - precision_1: 0.7078 - recall_1: 0.5699 - accuracy: 0.7512 - val_loss: 0.7006 - val_precision_1: 0.5161 - val_recall_1: 0.3494 - val_accuracy: 0.6341\n",
      "Epoch 42/300\n",
      "231/231 [==============================] - 7s 30ms/step - loss: 0.5091 - precision_1: 0.7042 - recall_1: 0.5626 - accuracy: 0.7480 - val_loss: 0.7174 - val_precision_1: 0.5072 - val_recall_1: 0.4147 - val_accuracy: 0.6303\n",
      "Epoch 43/300\n",
      "231/231 [==============================] - 7s 30ms/step - loss: 0.5049 - precision_1: 0.7067 - recall_1: 0.5702 - accuracy: 0.7507 - val_loss: 0.7065 - val_precision_1: 0.5012 - val_recall_1: 0.3866 - val_accuracy: 0.6266\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.6324 - precision_1: 0.5171 - recall_1: 0.5143 - accuracy: 0.6387\n",
      "Training took 1053 seconds to complete.\n",
      "38383 42973\n",
      "4590\n",
      "12267 12267\n",
      "Epoch 1/300\n",
      "230/230 [==============================] - 14s 48ms/step - loss: 0.6578 - precision_2: 0.3750 - recall_2: 0.0065 - accuracy: 0.6242 - val_loss: 0.6512 - val_precision_2: 1.0000 - val_recall_2: 0.0018 - val_accuracy: 0.6264\n",
      "Epoch 2/300\n",
      "230/230 [==============================] - 10s 45ms/step - loss: 0.6510 - precision_2: 0.5437 - recall_2: 0.0407 - accuracy: 0.6283 - val_loss: 0.6493 - val_precision_2: 0.5472 - val_recall_2: 0.0263 - val_accuracy: 0.6274\n",
      "Epoch 3/300\n",
      "230/230 [==============================] - 10s 45ms/step - loss: 0.6481 - precision_2: 0.5152 - recall_2: 0.0988 - accuracy: 0.6280 - val_loss: 0.6488 - val_precision_2: 0.5374 - val_recall_2: 0.0717 - val_accuracy: 0.6294\n",
      "Epoch 4/300\n",
      "230/230 [==============================] - 10s 45ms/step - loss: 0.6448 - precision_2: 0.5317 - recall_2: 0.1340 - accuracy: 0.6318 - val_loss: 0.6489 - val_precision_2: 0.5090 - val_recall_2: 0.2314 - val_accuracy: 0.6287\n",
      "Epoch 5/300\n",
      "230/230 [==============================] - 10s 45ms/step - loss: 0.6418 - precision_2: 0.5491 - recall_2: 0.2052 - accuracy: 0.6395 - val_loss: 0.6433 - val_precision_2: 0.5304 - val_recall_2: 0.2214 - val_accuracy: 0.6352\n",
      "Epoch 6/300\n",
      "230/230 [==============================] - 10s 45ms/step - loss: 0.6381 - precision_2: 0.5429 - recall_2: 0.2389 - accuracy: 0.6399 - val_loss: 0.6464 - val_precision_2: 0.5285 - val_recall_2: 0.1597 - val_accuracy: 0.6321\n",
      "Epoch 7/300\n",
      "230/230 [==============================] - 10s 45ms/step - loss: 0.6348 - precision_2: 0.5638 - recall_2: 0.2534 - accuracy: 0.6473 - val_loss: 0.6464 - val_precision_2: 0.5018 - val_recall_2: 0.3784 - val_accuracy: 0.6267\n",
      "Epoch 8/300\n",
      "230/230 [==============================] - 10s 45ms/step - loss: 0.6338 - precision_2: 0.5351 - recall_2: 0.3181 - accuracy: 0.6414 - val_loss: 0.6462 - val_precision_2: 0.5186 - val_recall_2: 0.1642 - val_accuracy: 0.6301\n",
      "Epoch 9/300\n",
      "230/230 [==============================] - 10s 45ms/step - loss: 0.6324 - precision_2: 0.5574 - recall_2: 0.2749 - accuracy: 0.6470 - val_loss: 0.6427 - val_precision_2: 0.5238 - val_recall_2: 0.2396 - val_accuracy: 0.6338\n",
      "Epoch 10/300\n",
      "230/230 [==============================] - 10s 45ms/step - loss: 0.6301 - precision_2: 0.5602 - recall_2: 0.2752 - accuracy: 0.6480 - val_loss: 0.6452 - val_precision_2: 0.5298 - val_recall_2: 0.2015 - val_accuracy: 0.6342\n",
      "Epoch 11/300\n",
      "230/230 [==============================] - 10s 45ms/step - loss: 0.6271 - precision_2: 0.5557 - recall_2: 0.3007 - accuracy: 0.6484 - val_loss: 0.6423 - val_precision_2: 0.5304 - val_recall_2: 0.2377 - val_accuracy: 0.6359\n",
      "Epoch 12/300\n",
      "230/230 [==============================] - 10s 45ms/step - loss: 0.6283 - precision_2: 0.5666 - recall_2: 0.2905 - accuracy: 0.6514 - val_loss: 0.6492 - val_precision_2: 0.5066 - val_recall_2: 0.2096 - val_accuracy: 0.6277\n",
      "Epoch 13/300\n",
      "230/230 [==============================] - 10s 45ms/step - loss: 0.6317 - precision_2: 0.5637 - recall_2: 0.2585 - accuracy: 0.6477 - val_loss: 0.6444 - val_precision_2: 0.5081 - val_recall_2: 0.3693 - val_accuracy: 0.6301\n",
      "Epoch 14/300\n",
      "230/230 [==============================] - 10s 45ms/step - loss: 0.6242 - precision_2: 0.5710 - recall_2: 0.3155 - accuracy: 0.6552 - val_loss: 0.6432 - val_precision_2: 0.5386 - val_recall_2: 0.3358 - val_accuracy: 0.6437\n",
      "Epoch 15/300\n",
      "230/230 [==============================] - 10s 45ms/step - loss: 0.6198 - precision_2: 0.5807 - recall_2: 0.3370 - accuracy: 0.6609 - val_loss: 0.6454 - val_precision_2: 0.5163 - val_recall_2: 0.4310 - val_accuracy: 0.6359\n",
      "Epoch 16/300\n",
      "230/230 [==============================] - 10s 45ms/step - loss: 0.6173 - precision_2: 0.5803 - recall_2: 0.3725 - accuracy: 0.6644 - val_loss: 0.6473 - val_precision_2: 0.5539 - val_recall_2: 0.2377 - val_accuracy: 0.6430\n",
      "Epoch 17/300\n",
      "230/230 [==============================] - 10s 45ms/step - loss: 0.6146 - precision_2: 0.5859 - recall_2: 0.3617 - accuracy: 0.6655 - val_loss: 0.6452 - val_precision_2: 0.5323 - val_recall_2: 0.2695 - val_accuracy: 0.6379\n",
      "Epoch 18/300\n",
      "230/230 [==============================] - 10s 45ms/step - loss: 0.6113 - precision_2: 0.5853 - recall_2: 0.3711 - accuracy: 0.6663 - val_loss: 0.6423 - val_precision_2: 0.5252 - val_recall_2: 0.3031 - val_accuracy: 0.6365\n",
      "Epoch 19/300\n",
      "230/230 [==============================] - 10s 45ms/step - loss: 0.6059 - precision_2: 0.6113 - recall_2: 0.3929 - accuracy: 0.6793 - val_loss: 0.6450 - val_precision_2: 0.5486 - val_recall_2: 0.2922 - val_accuracy: 0.6450\n",
      "Epoch 20/300\n",
      "230/230 [==============================] - 10s 45ms/step - loss: 0.6021 - precision_2: 0.6210 - recall_2: 0.3980 - accuracy: 0.6838 - val_loss: 0.6430 - val_precision_2: 0.5447 - val_recall_2: 0.2985 - val_accuracy: 0.6440\n",
      "Epoch 21/300\n",
      "230/230 [==============================] - 10s 45ms/step - loss: 0.6026 - precision_2: 0.6119 - recall_2: 0.3922 - accuracy: 0.6795 - val_loss: 0.6561 - val_precision_2: 0.5345 - val_recall_2: 0.2740 - val_accuracy: 0.6389\n",
      "Epoch 22/300\n",
      "230/230 [==============================] - 10s 45ms/step - loss: 0.5977 - precision_2: 0.6062 - recall_2: 0.4227 - accuracy: 0.6812 - val_loss: 0.6476 - val_precision_2: 0.5419 - val_recall_2: 0.3403 - val_accuracy: 0.6454\n",
      "Epoch 23/300\n",
      "230/230 [==============================] - 10s 45ms/step - loss: 0.5955 - precision_2: 0.6297 - recall_2: 0.4081 - accuracy: 0.6887 - val_loss: 0.6525 - val_precision_2: 0.5005 - val_recall_2: 0.4356 - val_accuracy: 0.6260\n",
      "Epoch 24/300\n",
      "230/230 [==============================] - 10s 45ms/step - loss: 0.5903 - precision_2: 0.6333 - recall_2: 0.4314 - accuracy: 0.6938 - val_loss: 0.6523 - val_precision_2: 0.5298 - val_recall_2: 0.3466 - val_accuracy: 0.6403\n",
      "Epoch 25/300\n",
      "230/230 [==============================] - 10s 45ms/step - loss: 0.5877 - precision_2: 0.6343 - recall_2: 0.4263 - accuracy: 0.6933 - val_loss: 0.6515 - val_precision_2: 0.5240 - val_recall_2: 0.3566 - val_accuracy: 0.6379\n",
      "Epoch 26/300\n",
      "230/230 [==============================] - 10s 45ms/step - loss: 0.5853 - precision_2: 0.6424 - recall_2: 0.4267 - accuracy: 0.6966 - val_loss: 0.6693 - val_precision_2: 0.4958 - val_recall_2: 0.4828 - val_accuracy: 0.6226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/300\n",
      "230/230 [==============================] - 10s 45ms/step - loss: 0.5798 - precision_2: 0.6349 - recall_2: 0.4357 - accuracy: 0.6951 - val_loss: 0.6547 - val_precision_2: 0.5373 - val_recall_2: 0.2813 - val_accuracy: 0.6403\n",
      "Epoch 28/300\n",
      "230/230 [==============================] - 10s 45ms/step - loss: 0.5800 - precision_2: 0.6465 - recall_2: 0.4575 - accuracy: 0.7034 - val_loss: 0.6486 - val_precision_2: 0.5363 - val_recall_2: 0.3285 - val_accuracy: 0.6423\n",
      "Epoch 29/300\n",
      "230/230 [==============================] - 10s 45ms/step - loss: 0.5829 - precision_2: 0.6390 - recall_2: 0.4198 - accuracy: 0.6942 - val_loss: 0.6491 - val_precision_2: 0.5583 - val_recall_2: 0.3258 - val_accuracy: 0.6512\n",
      "Epoch 30/300\n",
      "230/230 [==============================] - 10s 45ms/step - loss: 0.5748 - precision_2: 0.6504 - recall_2: 0.4412 - accuracy: 0.7022 - val_loss: 0.6601 - val_precision_2: 0.5334 - val_recall_2: 0.3911 - val_accuracy: 0.6440\n",
      "Epoch 31/300\n",
      "230/230 [==============================] - 10s 45ms/step - loss: 0.5717 - precision_2: 0.6451 - recall_2: 0.4699 - accuracy: 0.7049 - val_loss: 0.6691 - val_precision_2: 0.5068 - val_recall_2: 0.4737 - val_accuracy: 0.6304\n",
      "Epoch 32/300\n",
      "230/230 [==============================] - 10s 44ms/step - loss: 0.5697 - precision_2: 0.6608 - recall_2: 0.4521 - accuracy: 0.7082 - val_loss: 0.6612 - val_precision_2: 0.5268 - val_recall_2: 0.4102 - val_accuracy: 0.6413\n",
      "Epoch 33/300\n",
      "230/230 [==============================] - 10s 44ms/step - loss: 0.5630 - precision_2: 0.6658 - recall_2: 0.4927 - accuracy: 0.7177 - val_loss: 0.6564 - val_precision_2: 0.5527 - val_recall_2: 0.2523 - val_accuracy: 0.6437\n",
      "Epoch 34/300\n",
      "230/230 [==============================] - 10s 45ms/step - loss: 0.5601 - precision_2: 0.6707 - recall_2: 0.4680 - accuracy: 0.7149 - val_loss: 0.6672 - val_precision_2: 0.5185 - val_recall_2: 0.3938 - val_accuracy: 0.6362\n",
      "Epoch 35/300\n",
      "230/230 [==============================] - 10s 45ms/step - loss: 0.5587 - precision_2: 0.6599 - recall_2: 0.4924 - accuracy: 0.7151 - val_loss: 0.6650 - val_precision_2: 0.5232 - val_recall_2: 0.3584 - val_accuracy: 0.6376\n",
      "Epoch 36/300\n",
      "230/230 [==============================] - 10s 45ms/step - loss: 0.5525 - precision_2: 0.6824 - recall_2: 0.4829 - accuracy: 0.7224 - val_loss: 0.6721 - val_precision_2: 0.5394 - val_recall_2: 0.3103 - val_accuracy: 0.6427\n",
      "Epoch 37/300\n",
      "230/230 [==============================] - 10s 45ms/step - loss: 0.5505 - precision_2: 0.6719 - recall_2: 0.4938 - accuracy: 0.7204 - val_loss: 0.6648 - val_precision_2: 0.5512 - val_recall_2: 0.3466 - val_accuracy: 0.6498\n",
      "Epoch 38/300\n",
      "230/230 [==============================] - 10s 45ms/step - loss: 0.5473 - precision_2: 0.6894 - recall_2: 0.4916 - accuracy: 0.7269 - val_loss: 0.6747 - val_precision_2: 0.5372 - val_recall_2: 0.3866 - val_accuracy: 0.6457\n",
      "Epoch 39/300\n",
      "230/230 [==============================] - 10s 45ms/step - loss: 0.5462 - precision_2: 0.6995 - recall_2: 0.4978 - accuracy: 0.7321 - val_loss: 0.6748 - val_precision_2: 0.5336 - val_recall_2: 0.4111 - val_accuracy: 0.6450\n",
      "Epoch 40/300\n",
      "230/230 [==============================] - 10s 45ms/step - loss: 0.5395 - precision_2: 0.6858 - recall_2: 0.5073 - accuracy: 0.7287 - val_loss: 0.6808 - val_precision_2: 0.5470 - val_recall_2: 0.3167 - val_accuracy: 0.6461\n",
      "Epoch 41/300\n",
      "230/230 [==============================] - 10s 45ms/step - loss: 0.5409 - precision_2: 0.6965 - recall_2: 0.5033 - accuracy: 0.7321 - val_loss: 0.6765 - val_precision_2: 0.5219 - val_recall_2: 0.3993 - val_accuracy: 0.6382\n",
      "Epoch 42/300\n",
      "230/230 [==============================] - 10s 45ms/step - loss: 0.5362 - precision_2: 0.6960 - recall_2: 0.5087 - accuracy: 0.7330 - val_loss: 0.6803 - val_precision_2: 0.5346 - val_recall_2: 0.3439 - val_accuracy: 0.6423\n",
      "Epoch 43/300\n",
      "230/230 [==============================] - 10s 45ms/step - loss: 0.5255 - precision_2: 0.7132 - recall_2: 0.5265 - accuracy: 0.7436 - val_loss: 0.6880 - val_precision_2: 0.5274 - val_recall_2: 0.4192 - val_accuracy: 0.6420\n",
      "Epoch 44/300\n",
      "230/230 [==============================] - 10s 45ms/step - loss: 0.5260 - precision_2: 0.7114 - recall_2: 0.5272 - accuracy: 0.7431 - val_loss: 0.6995 - val_precision_2: 0.5458 - val_recall_2: 0.3730 - val_accuracy: 0.6491\n",
      "Epoch 45/300\n",
      "230/230 [==============================] - 10s 45ms/step - loss: 0.5281 - precision_2: 0.7017 - recall_2: 0.5363 - accuracy: 0.7412 - val_loss: 0.6761 - val_precision_2: 0.5407 - val_recall_2: 0.3076 - val_accuracy: 0.6430\n",
      "Epoch 46/300\n",
      "230/230 [==============================] - 10s 44ms/step - loss: 0.5207 - precision_2: 0.7117 - recall_2: 0.5316 - accuracy: 0.7442 - val_loss: 0.6972 - val_precision_2: 0.5316 - val_recall_2: 0.4120 - val_accuracy: 0.6440\n",
      "Epoch 47/300\n",
      "230/230 [==============================] - 10s 45ms/step - loss: 0.5175 - precision_2: 0.7161 - recall_2: 0.5458 - accuracy: 0.7490 - val_loss: 0.7035 - val_precision_2: 0.5232 - val_recall_2: 0.4201 - val_accuracy: 0.6396\n",
      "Epoch 48/300\n",
      "230/230 [==============================] - 10s 45ms/step - loss: 0.5331 - precision_2: 0.7023 - recall_2: 0.5182 - accuracy: 0.7375 - val_loss: 0.6921 - val_precision_2: 0.5339 - val_recall_2: 0.3358 - val_accuracy: 0.6416\n",
      "Epoch 49/300\n",
      "230/230 [==============================] - 10s 44ms/step - loss: 0.5199 - precision_2: 0.7240 - recall_2: 0.5258 - accuracy: 0.7476 - val_loss: 0.6898 - val_precision_2: 0.5442 - val_recall_2: 0.3630 - val_accuracy: 0.6478\n",
      "Epoch 50/300\n",
      "230/230 [==============================] - 10s 45ms/step - loss: 0.5118 - precision_2: 0.7325 - recall_2: 0.5370 - accuracy: 0.7534 - val_loss: 0.6953 - val_precision_2: 0.5346 - val_recall_2: 0.3721 - val_accuracy: 0.6437\n",
      "Epoch 51/300\n",
      "230/230 [==============================] - 10s 45ms/step - loss: 0.5074 - precision_2: 0.7245 - recall_2: 0.5566 - accuracy: 0.7549 - val_loss: 0.6961 - val_precision_2: 0.5467 - val_recall_2: 0.3984 - val_accuracy: 0.6512\n",
      "Epoch 52/300\n",
      "230/230 [==============================] - 10s 44ms/step - loss: 0.4987 - precision_2: 0.7467 - recall_2: 0.5566 - accuracy: 0.7635 - val_loss: 0.7136 - val_precision_2: 0.5496 - val_recall_2: 0.3820 - val_accuracy: 0.6515\n",
      "Epoch 53/300\n",
      "230/230 [==============================] - 10s 45ms/step - loss: 0.5000 - precision_2: 0.7462 - recall_2: 0.5574 - accuracy: 0.7635 - val_loss: 0.7071 - val_precision_2: 0.5393 - val_recall_2: 0.4238 - val_accuracy: 0.6488\n",
      "Epoch 54/300\n",
      "230/230 [==============================] - 10s 45ms/step - loss: 0.4992 - precision_2: 0.7443 - recall_2: 0.5548 - accuracy: 0.7621 - val_loss: 0.7090 - val_precision_2: 0.5225 - val_recall_2: 0.4428 - val_accuracy: 0.6399\n",
      "Epoch 55/300\n",
      "230/230 [==============================] - 10s 45ms/step - loss: 0.4975 - precision_2: 0.7498 - recall_2: 0.5614 - accuracy: 0.7658 - val_loss: 0.7232 - val_precision_2: 0.5105 - val_recall_2: 0.3966 - val_accuracy: 0.6318\n",
      "Epoch 56/300\n",
      "230/230 [==============================] - 10s 44ms/step - loss: 0.4850 - precision_2: 0.7582 - recall_2: 0.5726 - accuracy: 0.7717 - val_loss: 0.7191 - val_precision_2: 0.5381 - val_recall_2: 0.4102 - val_accuracy: 0.6474\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 0.6814 - precision_2: 0.4801 - recall_2: 0.4768 - accuracy: 0.6113\n",
      "Training took 1258 seconds to complete.\n",
      "38344 42933\n",
      "4589\n",
      "12258 12258\n",
      "Epoch 1/300\n",
      "230/230 [==============================] - 24s 91ms/step - loss: 0.6584 - precision_3: 0.4091 - recall_3: 0.0294 - accuracy: 0.6208 - val_loss: 0.6555 - val_precision_3: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_accuracy: 0.6258\n",
      "Epoch 2/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.6527 - precision_3: 0.5252 - recall_3: 0.0265 - accuracy: 0.6266 - val_loss: 0.6580 - val_precision_3: 0.4706 - val_recall_3: 0.1671 - val_accuracy: 0.6179\n",
      "Epoch 3/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.6503 - precision_3: 0.4991 - recall_3: 0.0981 - accuracy: 0.6255 - val_loss: 0.6542 - val_precision_3: 0.6667 - val_recall_3: 0.0018 - val_accuracy: 0.6261\n",
      "Epoch 4/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.6479 - precision_3: 0.5144 - recall_3: 0.0650 - accuracy: 0.6270 - val_loss: 0.6585 - val_precision_3: 0.4167 - val_recall_3: 0.0045 - val_accuracy: 0.6251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.6453 - precision_3: 0.4972 - recall_3: 0.0643 - accuracy: 0.6254 - val_loss: 0.6545 - val_precision_3: 0.4670 - val_recall_3: 0.0899 - val_accuracy: 0.6210\n",
      "Epoch 6/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.6452 - precision_3: 0.5010 - recall_3: 0.0897 - accuracy: 0.6258 - val_loss: 0.6626 - val_precision_3: 0.3750 - val_recall_3: 0.0054 - val_accuracy: 0.6244\n",
      "Epoch 7/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.6417 - precision_3: 0.5087 - recall_3: 0.1162 - accuracy: 0.6271 - val_loss: 0.6571 - val_precision_3: 0.5098 - val_recall_3: 0.0708 - val_accuracy: 0.6268\n",
      "Epoch 8/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.6410 - precision_3: 0.5020 - recall_3: 0.1391 - accuracy: 0.6261 - val_loss: 0.6583 - val_precision_3: 0.5714 - val_recall_3: 0.0036 - val_accuracy: 0.6261\n",
      "Epoch 9/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.6408 - precision_3: 0.5479 - recall_3: 0.1329 - accuracy: 0.6343 - val_loss: 0.6568 - val_precision_3: 0.4483 - val_recall_3: 0.1971 - val_accuracy: 0.6088\n",
      "Epoch 10/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.6360 - precision_3: 0.5359 - recall_3: 0.1900 - accuracy: 0.6352 - val_loss: 0.6548 - val_precision_3: 0.4482 - val_recall_3: 0.2434 - val_accuracy: 0.6047\n",
      "Epoch 11/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.6358 - precision_3: 0.5431 - recall_3: 0.1671 - accuracy: 0.6356 - val_loss: 0.6579 - val_precision_3: 0.4618 - val_recall_3: 0.2144 - val_accuracy: 0.6125\n",
      "Epoch 12/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.6371 - precision_3: 0.5421 - recall_3: 0.2219 - accuracy: 0.6386 - val_loss: 0.6526 - val_precision_3: 0.4834 - val_recall_3: 0.2243 - val_accuracy: 0.6200\n",
      "Epoch 13/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.6436 - precision_3: 0.5126 - recall_3: 0.1475 - accuracy: 0.6284 - val_loss: 0.6570 - val_precision_3: 0.5126 - val_recall_3: 0.0926 - val_accuracy: 0.6275\n",
      "Epoch 14/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.6389 - precision_3: 0.5185 - recall_3: 0.2038 - accuracy: 0.6311 - val_loss: 0.6552 - val_precision_3: 0.4865 - val_recall_3: 0.2134 - val_accuracy: 0.6213\n",
      "Epoch 15/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.6356 - precision_3: 0.5391 - recall_3: 0.2281 - accuracy: 0.6380 - val_loss: 0.6560 - val_precision_3: 0.4815 - val_recall_3: 0.1889 - val_accuracy: 0.6203\n",
      "Epoch 16/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.6324 - precision_3: 0.5364 - recall_3: 0.2568 - accuracy: 0.6387 - val_loss: 0.6595 - val_precision_3: 0.4824 - val_recall_3: 0.1244 - val_accuracy: 0.6224\n",
      "Epoch 17/300\n",
      "230/230 [==============================] - 20s 88ms/step - loss: 0.6290 - precision_3: 0.5529 - recall_3: 0.2488 - accuracy: 0.6435 - val_loss: 0.6581 - val_precision_3: 0.4808 - val_recall_3: 0.2498 - val_accuracy: 0.6183\n",
      "Epoch 18/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.6261 - precision_3: 0.5564 - recall_3: 0.2793 - accuracy: 0.6469 - val_loss: 0.6641 - val_precision_3: 0.4940 - val_recall_3: 0.1508 - val_accuracy: 0.6244\n",
      "Epoch 19/300\n",
      "230/230 [==============================] - 20s 88ms/step - loss: 0.6215 - precision_3: 0.5547 - recall_3: 0.2928 - accuracy: 0.6473 - val_loss: 0.6637 - val_precision_3: 0.4718 - val_recall_3: 0.2961 - val_accuracy: 0.6125\n",
      "Epoch 20/300\n",
      "230/230 [==============================] - 20s 88ms/step - loss: 0.6204 - precision_3: 0.5690 - recall_3: 0.2964 - accuracy: 0.6526 - val_loss: 0.6576 - val_precision_3: 0.4842 - val_recall_3: 0.2643 - val_accuracy: 0.6193\n",
      "Epoch 21/300\n",
      "230/230 [==============================] - 20s 88ms/step - loss: 0.6172 - precision_3: 0.5854 - recall_3: 0.2939 - accuracy: 0.6577 - val_loss: 0.6637 - val_precision_3: 0.4820 - val_recall_3: 0.1944 - val_accuracy: 0.6203\n",
      "Epoch 22/300\n",
      "230/230 [==============================] - 20s 89ms/step - loss: 0.6130 - precision_3: 0.5880 - recall_3: 0.2888 - accuracy: 0.6580 - val_loss: 0.6698 - val_precision_3: 0.4788 - val_recall_3: 0.2252 - val_accuracy: 0.6183\n",
      "Epoch 23/300\n",
      "230/230 [==============================] - 20s 89ms/step - loss: 0.6102 - precision_3: 0.5825 - recall_3: 0.3207 - accuracy: 0.6596 - val_loss: 0.6740 - val_precision_3: 0.4870 - val_recall_3: 0.2216 - val_accuracy: 0.6213\n",
      "Epoch 24/300\n",
      "230/230 [==============================] - 20s 89ms/step - loss: 0.6041 - precision_3: 0.5901 - recall_3: 0.3436 - accuracy: 0.6649 - val_loss: 0.6758 - val_precision_3: 0.4762 - val_recall_3: 0.2361 - val_accuracy: 0.6169\n",
      "Epoch 25/300\n",
      "230/230 [==============================] - 20s 89ms/step - loss: 0.6017 - precision_3: 0.5901 - recall_3: 0.3473 - accuracy: 0.6654 - val_loss: 0.6711 - val_precision_3: 0.4536 - val_recall_3: 0.3061 - val_accuracy: 0.6023\n",
      "Epoch 26/300\n",
      "230/230 [==============================] - 20s 89ms/step - loss: 0.5969 - precision_3: 0.5900 - recall_3: 0.3738 - accuracy: 0.6683 - val_loss: 0.6730 - val_precision_3: 0.4800 - val_recall_3: 0.2071 - val_accuracy: 0.6193\n",
      "Epoch 27/300\n",
      "230/230 [==============================] - 20s 89ms/step - loss: 0.5952 - precision_3: 0.5981 - recall_3: 0.3676 - accuracy: 0.6708 - val_loss: 0.6759 - val_precision_3: 0.4842 - val_recall_3: 0.2643 - val_accuracy: 0.6193\n",
      "Epoch 28/300\n",
      "230/230 [==============================] - 20s 89ms/step - loss: 0.5934 - precision_3: 0.5962 - recall_3: 0.3963 - accuracy: 0.6735 - val_loss: 0.6768 - val_precision_3: 0.4803 - val_recall_3: 0.2652 - val_accuracy: 0.6176\n",
      "Epoch 29/300\n",
      "230/230 [==============================] - 20s 89ms/step - loss: 0.5839 - precision_3: 0.6174 - recall_3: 0.4039 - accuracy: 0.6832 - val_loss: 0.6786 - val_precision_3: 0.4756 - val_recall_3: 0.3188 - val_accuracy: 0.6135\n",
      "Epoch 30/300\n",
      "230/230 [==============================] - 20s 89ms/step - loss: 0.5839 - precision_3: 0.6015 - recall_3: 0.4304 - accuracy: 0.6800 - val_loss: 0.6774 - val_precision_3: 0.4736 - val_recall_3: 0.2443 - val_accuracy: 0.6156\n",
      "Epoch 31/300\n",
      "230/230 [==============================] - 20s 89ms/step - loss: 0.5822 - precision_3: 0.6115 - recall_3: 0.4083 - accuracy: 0.6814 - val_loss: 0.6785 - val_precision_3: 0.4721 - val_recall_3: 0.3224 - val_accuracy: 0.6115\n",
      "Epoch 32/300\n",
      "230/230 [==============================] - 20s 89ms/step - loss: 0.5723 - precision_3: 0.6334 - recall_3: 0.4373 - accuracy: 0.6946 - val_loss: 0.6944 - val_precision_3: 0.4949 - val_recall_3: 0.2643 - val_accuracy: 0.6237\n",
      "Epoch 33/300\n",
      "230/230 [==============================] - 20s 89ms/step - loss: 0.5847 - precision_3: 0.6182 - recall_3: 0.4028 - accuracy: 0.6833 - val_loss: 0.6895 - val_precision_3: 0.4601 - val_recall_3: 0.2407 - val_accuracy: 0.6101\n",
      "Epoch 34/300\n",
      "230/230 [==============================] - 20s 89ms/step - loss: 0.5800 - precision_3: 0.6253 - recall_3: 0.4195 - accuracy: 0.6886 - val_loss: 0.6997 - val_precision_3: 0.4446 - val_recall_3: 0.3279 - val_accuracy: 0.5952\n",
      "Epoch 35/300\n",
      "230/230 [==============================] - 20s 89ms/step - loss: 0.5768 - precision_3: 0.6300 - recall_3: 0.4348 - accuracy: 0.6928 - val_loss: 0.6882 - val_precision_3: 0.4743 - val_recall_3: 0.2434 - val_accuracy: 0.6159\n",
      "Epoch 36/300\n",
      "230/230 [==============================] - 20s 89ms/step - loss: 0.5683 - precision_3: 0.6372 - recall_3: 0.4352 - accuracy: 0.6958 - val_loss: 0.7050 - val_precision_3: 0.4762 - val_recall_3: 0.2725 - val_accuracy: 0.6156\n",
      "Epoch 37/300\n",
      "230/230 [==============================] - 20s 89ms/step - loss: 0.5654 - precision_3: 0.6346 - recall_3: 0.4392 - accuracy: 0.6954 - val_loss: 0.7080 - val_precision_3: 0.4830 - val_recall_3: 0.2970 - val_accuracy: 0.6179\n",
      "Epoch 38/300\n",
      "230/230 [==============================] - 20s 88ms/step - loss: 0.5614 - precision_3: 0.6414 - recall_3: 0.4679 - accuracy: 0.7029 - val_loss: 0.7105 - val_precision_3: 0.4564 - val_recall_3: 0.3379 - val_accuracy: 0.6016\n",
      "Epoch 39/300\n",
      "230/230 [==============================] - 20s 89ms/step - loss: 0.5543 - precision_3: 0.6445 - recall_3: 0.4715 - accuracy: 0.7048 - val_loss: 0.7340 - val_precision_3: 0.4553 - val_recall_3: 0.2961 - val_accuracy: 0.6040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/300\n",
      "230/230 [==============================] - 20s 89ms/step - loss: 0.5511 - precision_3: 0.6422 - recall_3: 0.4980 - accuracy: 0.7082 - val_loss: 0.7157 - val_precision_3: 0.4606 - val_recall_3: 0.2707 - val_accuracy: 0.6084\n",
      "Epoch 41/300\n",
      "230/230 [==============================] - 20s 89ms/step - loss: 0.5461 - precision_3: 0.6660 - recall_3: 0.4933 - accuracy: 0.7177 - val_loss: 0.7200 - val_precision_3: 0.4699 - val_recall_3: 0.3261 - val_accuracy: 0.6101\n",
      "Epoch 42/300\n",
      "230/230 [==============================] - 20s 89ms/step - loss: 0.5522 - precision_3: 0.6497 - recall_3: 0.4697 - accuracy: 0.7067 - val_loss: 0.7195 - val_precision_3: 0.4861 - val_recall_3: 0.3342 - val_accuracy: 0.6186\n",
      "Epoch 43/300\n",
      "230/230 [==============================] - 20s 89ms/step - loss: 0.5446 - precision_3: 0.6704 - recall_3: 0.4944 - accuracy: 0.7197 - val_loss: 0.7337 - val_precision_3: 0.4682 - val_recall_3: 0.3279 - val_accuracy: 0.6091\n",
      "Epoch 44/300\n",
      "230/230 [==============================] - 20s 89ms/step - loss: 0.5534 - precision_3: 0.6500 - recall_3: 0.4904 - accuracy: 0.7104 - val_loss: 0.7175 - val_precision_3: 0.4716 - val_recall_3: 0.2189 - val_accuracy: 0.6159\n",
      "Epoch 45/300\n",
      "230/230 [==============================] - 20s 89ms/step - loss: 0.5428 - precision_3: 0.6812 - recall_3: 0.4657 - accuracy: 0.7184 - val_loss: 0.7351 - val_precision_3: 0.4604 - val_recall_3: 0.2852 - val_accuracy: 0.6074\n",
      "Epoch 46/300\n",
      "230/230 [==============================] - 20s 89ms/step - loss: 0.5342 - precision_3: 0.6821 - recall_3: 0.4918 - accuracy: 0.7240 - val_loss: 0.7398 - val_precision_3: 0.4620 - val_recall_3: 0.3588 - val_accuracy: 0.6037\n",
      "Epoch 47/300\n",
      "230/230 [==============================] - 20s 89ms/step - loss: 0.5304 - precision_3: 0.6842 - recall_3: 0.5053 - accuracy: 0.7275 - val_loss: 0.7392 - val_precision_3: 0.4636 - val_recall_3: 0.3070 - val_accuracy: 0.6077\n",
      "Epoch 48/300\n",
      "230/230 [==============================] - 20s 89ms/step - loss: 0.5320 - precision_3: 0.6766 - recall_3: 0.5212 - accuracy: 0.7275 - val_loss: 0.7508 - val_precision_3: 0.4656 - val_recall_3: 0.3442 - val_accuracy: 0.6067\n",
      "Epoch 49/300\n",
      "230/230 [==============================] - 21s 89ms/step - loss: 0.5214 - precision_3: 0.6957 - recall_3: 0.5191 - accuracy: 0.7350 - val_loss: 0.7406 - val_precision_3: 0.4726 - val_recall_3: 0.3842 - val_accuracy: 0.6091\n",
      "Epoch 50/300\n",
      "230/230 [==============================] - 20s 89ms/step - loss: 0.5181 - precision_3: 0.6799 - recall_3: 0.5340 - accuracy: 0.7314 - val_loss: 0.7566 - val_precision_3: 0.4663 - val_recall_3: 0.3579 - val_accuracy: 0.6064\n",
      "Epoch 51/300\n",
      "230/230 [==============================] - 20s 89ms/step - loss: 0.5202 - precision_3: 0.6893 - recall_3: 0.5318 - accuracy: 0.7350 - val_loss: 0.7717 - val_precision_3: 0.4540 - val_recall_3: 0.3678 - val_accuracy: 0.5979\n",
      "Epoch 52/300\n",
      "230/230 [==============================] - 20s 89ms/step - loss: 0.5164 - precision_3: 0.6869 - recall_3: 0.5521 - accuracy: 0.7381 - val_loss: 0.7447 - val_precision_3: 0.4722 - val_recall_3: 0.3470 - val_accuracy: 0.6105\n",
      "Epoch 53/300\n",
      "230/230 [==============================] - 20s 89ms/step - loss: 0.5074 - precision_3: 0.7011 - recall_3: 0.5529 - accuracy: 0.7444 - val_loss: 0.7506 - val_precision_3: 0.4873 - val_recall_3: 0.3824 - val_accuracy: 0.6183\n",
      "Epoch 54/300\n",
      "230/230 [==============================] - 20s 89ms/step - loss: 0.5105 - precision_3: 0.6867 - recall_3: 0.5532 - accuracy: 0.7382 - val_loss: 0.7761 - val_precision_3: 0.4699 - val_recall_3: 0.3115 - val_accuracy: 0.6108\n",
      "Epoch 55/300\n",
      "230/230 [==============================] - 20s 89ms/step - loss: 0.5145 - precision_3: 0.6954 - recall_3: 0.5267 - accuracy: 0.7365 - val_loss: 0.7635 - val_precision_3: 0.4637 - val_recall_3: 0.3252 - val_accuracy: 0.6067\n",
      "Epoch 56/300\n",
      "230/230 [==============================] - 20s 89ms/step - loss: 0.5085 - precision_3: 0.7048 - recall_3: 0.5456 - accuracy: 0.7444 - val_loss: 0.7615 - val_precision_3: 0.4797 - val_recall_3: 0.3215 - val_accuracy: 0.6156\n",
      "Epoch 57/300\n",
      "230/230 [==============================] - 20s 88ms/step - loss: 0.4962 - precision_3: 0.7136 - recall_3: 0.5547 - accuracy: 0.7499 - val_loss: 0.7775 - val_precision_3: 0.4720 - val_recall_3: 0.3370 - val_accuracy: 0.6108\n",
      "Epoch 58/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.4996 - precision_3: 0.7072 - recall_3: 0.5510 - accuracy: 0.7465 - val_loss: 0.7758 - val_precision_3: 0.4806 - val_recall_3: 0.3606 - val_accuracy: 0.6149\n",
      "Epoch 59/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.4862 - precision_3: 0.7126 - recall_3: 0.5783 - accuracy: 0.7548 - val_loss: 0.8056 - val_precision_3: 0.4685 - val_recall_3: 0.2970 - val_accuracy: 0.6108\n",
      "Epoch 60/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.4853 - precision_3: 0.7175 - recall_3: 0.5710 - accuracy: 0.7552 - val_loss: 0.7786 - val_precision_3: 0.4648 - val_recall_3: 0.3960 - val_accuracy: 0.6033\n",
      "Epoch 61/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.4834 - precision_3: 0.7175 - recall_3: 0.5859 - accuracy: 0.7586 - val_loss: 0.8098 - val_precision_3: 0.4219 - val_recall_3: 0.5032 - val_accuracy: 0.5561\n",
      "Epoch 62/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.4817 - precision_3: 0.7213 - recall_3: 0.5855 - accuracy: 0.7601 - val_loss: 0.7900 - val_precision_3: 0.4721 - val_recall_3: 0.3379 - val_accuracy: 0.6108\n",
      "Epoch 63/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.4886 - precision_3: 0.7087 - recall_3: 0.6037 - accuracy: 0.7588 - val_loss: 0.7768 - val_precision_3: 0.4681 - val_recall_3: 0.3460 - val_accuracy: 0.6081\n",
      "Epoch 64/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.4767 - precision_3: 0.7271 - recall_3: 0.5932 - accuracy: 0.7643 - val_loss: 0.8157 - val_precision_3: 0.4729 - val_recall_3: 0.3569 - val_accuracy: 0.6105\n",
      "Epoch 65/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.4708 - precision_3: 0.7332 - recall_3: 0.6121 - accuracy: 0.7714 - val_loss: 0.8045 - val_precision_3: 0.4576 - val_recall_3: 0.4024 - val_accuracy: 0.5979\n",
      "Epoch 66/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.4736 - precision_3: 0.7287 - recall_3: 0.6001 - accuracy: 0.7667 - val_loss: 0.8156 - val_precision_3: 0.4683 - val_recall_3: 0.3424 - val_accuracy: 0.6084\n",
      "Epoch 67/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.4948 - precision_3: 0.7104 - recall_3: 0.5765 - accuracy: 0.7535 - val_loss: 0.7953 - val_precision_3: 0.4636 - val_recall_3: 0.3524 - val_accuracy: 0.6050\n",
      "Epoch 68/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.4694 - precision_3: 0.7361 - recall_3: 0.6008 - accuracy: 0.7699 - val_loss: 0.8071 - val_precision_3: 0.4811 - val_recall_3: 0.3924 - val_accuracy: 0.6142\n",
      "Epoch 69/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.4712 - precision_3: 0.7374 - recall_3: 0.6059 - accuracy: 0.7717 - val_loss: 0.8308 - val_precision_3: 0.4800 - val_recall_3: 0.3715 - val_accuracy: 0.6142\n",
      "Epoch 70/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.4638 - precision_3: 0.7432 - recall_3: 0.6044 - accuracy: 0.7737 - val_loss: 0.8502 - val_precision_3: 0.4753 - val_recall_3: 0.3751 - val_accuracy: 0.6111\n",
      "Epoch 71/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.4592 - precision_3: 0.7407 - recall_3: 0.6204 - accuracy: 0.7766 - val_loss: 0.8328 - val_precision_3: 0.4694 - val_recall_3: 0.3488 - val_accuracy: 0.6088\n",
      "Epoch 72/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.4594 - precision_3: 0.7455 - recall_3: 0.6299 - accuracy: 0.7809 - val_loss: 0.8303 - val_precision_3: 0.4892 - val_recall_3: 0.3497 - val_accuracy: 0.6200\n",
      "Epoch 73/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.4586 - precision_3: 0.7421 - recall_3: 0.6262 - accuracy: 0.7786 - val_loss: 0.8179 - val_precision_3: 0.4715 - val_recall_3: 0.4051 - val_accuracy: 0.6074\n",
      "Epoch 74/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.4502 - precision_3: 0.7379 - recall_3: 0.6248 - accuracy: 0.7764 - val_loss: 0.8436 - val_precision_3: 0.4692 - val_recall_3: 0.4015 - val_accuracy: 0.6061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.4447 - precision_3: 0.7572 - recall_3: 0.6389 - accuracy: 0.7881 - val_loss: 0.8618 - val_precision_3: 0.4655 - val_recall_3: 0.3497 - val_accuracy: 0.6064\n",
      "Epoch 76/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.4430 - precision_3: 0.7548 - recall_3: 0.6306 - accuracy: 0.7850 - val_loss: 0.8583 - val_precision_3: 0.4530 - val_recall_3: 0.4024 - val_accuracy: 0.5945\n",
      "Epoch 77/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.4463 - precision_3: 0.7427 - recall_3: 0.6364 - accuracy: 0.7813 - val_loss: 0.8426 - val_precision_3: 0.4689 - val_recall_3: 0.3497 - val_accuracy: 0.6084\n",
      "Epoch 78/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.4445 - precision_3: 0.7533 - recall_3: 0.6335 - accuracy: 0.7852 - val_loss: 0.8409 - val_precision_3: 0.4735 - val_recall_3: 0.4060 - val_accuracy: 0.6088\n",
      "Epoch 79/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.4636 - precision_3: 0.7241 - recall_3: 0.6273 - accuracy: 0.7710 - val_loss: 0.8435 - val_precision_3: 0.4645 - val_recall_3: 0.3451 - val_accuracy: 0.6061\n",
      "Epoch 80/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.4453 - precision_3: 0.7546 - recall_3: 0.6299 - accuracy: 0.7847 - val_loss: 0.8170 - val_precision_3: 0.4757 - val_recall_3: 0.3560 - val_accuracy: 0.6122\n",
      "Epoch 81/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.4662 - precision_3: 0.7336 - recall_3: 0.6041 - accuracy: 0.7696 - val_loss: 0.8403 - val_precision_3: 0.4850 - val_recall_3: 0.3515 - val_accuracy: 0.6176\n",
      "Epoch 82/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.4351 - precision_3: 0.7533 - recall_3: 0.6335 - accuracy: 0.7852 - val_loss: 0.8705 - val_precision_3: 0.4681 - val_recall_3: 0.3669 - val_accuracy: 0.6071\n",
      "Epoch 83/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.4369 - precision_3: 0.7589 - recall_3: 0.6357 - accuracy: 0.7880 - val_loss: 0.8872 - val_precision_3: 0.4603 - val_recall_3: 0.3370 - val_accuracy: 0.6040\n",
      "Epoch 84/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.4413 - precision_3: 0.7624 - recall_3: 0.6317 - accuracy: 0.7884 - val_loss: 0.8830 - val_precision_3: 0.4514 - val_recall_3: 0.3415 - val_accuracy: 0.5982\n",
      "Epoch 85/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.4336 - precision_3: 0.7538 - recall_3: 0.6440 - accuracy: 0.7880 - val_loss: 0.8757 - val_precision_3: 0.4754 - val_recall_3: 0.2988 - val_accuracy: 0.6142\n",
      "Epoch 86/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.4341 - precision_3: 0.7722 - recall_3: 0.6306 - accuracy: 0.7921 - val_loss: 0.8678 - val_precision_3: 0.4713 - val_recall_3: 0.3878 - val_accuracy: 0.6081\n",
      "Epoch 87/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.4222 - precision_3: 0.7800 - recall_3: 0.6517 - accuracy: 0.8008 - val_loss: 0.8884 - val_precision_3: 0.4631 - val_recall_3: 0.3760 - val_accuracy: 0.6033\n",
      "Epoch 88/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.4380 - precision_3: 0.7662 - recall_3: 0.6226 - accuracy: 0.7876 - val_loss: 0.8561 - val_precision_3: 0.4736 - val_recall_3: 0.3915 - val_accuracy: 0.6094\n",
      "Epoch 89/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.4215 - precision_3: 0.7676 - recall_3: 0.6564 - accuracy: 0.7970 - val_loss: 0.8984 - val_precision_3: 0.4734 - val_recall_3: 0.3878 - val_accuracy: 0.6094\n",
      "Epoch 90/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.4223 - precision_3: 0.7651 - recall_3: 0.6520 - accuracy: 0.7948 - val_loss: 0.8933 - val_precision_3: 0.4761 - val_recall_3: 0.3887 - val_accuracy: 0.6111\n",
      "Epoch 91/300\n",
      "230/230 [==============================] - 20s 87ms/step - loss: 0.4312 - precision_3: 0.7655 - recall_3: 0.6473 - accuracy: 0.7937 - val_loss: 0.8563 - val_precision_3: 0.4638 - val_recall_3: 0.3669 - val_accuracy: 0.6044\n",
      "62/62 [==============================] - 1s 15ms/step - loss: 0.8039 - precision_3: 0.4273 - recall_3: 0.5156 - accuracy: 0.5596\n",
      "Training took 2511 seconds to complete.\n"
     ]
    }
   ],
   "source": [
    "seq_result = {}\n",
    "lookback = [12, 24, 48, 72]\n",
    "\n",
    "monitor = 'val_recall'\n",
    "mode = 'max'\n",
    "\n",
    "count = 0\n",
    "for hour in lookback:\n",
    "    if count  == 0:\n",
    "        monitor = \"val_recall\"\n",
    "    else:\n",
    "        monitor = f\"val_recall_{count}\"\n",
    "    \n",
    "    count += 1\n",
    "    start = datetime.now()\n",
    "\n",
    "    wba123_fullalarm = pd.read_excel(\"Data/WBA123_FullAlarm.xlsx\", usecols = \"B,C,D,F,M\")\n",
    "    wba123 = seven_days_LSTM(\"WBA123\", wba123_fullalarm, hour, 3)\n",
    "\n",
    "    wba124_fullalarm = pd.read_excel(\"Data/WBA124_FullAlarm.xlsx\", usecols = \"B,C,D,F,M\")\n",
    "    wba124 = seven_days_LSTM(\"WBA124\", wba124_fullalarm, hour, 3)\n",
    "\n",
    "    wba126_fullalarm = pd.read_excel(\"Data/WBA126_FullAlarm.xlsx\", usecols = \"B,C,D,F,M\")\n",
    "    wba126 = seven_days_LSTM(\"WBA126\", wba126_fullalarm, hour, 3)\n",
    "\n",
    "    wba127_fullalarm = pd.read_excel(\"Data/WBA127_FullAlarm.xlsx\", usecols = \"B,C,D,F,M\")\n",
    "    wba127 = seven_days_LSTM(\"WBA127\", wba127_fullalarm, hour, 3)\n",
    "\n",
    "    wba128_fullalarm = pd.read_excel(\"Data/WBA128_FullAlarm.xlsx\", usecols = \"B,C,D,F,M\")\n",
    "    wba128 = seven_days_LSTM(\"WBA128\", wba128_fullalarm, hour, 3)\n",
    "\n",
    "    tmp1 = np.concatenate((wba123.X_seq, wba124.X_seq, wba126.X_seq, wba127.X_seq, wba128.X_seq))\n",
    "    encoded_X_seq, n_alarms = label_encode(tmp1)\n",
    "    target = np.concatenate((wba123.major_down_arr, wba124.major_down_arr, wba126.major_down_arr, wba127.major_down_arr, wba128.major_down_arr))\n",
    "\n",
    "#     #remove empty datapoints to help reduce loss\n",
    "#     encoded_X_seq, target = remove_empty_datapoint(encoded_X_seq, target)\n",
    "    \n",
    "    #hand pick negative data to counter imbalance\n",
    "    encoded_X_seq, target = randomly_select_negative(encoded_X_seq, target, 0.2)\n",
    "    print(len(encoded_X_seq), len(target))\n",
    "\n",
    "    #shuffle the X and Y values to make generalize better\n",
    "    shuffled = shuffle(encoded_X_seq, target)\n",
    "    encoded_X_seq = shuffled[0]\n",
    "    target = shuffled[1]\n",
    "\n",
    "    # padding to average length\n",
    "    mean_length = int(np.mean([len(x) for x in encoded_X_seq]))\n",
    "    padded_alarm = np.zeros([len(encoded_X_seq), mean_length])\n",
    "    for i,j in enumerate(encoded_X_seq):\n",
    "        padded_alarm[i][0:len(j)] = j[:mean_length]\n",
    "\n",
    "    #train_val_test split\n",
    "    X_train_seq, X_val_seq, y_train_seq, y_val_seq =  train_test_split(padded_alarm, target, test_size=0.4, random_state=42, stratify=target)\n",
    "    X_val_seq, X_test_seq, y_val_seq, y_test_seq =  train_test_split(X_val_seq, y_val_seq, test_size=0.4, random_state=42, stratify=y_val_seq)\n",
    "\n",
    "    model = create_model(padded_alarm, n_alarms)\n",
    "    # model seems to be overfitting, try to reduce overfitting by reduce LR, but model should take longer to converge so use a larger EPOCH\n",
    "    callbacks = [ReduceLROnPlateau(monitor=monitor, factor=0.2, patience=5, min_lr=0.001), \\\n",
    "                EarlyStopping(monitor=monitor, patience=30, mode=mode, restore_best_weights=True)]\n",
    "\n",
    "    history = model.fit(X_train_seq, y_train_seq, \n",
    "                    batch_size=BATCH_SIZE, epochs=EPOCHS, \n",
    "                    validation_data=(X_val_seq, y_val_seq), \n",
    "                    callbacks=callbacks)\n",
    "\n",
    "    evaluate = model.evaluate(X_test_seq, y_test_seq) #loss, mse\n",
    "    \n",
    "    pred = model.predict(X_test_seq)\n",
    "    classes = []\n",
    "    for ele in pred:\n",
    "        classes.append(int((ele>0.5)[0]))\n",
    "\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "    cm = confusion_matrix(y_test_seq, classes)\n",
    "    \n",
    "    seq_result[hour] = [evaluate, cm]\n",
    "\n",
    "    end = datetime.now()\n",
    "    time = end - start\n",
    "    print(f\"Training took {time.seconds} seconds to complete.\")\n",
    "    seq_result[hour, mean_length] = evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "701dbb0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{12: [[0.6287420988082886,\n",
       "   0.5336463451385498,\n",
       "   0.4633152186870575,\n",
       "   0.6475076079368591],\n",
       "  array([[932, 298],\n",
       "         [395, 341]])],\n",
       " (12, 30): [0.6287420988082886,\n",
       "  0.5336463451385498,\n",
       "  0.4633152186870575,\n",
       "  0.6475076079368591],\n",
       " 24: [[0.6324173808097839,\n",
       "   0.5170998573303223,\n",
       "   0.5142857432365417,\n",
       "   0.6386768221855164],\n",
       "  array([[877, 353],\n",
       "         [357, 378]])],\n",
       " (24, 58): [0.6324173808097839,\n",
       "  0.5170998573303223,\n",
       "  0.5142857432365417,\n",
       "  0.6386768221855164],\n",
       " 48: [[0.6813942790031433,\n",
       "   0.4801097512245178,\n",
       "   0.47683924436569214,\n",
       "   0.6113092303276062],\n",
       "  array([[850, 379],\n",
       "         [384, 350]])],\n",
       " (48, 115): [0.6813942790031433,\n",
       "  0.4801097512245178,\n",
       "  0.47683924436569214,\n",
       "  0.6113092303276062],\n",
       " 72: [[0.8038536310195923,\n",
       "   0.42728298902511597,\n",
       "   0.5156462788581848,\n",
       "   0.5596330165863037],\n",
       "  array([[719, 508],\n",
       "         [356, 379]])],\n",
       " (72, 169): [0.8038536310195923,\n",
       "  0.42728298902511597,\n",
       "  0.5156462788581848,\n",
       "  0.5596330165863037]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_result # stack Conv1D above LSTM (without removing noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cd13298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{12: [[0.6843389272689819,\n",
       "   0.4783889949321747,\n",
       "   0.7767145037651062,\n",
       "   0.5706973671913147],\n",
       "  array([[405, 531],\n",
       "         [140, 487]])],\n",
       " (12, 38): [0.6843389272689819,\n",
       "  0.4783889949321747,\n",
       "  0.7767145037651062,\n",
       "  0.5706973671913147],\n",
       " 24: [[0.6756141185760498,\n",
       "   0.4753146171569824,\n",
       "   0.7695924639701843,\n",
       "   0.5767813324928284],\n",
       "  array([[448, 542],\n",
       "         [147, 491]])],\n",
       " (24, 71): [0.6756141185760498,\n",
       "  0.4753146171569824,\n",
       "  0.7695924639701843,\n",
       "  0.5767813324928284],\n",
       " 48: [[0.686356782913208,\n",
       "   0.4554730951786041,\n",
       "   0.7577160596847534,\n",
       "   0.5595026612281799],\n",
       "  array([[454, 587],\n",
       "         [157, 491]])],\n",
       " (48, 133): [0.686356782913208,\n",
       "  0.4554730951786041,\n",
       "  0.7577160596847534,\n",
       "  0.5595026612281799],\n",
       " 72: [[0.6810200810432434,\n",
       "   0.4411483108997345,\n",
       "   0.7048929929733276,\n",
       "   0.5485183000564575],\n",
       "  array([[483, 584],\n",
       "         [193, 461]])],\n",
       " (72, 193): [0.6810200810432434,\n",
       "  0.4411483108997345,\n",
       "  0.7048929929733276,\n",
       "  0.5485183000564575]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_result # downsample negative data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1b9a45c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(12, 34): [0.7047469019889832,\n",
       "  0.1626969575881958,\n",
       "  0.7081339955329895,\n",
       "  0.534690797328949],\n",
       " (24, 66): [0.7164930105209351,\n",
       "  0.15516085922718048,\n",
       "  0.7257053256034851,\n",
       "  0.517192006111145],\n",
       " (48, 126): [0.6946452856063843,\n",
       "  0.14175792038440704,\n",
       "  0.7391975522041321,\n",
       "  0.4753846228122711]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_result # monitor val recall\n",
    "# {72: [0.7307, 0.1300, 0.8012, 0.3925]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c67a3624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(24, 34): [0.5954988598823547,\n",
       "  0.1780264526605606,\n",
       "  0.5582137107849121,\n",
       "  0.6430995464324951],\n",
       " (12, 34): [0.6063371300697327,\n",
       "  0.17000912129878998,\n",
       "  0.5948963165283203,\n",
       "  0.6087858080863953],\n",
       " (24, 66): [0.6042451858520508,\n",
       "  0.1820913404226303,\n",
       "  0.47492164373397827,\n",
       "  0.6962750554084778],\n",
       " (48, 126): [0.5850675106048584,\n",
       "  0.1593644618988037,\n",
       "  0.5108024477958679,\n",
       "  0.6473504304885864],\n",
       " (72, 185): [0.5972234010696411,\n",
       "  0.1689220666885376,\n",
       "  0.5535168051719666,\n",
       "  0.6538076400756836]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_result # train on 5 machines same EQ family with shuffle and masking and empty datapoints removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8529d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_result # everything on top but trained on a bidirectional LSTM layer\n",
    "# {12: [1.4355, 0.2238, 0.2041, 0.8222]}\n",
    "# {24: [1.0140, 0.2651, 0.2962, 0.8258]}\n",
    "# {48: [0.9016, 0.2873, 0.3210, 0.8366]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c179781",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d11b162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "plot_model(model, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef04aea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENV-MAL-VE",
   "language": "python",
   "name": "env-mal-ve_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
