{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f01b393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Input, \\\n",
    "                                        BatchNormalization, Embedding, Masking,\\\n",
    "                                        Bidirectional, Conv1D, MaxPooling1D, Flatten, concatenate, MaxPooling1D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "import lightgbm\n",
    "\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 300\n",
    "\n",
    "from datetime import datetime, timedelta, date\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cx_Oracle\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import joblib\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b95cdaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time(start_date:str, end_date:str, hour:int):\n",
    "        start = datetime.strptime(start_date, '%d/%m/%Y')\n",
    "        end = datetime.strptime(end_date, '%d/%m/%Y')\n",
    "\n",
    "        dates = []\n",
    "        while start<=end:\n",
    "            row = [start]\n",
    "            dates.append(row)\n",
    "            start += timedelta(hours=hour)\n",
    "\n",
    "        return pd.DataFrame(dates, columns=['TIMESTAMP'])\n",
    "    \n",
    "def query_status(eq_id):\n",
    "    try:\n",
    "        oracle_string = \"oracle+cx_oracle://{username}:{password}@{hostname}:{port}/{database}\"\n",
    "        engine = create_engine(\n",
    "            oracle_string.format(\n",
    "                username = 'TFM4CEBERUS',\n",
    "                password = 'TFM4CEBERUS',\n",
    "                hostname = 'ome-db.bth.infineon.com',\n",
    "                port = '1538',\n",
    "                database = 'ome'\n",
    "                )\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "    query = f\"\"\"select EQ_ID, TIMESTAMP_START, TIMESTAMP_END, DURATION, STATE_NAME, LEVEL3_NAME, LEVEL3 \n",
    "            from (SELECT\n",
    "              eq.eq_id, eq.name, eq.eq_type_ident\n",
    "            , data.timestamp_start,data.timestamp_end\n",
    "            , ROUND((data.timestamp_end - data.timestamp_start)*24*60*60,0) AS Duration\n",
    "            , data.tr25_3_status,data.tr25_4_status,data.tr25_5_status,data.eq_status\n",
    "            , level5s.state_name\n",
    "            , level5.state_name Level5_Name, level5.state_sign Level5\n",
    "            , level4.state_name Level4_Name, level4.state_sign Level4\n",
    "            , level3.state_name Level3_Name, level3.state_sign Level3\n",
    "            ,mh.device\n",
    "            ,mh.package,\n",
    "            mh.lotid as lot,\n",
    "            mh.product,\n",
    "            mh.operation\n",
    "\n",
    "            FROM OMEDATA.EQUIPMENT_STATE_HISTORY data\n",
    "            , OMEADMIN.EQUIPMENT_INSTANCES eq\n",
    "            , V_EQ_STATES level5s\n",
    "            , OMEADMIN.DEF_STANDARD_STATEMODEL level5\n",
    "            , OMEADMIN.DEF_STANDARD_STATEMODEL level4\n",
    "            , OMEADMIN.DEF_STANDARD_STATEMODEL level3\n",
    "            , OMEDATA.METAKEY_HISTORY mh\n",
    "\n",
    "            WHERE data.eq_ident  = eq.eq_ident\n",
    "            AND  data.eq_status = level5s.state_ident(+)\n",
    "            AND level5.state_ident = data.tr25_5_status\n",
    "            AND level4.state_ident = data.tr25_4_status\n",
    "            AND level3.state_ident = data.tr25_3_status\n",
    "            AND  data.metakey_ident =mh.ident(+)\n",
    "            and data.timestamp_start > sysdate - 1050)\n",
    "            where eq_id = '{eq_id}'\n",
    "            ORDER BY TIMESTAMP_START\"\"\"\n",
    "\n",
    "    status = pd.read_sql(query, engine)\n",
    "    status.columns = map(lambda x: str(x).upper(), status.columns) \n",
    "\n",
    "    return status\n",
    "\n",
    "def aggregate(timeframe_table, lookback_window, status_table):\n",
    "    statename_df = pd.DataFrame(columns=status_table[\"STATE_NAME\"].unique())\n",
    "\n",
    "    for idx, row in timeframe_table.iterrows():\n",
    "        end = row[\"TIMESTAMP\"]\n",
    "        start = end - timedelta(hours=lookback_window)\n",
    "\n",
    "        ## count the frequencies of each statename, include everything since feature engineering would be performed\n",
    "        filtered_statename = status_table.loc[(status_table[\"TIMESTAMP_START\"] >= start) & \n",
    "                                              (status_table[\"TIMESTAMP_START\"] <= end)]\n",
    "        unique = filtered_statename[\"STATE_NAME\"].unique()\n",
    "        status_dict = {key:int(sum(filtered_statename.loc[filtered_statename.STATE_NAME==key][\"DURATION\"])) \n",
    "                       for key in unique}\n",
    "        \n",
    "        statename_df = statename_df.append(status_dict, ignore_index=True)\n",
    "            \n",
    "    statename_df = statename_df.fillna(0)\n",
    "    cols = statename_df.columns\n",
    "    statename_df[cols] = statename_df[cols].astype('int')\n",
    "    return statename_df\n",
    "\n",
    "\n",
    "def status_sequence(input_table, status_table, hour, scaled=False):\n",
    "        status_seq = []\n",
    "        duration_seq = []\n",
    "        \n",
    "        # validation check\n",
    "        if status_table.iloc[0][\"TIMESTAMP_START\"] > input_table.iloc[0][\"TIMESTAMP\"]:\n",
    "            raise Exception(\"Timeframe table must be a subset of the status table\")\n",
    "        if status_table.iloc[len(status_table)-1][\"TIMESTAMP_START\"] <= input_table.iloc[len(input_table)-1][\"TIMESTAMP\"]:\n",
    "                raise Exception(\"Timeframe table must be a subset of the status table\")\n",
    "        \n",
    "        for idx, row in input_table.iterrows():\n",
    "            end = row[\"TIMESTAMP\"]\n",
    "            start = end - timedelta(hours=hour)\n",
    "            \n",
    "            condition = (status_table[\"TIMESTAMP_START\"]>=start) & (status_table[\"TIMESTAMP_START\"]<=end)\n",
    "\n",
    "            table = status_table[condition]\n",
    "            status_seq.append(table[\"STATE_NAME\"].values)\n",
    "            if scaled:\n",
    "                duration_seq.append(table[\"SCALED_DURATION\"].values)\n",
    "            else:\n",
    "                duration_seq.append(table[\"DURATION\"].values)\n",
    "\n",
    "        return status_seq, duration_seq\n",
    "\n",
    "\n",
    "def major_down(input_df, status_table, hour, threshold): \n",
    "        hour = pd.Timedelta(hours=hour)\n",
    "        major_down = []\n",
    "\n",
    "        for idx, row in input_df.iterrows():\n",
    "            start = row['TIMESTAMP']\n",
    "            end = start+hour\n",
    "            frame = status_table[(status_table['TIMESTAMP_START']>start) & (status_table['TIMESTAMP_START']<end)]\n",
    "            UD = frame.loc[frame['LEVEL3']=='UDT']\n",
    "            \n",
    "            # disregard \"waiting\" in statename\n",
    "\n",
    "            if len(UD) == 0: #no record within this 6 hours:\n",
    "                major_down.append(0)\n",
    "            else:\n",
    "                time_diff = (UD['TIMESTAMP_END']-UD['TIMESTAMP_START']).dt.seconds\n",
    "                if any(time_diff>=threshold): #threshold = 3600s\n",
    "                    major_down.append(1)\n",
    "                else:\n",
    "                    major_down.append(0)\n",
    "        return np.array(major_down)\n",
    "\n",
    "def query_CAMSTAR(eq_id):\n",
    "    try:\n",
    "        oracle_string = \"oracle+cx_oracle://{username}:{password}@{hostname}:{port}/{database}\"\n",
    "        engine = create_engine(\n",
    "            oracle_string.format(\n",
    "                username = 'bth_odsprod',\n",
    "                password = 'bth_odsprodbth',\n",
    "                hostname = 'odsprod-db.bth.infineon.com',\n",
    "                port = '1523',\n",
    "                database = 'odsprod'\n",
    "                )\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "    query = f\"\"\"select EQUIPMENTNAME AS EQ_ID, TRACKINTIMESTAMP, TRACKOUTTIMESTAMP from A_WIPEQUIPMENTHISTORY t\n",
    "                where t.equipmentname = '{eq_id}'\n",
    "                ORDER BY TRACKINTIMESTAMP\"\"\"\n",
    "\n",
    "    status = pd.read_sql(query, engine)\n",
    "\n",
    "    return status\n",
    "\n",
    "def label_encode(statename_seq): # do this the manual way as we are not certain if sklearn LabelEncoder can handle 3D array\n",
    "    all_unique_statename = [set(ele) for ele in statename_seq]\n",
    "    unique_statenames = set()\n",
    "    for ele in all_unique_statename:\n",
    "        unique_statenames |= ele\n",
    "    \n",
    "    enc_label = 1  #start encoding from 1 as we have to pad the sequence with 0\n",
    "    mapping_dict = {}\n",
    "    for ele in unique_statenames:\n",
    "        mapping_dict[ele] = enc_label\n",
    "        enc_label += 1\n",
    "\n",
    "    enc_array = []\n",
    "    #X_seq is a 3D array\n",
    "    for timestamp in statename_seq:\n",
    "        tmp_arr = []\n",
    "        for ele in timestamp:\n",
    "            tmp_arr.append(mapping_dict[ele])\n",
    "        enc_array.append(np.array(tmp_arr))\n",
    "\n",
    "    return np.array(enc_array), len(unique_statenames)+1, mapping_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0056ba1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA for WBA124\n",
    "wba124_status = query_status(\"WBA124\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c92e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Baseline model on uncleaned raw data #####\n",
    "\n",
    "wba124_initial = wba124_status.copy()\n",
    "hour = 24\n",
    "beginning_major_down = []\n",
    "\n",
    "for idx, row in wba124_initial.iterrows(): \n",
    "    start = row['TIMESTAMP_START']\n",
    "    end = start+timedelta(hours=hour)\n",
    "    frame = wba124_initial[(wba124_initial['TIMESTAMP_START']>start) & \\\n",
    "                                 (wba124_initial['TIMESTAMP_START']<end)]\n",
    "    UD = frame.loc[frame['LEVEL3']=='UDT']\n",
    "\n",
    "    if len(UD) == 0: #no record within this 6 hours:\n",
    "        beginning_major_down.append(0)\n",
    "    else:\n",
    "        time_diff = (UD['TIMESTAMP_END']-UD['TIMESTAMP_START']).dt.seconds\n",
    "        if any(time_diff>=threshold): #threshold = 3600s\n",
    "            beginning_major_down.append(1)\n",
    "        else:\n",
    "            beginning_major_down.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f983d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wba124_initial[\"24 HOUR DOWN\"] = beginning_major_down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379982d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Directly train on the raw data to test performance #####\n",
    "not_feature = ['TIMESTAMP_START', 'TIMESTAMP_END', 'EQ_ID', 'LEVEL3_NAME', 'LEVEL3', 'STATE_NAME']\n",
    "\n",
    "lb = LabelEncoder()\n",
    "wba124_initial['ENC STATE NAME'] = lb.fit_transform(wba124_initial['STATE_NAME'])\n",
    "\n",
    "tmp_table = wba124_initial.drop(not_feature, axis=1)\n",
    "\n",
    "df = tmp_table[['DURATION', 'ENC STATE NAME']]\n",
    "target = tmp_table['24 HOUR DOWN'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a600c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_idx = int(0.7*len(df))\n",
    "val_idx = int(0.8*(len(df)))\n",
    "\n",
    "X_train, y_train = df[:train_idx], target[:train_idx]\n",
    "X_val, y_val = df[train_idx:val_idx], target[train_idx:val_idx]\n",
    "X_test, y_test = df[val_idx:], target[val_idx:]\n",
    "\n",
    "clf = lightgbm.LGBMClassifier(\n",
    "    boosting_type='gbdt',\n",
    "    objective='binary',\n",
    "    random_state=42,\n",
    "    class_weight='balanced',\n",
    "    reg_lambda=0.01\n",
    "            )\n",
    "\n",
    "clf.fit(X_train, y_train,\n",
    "       eval_set=(X_val, y_val),\n",
    "       eval_metric='f1',\n",
    "        verbose=True)\n",
    "\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "confusion_matrix(y_test, pred), accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b146cecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Model after Feature Engineering #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d72386",
   "metadata": {},
   "outputs": [],
   "source": [
    "wba124_duration_sort = wba124_status.sort_values('DURATION', ascending=False).head(10)\n",
    "wba124_sorted_duration_statename_val = wba124_duration_sort[\"STATE_NAME\"].values\n",
    "wba124_duration_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e17c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check these rows, how many transactions are in the middle\n",
    "for idx in wba124_duration_sort.index:\n",
    "    start = wba124_status.iloc[idx][\"TIMESTAMP_START\"]\n",
    "    end = wba124_status.iloc[idx][\"TIMESTAMP_END\"]\n",
    "    filtered = wba124_status.loc[(wba124_status.TIMESTAMP_START>=start) & \n",
    "                                (wba124_status.TIMESTAMP_START<end)]\n",
    "    print(f'In between timestamp of row {idx}, there are {len(filtered)-1} transactions')\n",
    "    \n",
    "# for status with other transaction in the middle, should they be invalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f931dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice how 2021-04-22 has the longest duration, for WBA127 & WBA124 likely for other machines as well\n",
    "# someone closed all the status on this day\n",
    "# remove these data points\n",
    "\n",
    "apr = datetime(2021,4,22)\n",
    "end = apr + timedelta(days=1)\n",
    "wba124_status[(wba124_status[\"TIMESTAMP_END\"]>=apr)&\n",
    "               (wba124_status[\"TIMESTAMP_END\"]<=end)].sort_values('DURATION', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26709f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for 0 duration\n",
    "wba124_duration_sort_asc = wba124_status.sort_values('DURATION')\n",
    "zero_seconds = wba124_status.loc[wba124_status.DURATION==0.0]\n",
    "print(f'There are {len(zero_seconds)} rows with 0.0 seconds as DURATION')\n",
    "print(f'Distribution of LEVEL3:\\n{zero_seconds[\"LEVEL3\"].value_counts()}')\n",
    "wba124_duration_sort_asc.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add83794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove one row of negative DURATION \n",
    "check_duration = wba124_status.loc[wba124_status.DURATION<0.0]\n",
    "print(len(check_duration))\n",
    "wba124_status = wba124_status.drop(index=5195, axis=0)\n",
    "check_duration1 = wba124_status.loc[wba124_status.DURATION<0.0]\n",
    "len(check_duration1) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c93cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wba124_status = wba124_status.reset_index(drop=True) # always reset index after you drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e31a4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apparently status can be uncontinuous, there might be some transaction in between timestamp of each row\n",
    "# find rows which the timestamp end is later than the next timestamp start\n",
    "# to prove all those with status in the middle are recorded wrongly\n",
    "\n",
    "# also find out rows with the same state name and is split into 2\n",
    "\n",
    "idx_row = []\n",
    "same_state_idx = []\n",
    "for idx, row in wba124_status.iterrows():\n",
    "    if idx == len(wba124_status)-1:\n",
    "        break\n",
    "    end = row[\"TIMESTAMP_END\"]\n",
    "    state1 = row[\"STATE_NAME\"]\n",
    "    next_row = wba124_status.iloc[idx+1]\n",
    "    next_start = next_row[\"TIMESTAMP_START\"]\n",
    "    state2 = next_row[\"STATE_NAME\"]\n",
    "    \n",
    "    # timestamp end of this row later than timestamp_start of next row\n",
    "    if end > next_start:\n",
    "        idx_row.append(idx)\n",
    "        \n",
    "    # same state name but broken into 2 records\n",
    "    if end == next_start and state1 == state2:\n",
    "        same_state_idx.append(idx)\n",
    "    \n",
    "print(f'Rows with overlapping transaction {idx_row}')\n",
    "print(f'There are {len(same_state_idx)} rows with the same state name as the next row with the same timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6177a7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this block is to check that the index with overlapping rows are correct\n",
    "idx = 6058\n",
    "wba124_status.iloc[idx:idx+2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a0e599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up status data \n",
    "# (do not remove special level3 as sometimes engineers would run dummy lot and CAMSTAR will not have record but TFM has)\n",
    "\n",
    "# 1. combine timestamp_end == next row timestamp_start, start from the back as there are some with continuous rows\n",
    "wba124_clean_status = wba124_status.copy()\n",
    "for idx in reversed(same_state_idx):\n",
    "    duration = sum(wba124_clean_status.iloc[idx:idx+2][\"DURATION\"])\n",
    "    wba124_clean_status.at[idx, 'DURATION'] = duration\n",
    "    wba124_clean_status.at[idx, 'TIMESTAMP_END'] = wba124_clean_status.iloc[idx+1][\"TIMESTAMP_END\"]\n",
    "    wba124_clean_status.drop(index=idx+1, axis=0, inplace=True)\n",
    "\n",
    "# validate all timestamp_end == timestamp start with same state name datapoints have been combined\n",
    "idx_rows = []\n",
    "same_state = []\n",
    "wba124_clean_status = wba124_clean_status.reset_index(drop=True)\n",
    "for idx, row in wba124_clean_status.iterrows():\n",
    "    if idx == len(wba124_clean_status)-1:\n",
    "        break\n",
    "    end = row[\"TIMESTAMP_END\"]\n",
    "    state1 = row[\"STATE_NAME\"]\n",
    "    next_row = wba124_clean_status.iloc[idx+1]\n",
    "    next_start = next_row[\"TIMESTAMP_START\"]\n",
    "    state2 = next_row[\"STATE_NAME\"]\n",
    "    \n",
    "    if end > next_start:\n",
    "        idx_rows.append(idx)\n",
    "    if end == next_start and state1 == state2:\n",
    "        same_state.append(idx)\n",
    "    \n",
    "print(f'Rows with overlapping transaction {idx_rows}')\n",
    "print(f'There are {len(same_state)} rows with the same state name as the next row with the same timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3be130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Remove duration = 0\n",
    "len1 = len(wba124_clean_status)\n",
    "wba124_clean_status = wba124_clean_status[~(wba124_clean_status.DURATION==0.0)]\n",
    "len2 = len(wba124_clean_status)\n",
    "print(f'Removed {len1-len2} rows of data with duration as 0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c71cb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Remove the rows with overlapping time frame, either due to long duration or wrong record4\n",
    "len3 = len(wba124_clean_status)\n",
    "wba124_clean_status = wba124_clean_status.drop(index=idx_rows, axis=0)\n",
    "len4 = len(wba124_clean_status)\n",
    "print(f'Removed {len3-len4} rows of data with long duration (overlapping timestamp start and end)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf93e6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Check again and make sure no overlapping timestamp (all timestamp_start must be >= last timestamp_end)\n",
    "idx_rows_check = []\n",
    "same_state_check = []\n",
    "wba124_clean_status = wba124_clean_status.reset_index(drop=True)\n",
    "for idx, row in wba124_clean_status.iterrows():\n",
    "    \n",
    "    if idx == len(wba124_clean_status)-1:\n",
    "        break\n",
    "    end = row[\"TIMESTAMP_END\"]\n",
    "    state1 = row[\"STATE_NAME\"]\n",
    "    next_row = wba124_clean_status.iloc[idx+1]\n",
    "    next_start = next_row[\"TIMESTAMP_START\"]\n",
    "    state2 = next_row[\"STATE_NAME\"]\n",
    "    \n",
    "    if end > next_start:\n",
    "        idx_rows_check.append(idx)\n",
    "    if end == next_start and state1 == state2:\n",
    "        same_state_check.append(idx)\n",
    "    \n",
    "print(f'Rows with overlapping transaction {idx_rows_check}')\n",
    "print(f'There are {len(same_state_check)} rows with the same state name as the next row with the same timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d527db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Removed a total of {len(wba124_status)-len(wba124_clean_status)} rows from {len(wba124_status)}.')\n",
    "\n",
    "wba124_clean_status = wba124_clean_status.sort_values('TIMESTAMP_START', ascending=True) # rearrange just in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5994850e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute major down and check the relationship between alarms using histogram (split into class 0 and class 1)\n",
    "wba124_timeframe_table = generate_time('5/12/2018', '31/8/2021', 24)\n",
    "wba124_major_down_arr = major_down(wba124_timeframe_table, wba124_clean_status, 24, 3600)\n",
    "wba124_timeframe_table[\"MAJOR DOWN\"] = wba124_major_down_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005e8718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# major down validation check, just make sure at least one row will show here\n",
    "timestamp_check = wba124_timeframe_table[wba124_timeframe_table[\"MAJOR DOWN\"]==1].iloc[250][\"TIMESTAMP\"]\n",
    "print(timestamp_check)\n",
    "timestampend_check = timestamp_check + timedelta(hours=24)\n",
    "filtered = wba124_clean_status[(wba124_clean_status[\"TIMESTAMP_START\"]>=timestamp_check) &\n",
    "             (wba124_clean_status[\"TIMESTAMP_START\"]<=timestampend_check)]\n",
    "filtered.loc[(filtered.LEVEL3 == \"UDT\") & (filtered.DURATION >= 3600)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88071978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram to see relationship between state name and major down\n",
    "statename_seq, duration_seq = status_sequence(wba124_timeframe_table, wba124_clean_status, 24)\n",
    "\n",
    "wba124_timeframe_table[\"STATENAME SEQ\"] = statename_seq\n",
    "wba124_timeframe_table[\"DURATION SEQ\"] = duration_seq\n",
    "\n",
    "positive = wba124_timeframe_table.loc[wba124_timeframe_table[\"MAJOR DOWN\"]==1]\n",
    "negative = wba124_timeframe_table.loc[wba124_timeframe_table[\"MAJOR DOWN\"]==0]\n",
    "\n",
    "statename_pos = positive[\"STATENAME SEQ\"].values\n",
    "statename_neg = negative[\"STATENAME SEQ\"].values\n",
    "duration_pos = positive[\"DURATION SEQ\"].values\n",
    "duration_neg = negative[\"DURATION SEQ\"].values\n",
    "\n",
    "pos = []\n",
    "dur_pos = []\n",
    "neg = []\n",
    "dur_neg = []\n",
    "for state_p, dur_p in zip(statename_pos, duration_pos):\n",
    "    pos.extend(state_p)\n",
    "    dur_pos.extend(dur_p)\n",
    "\n",
    "for state_n, dur_n in zip(statename_neg, duration_neg):\n",
    "    neg.extend(state_n)\n",
    "    dur_neg.extend(dur_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19270934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data validation check\n",
    "print(\"negative length = \", len(neg))\n",
    "print(\"positive length = \", len(pos))\n",
    "print(f'Total number of data collected = {len(neg) + len(pos)}') \n",
    "\n",
    "# the 3 extra collections is because 3 rows of data in clean_status just so happen to start at midnight\n",
    "aaa = wba124_timeframe_table.iloc[0][\"TIMESTAMP\"] - timedelta(hours=24)\n",
    "bbb = wba124_timeframe_table.iloc[len(wba124_timeframe_table)-1][\"TIMESTAMP\"]\n",
    "ccc = wba124_clean_status[(wba124_clean_status[\"TIMESTAMP_START\"]>=aaa) & (wba124_clean_status[\"TIMESTAMP_START\"]<=bbb)]\n",
    "timestamps = wba124_timeframe_table[\"TIMESTAMP\"].values\n",
    "print(f'Total length of clean status table = {len(ccc)}')\n",
    "wba124_clean_status[wba124_clean_status[\"TIMESTAMP_START\"].isin(timestamps)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1108e99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice that the top few state name frequencies are very similar for both machines\n",
    "count_helper = Counter(pos)\n",
    "NP_pos_count = count_helper.pop('Normal Production') # remove normal production to or else cannot visualize the graph\n",
    "\n",
    "pos_sort_count = {}\n",
    "for key, val in sorted(count_helper.items(), key=lambda x: x[1], reverse=True):\n",
    "    if key is not None: # putting in 'Exception' LEVEL3 will cause matplotlib error\n",
    "        pos_sort_count[key] = val\n",
    "\n",
    "plt.figure(figsize=(16,9))\n",
    "plt.bar(pos_sort_count.keys(), pos_sort_count.values(), width=0.4)\n",
    "plt.title(\"Positive class 24 hours state name distribution\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "neg_count_helper = Counter(neg)\n",
    "\n",
    "NP_neg_count = neg_count_helper.pop('Normal Production') # remove normal production to or else cannot visualize the graph\n",
    "Level3_Exception_count = neg_count_helper.pop(None) # Exception all happen during positive class\n",
    "\n",
    "neg_sort_count = {}\n",
    "for key, val in sorted(neg_count_helper.items(), key=lambda x: x[1], reverse=True):\n",
    "    if key is not None:\n",
    "        neg_sort_count[key] = val\n",
    "\n",
    "plt.figure(figsize=(16,9))\n",
    "plt.bar(neg_sort_count.keys(), neg_sort_count.values(), width=0.4)\n",
    "plt.title(\"Negative class 24 hours state name distribution\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a6080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rare occurence item, probably not helpful\n",
    "print('State name that only appears in positive class = \\n',\n",
    "      [[ele, val] for ele, val in pos_sort_count.items() if ele not in neg_sort_count.keys()])\n",
    "print('\\nOnly in negative class = \\n', [[ele, val] for ele, val in neg_sort_count.items() if ele not in pos_sort_count.keys()])\n",
    "\n",
    "# check on other equipment as well, keep rare STATE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3b9ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for DURATION outliers\n",
    "# still about 5 spots standing out, investigate\n",
    "wba124_clean_status.plot(x='TIMESTAMP_START', y='DURATION', marker='o', linestyle='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556fd902",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_dur = wba124_clean_status.sort_values('DURATION', ascending=False).head(5)\n",
    "outlier_dur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8b2f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there is any major down 24 hours after these values\n",
    "check_date = outlier_dur.iloc[0]['TIMESTAMP_END']\n",
    "end = check_date + timedelta(hours=24)\n",
    "wba124_clean_status[(wba124_clean_status['TIMESTAMP_START']>=check_date) &\n",
    "                   (wba124_clean_status['TIMESTAMP_START']<=end) &\n",
    "                   (wba124_clean_status['LEVEL3'] == 'UDT') &\n",
    "                   (wba124_clean_status['DURATION']>=3600)]\n",
    "\n",
    "# found out that for majority of the time, give long duration, high chance a major down would occur within 24 hours\n",
    "# keep the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9954e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### collect major down at each row #####\n",
    "hour = 24\n",
    "threshold = 3600\n",
    "lookback_window = 30\n",
    "\n",
    "# take out the last 2 days in order to collect major down correctly\n",
    "last_date = wba124_clean_status.iloc[len(wba124_clean_status)-1][\"TIMESTAMP_START\"]\n",
    "two_days = last_date - timedelta(days=2)\n",
    "walk_forward_dataset = wba124_clean_status[wba124_clean_status[\"TIMESTAMP_START\"]<two_days].reset_index(drop=True)\n",
    "\n",
    "major_down = []\n",
    "for idx, row in walk_forward_dataset.iterrows(): \n",
    "    start = row['TIMESTAMP_START']\n",
    "    end = start+timedelta(hours=hour)\n",
    "    frame = wba124_clean_status[(wba124_clean_status['TIMESTAMP_START']>start) & \\\n",
    "                                 (wba124_clean_status['TIMESTAMP_START']<end)]\n",
    "    UD = frame.loc[frame['LEVEL3']=='UDT']\n",
    "\n",
    "    if len(UD) == 0: #no record within this 6 hours:\n",
    "        major_down.append(0)\n",
    "    else:\n",
    "        time_diff = (UD['TIMESTAMP_END']-UD['TIMESTAMP_START']).dt.seconds\n",
    "        if any(time_diff>=threshold): #threshold = 3600s\n",
    "            major_down.append(1)\n",
    "        else:\n",
    "            major_down.append(0)\n",
    "    \n",
    "walk_forward_dataset[\"24 HOUR DOWN\"] = major_down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7ab490",
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_forward_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0032783a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Baseline model 2 on cleaned removed data #####\n",
    "\n",
    "not_feature = ['TIMESTAMP_START', 'TIMESTAMP_END', 'EQ_ID', 'LEVEL3_NAME', 'LEVEL3', 'STATE_NAME']\n",
    "\n",
    "lb = LabelEncoder()\n",
    "walk_forward_dataset['ENC STATE NAME'] = lb.fit_transform(walk_forward_dataset['STATE_NAME'])\n",
    "\n",
    "tmp_table = walk_forward_dataset.drop(not_feature, axis=1)\n",
    "\n",
    "df = tmp_table[['DURATION', 'ENC STATE NAME']]\n",
    "target = tmp_table['24 HOUR DOWN'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11aca566",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_idx = int(0.7*len(df))\n",
    "val_idx = int(0.8*(len(df)))\n",
    "\n",
    "X_train, y_train = df[:train_idx], target[:train_idx]\n",
    "X_val, y_val = df[train_idx:val_idx], target[train_idx:val_idx]\n",
    "X_test, y_test = df[val_idx:], target[val_idx:]\n",
    "\n",
    "clf = lightgbm.LGBMClassifier(\n",
    "    boosting_type='gbdt',\n",
    "    objective='binary',\n",
    "    random_state=42,\n",
    "    class_weight='balanced',\n",
    "    reg_lambda=0.01\n",
    "            )\n",
    "\n",
    "clf.fit(X_train, y_train,\n",
    "       eval_set=(X_val, y_val),\n",
    "       eval_metric='auc',\n",
    "        verbose=True)\n",
    "\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "confusion_matrix(y_test, pred), accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fe7027",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab177cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### TIME SINCE FAILURE #####\n",
    "major_down_idx = walk_forward_dataset[(walk_forward_dataset[\"LEVEL3\"]=='UDT') &\n",
    "                                     (walk_forward_dataset[\"DURATION\"] >= 3600)].index.to_list()\n",
    "\n",
    "# there are 515 transaction of major down\n",
    "tsf =[]\n",
    "tsf_df = walk_forward_dataset.iloc[major_down_idx[0]:major_down_idx[-1]+1]\n",
    "count = 0\n",
    "\n",
    "for idx, row in tsf_df.iterrows():\n",
    "    if idx in major_down_idx:\n",
    "        count = major_down_idx.index(idx)\n",
    "        tsf.append(0)\n",
    "        continue\n",
    "        \n",
    "    last_down_time = tsf_df.iloc[count][\"TIMESTAMP_END\"]\n",
    "    curr_time = row[\"TIMESTAMP_START\"]\n",
    "    diff = (curr_time - last_down_time).seconds\n",
    "    tsf.append(diff)\n",
    "tsf_df[\"TIME SINCE FAILURE\"] = tsf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5604896",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd2122f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsf_df['TIME SINCE FAILURE'][:900].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53c29e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelEncoder()\n",
    "enc_state = lb.fit_transform(tsf_df['STATE_NAME'])\n",
    "tsf_df['ENC STATE NAME'] = enc_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f2993c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Lag DURATION and STATE NAME feature #####\n",
    "lags = [1,2,3,4,5]\n",
    "for lag in lags:\n",
    "    tsf_df[f'DURATION LAGGED {lag}'] = tsf_df['DURATION'].shift(lag)\n",
    "    tsf_df[f'ENC STATE NAME LAGGED {lag}'] = tsf_df['ENC STATE NAME'].shift(lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341fa3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Rolling aggregation of DURATION and TIME SINCE FAILURE #####\n",
    "rolling = [5, 10, 15, 20]\n",
    "for roll in rolling:\n",
    "    tsf_df[f'DURATION ROLL {roll}'] = tsf_df['DURATION'].rolling(roll).mean()\n",
    "    tsf_df[f'TSF ROLL {roll}'] = tsf_df['TIME SINCE FAILURE'].rolling(roll).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f333d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### calculate number of occurrence of each state name within this day #####\n",
    "tsf_df = tsf_df.dropna(axis=0)\n",
    "\n",
    "tsf_df['DATE'] = tsf_df['TIMESTAMP_START'].dt.date\n",
    "tsf_df['DAY'] = tsf_df['TIMESTAMP_START'].dt.day\n",
    "tsf_df['WEEK'] = tsf_df['TIMESTAMP_START'].dt.isocalendar()['week']\n",
    "tsf_df['MONTH'] = tsf_df['TIMESTAMP_START'].dt.month\n",
    "tsf_df['YEAR'] = tsf_df['TIMESTAMP_START'].dt.isocalendar()['year']\n",
    "\n",
    "day_gb = tsf_df.groupby(['DATE', 'STATE_NAME']).count()['EQ_ID']\n",
    "week_gb = tsf_df.groupby(['YEAR', 'WEEK', 'STATE_NAME']).count()['EQ_ID']\n",
    "month_gb = tsf_df.groupby(['YEAR', 'MONTH', 'STATE_NAME']).count()['EQ_ID']\n",
    "\n",
    "freq_day = []\n",
    "freq_week = []\n",
    "freq_month = []\n",
    "\n",
    "for idx, row in tsf_df.iterrows():\n",
    "    state_name = row['STATE_NAME']\n",
    "    date = row['DATE']\n",
    "    week = row['WEEK']\n",
    "    month = row['MONTH']\n",
    "    year = row['YEAR']\n",
    "    \n",
    "    FREQ_DAY = day_gb.iloc[(day_gb.index.get_level_values('STATE_NAME')==state_name) & \n",
    "                        (day_gb.index.get_level_values('DATE')==date)].values[0]\n",
    "    \n",
    "    FREQ_WEEK = week_gb.iloc[(week_gb.index.get_level_values('STATE_NAME')==state_name) & \n",
    "                         (week_gb.index.get_level_values('YEAR')==year) &\n",
    "                         (week_gb.index.get_level_values('WEEK')==week)].values[0]\n",
    "    \n",
    "    FREQ_MONTH = month_gb.iloc[(month_gb.index.get_level_values('STATE_NAME')==state_name) & \n",
    "                          (month_gb.index.get_level_values('YEAR')==year) &\n",
    "                          (month_gb.index.get_level_values('MONTH')==month)].values[0]\n",
    "    \n",
    "    freq_day.append(FREQ_DAY)\n",
    "    freq_week.append(FREQ_WEEK)\n",
    "    freq_month.append(FREQ_MONTH)\n",
    "\n",
    "tsf_df['FREQ DAY'] = freq_day\n",
    "tsf_df['FREQ WEEK'] = freq_week\n",
    "tsf_df['FREQ MONTH'] = freq_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8533953",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsf_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a938a60b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fe3b56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ed7a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3b306f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "not_features = ['DATE','TIMESTAMP_START', 'TIMESTAMP_END', 'EQ_ID',\n",
    "               'LEVEL3_NAME', 'LEVEL3', 'STATE_NAME', 'DATE', '24 HOUR DOWN',\n",
    "               'DATE', 'YEAR', 'MONTH', 'WEEK', 'DAY',\n",
    "               'FREQ DAY', 'FREQ WEEK', 'FREQ MONTH']\n",
    "\n",
    "df = tsf_df.drop(not_features, axis=1)\n",
    "target = tsf_df['24 HOUR DOWN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fe7d10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_idx = int(0.7*len(df))\n",
    "val_idx = int(0.8*(len(df)))\n",
    "\n",
    "X_train, y_train = df[:train_idx], target[:train_idx]\n",
    "X_val, y_val = df[train_idx:val_idx], target[train_idx:val_idx]\n",
    "X_test, y_test = df[val_idx:], target[val_idx:]\n",
    "\n",
    "clf = lightgbm.LGBMClassifier(\n",
    "    boosting_type='gbdt',\n",
    "    objective='binary',\n",
    "    random_state=42,\n",
    "    learning_rate=0.05,\n",
    "    class_weight='balanced',\n",
    "    reg_lambda=0.01\n",
    "            )\n",
    "\n",
    "clf.fit(X_train, y_train,\n",
    "       eval_set=(X_val, y_val),\n",
    "       eval_metric='AUC',\n",
    "        verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c387f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19977e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(X_test)\n",
    "\n",
    "confusion_matrix(y_test, pred), accuracy_score(y_test, pred)\n",
    "# without class weight 57\n",
    "# with class weight 56, but recall improves "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91f6497",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e22399",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "pred = rf.predict(X_test)\n",
    "confusion_matrix(y_test, pred), accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1aff80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sample weights based on important state name\n",
    "# sample weight is the inverse of its frequency\n",
    "wba124_initial['STATE_NAME'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335aabed",
   "metadata": {},
   "outputs": [],
   "source": [
    "waitings = ['Waiting For Response', 'Waiting For Operator', 'Waiting For Repair', 'Waiting For Technician',\n",
    "           'Waiting For Spares', 'Waiting for Setup', 'Undefined Waiting']\n",
    "\n",
    "aaaaa = wba124_initial.loc[(wba124_initial.STATE_NAME.isin(waitings)) &\n",
    "              (wba124_initial.DURATION>=3600)].sort_values('DURATION',ascending=False)\n",
    "\n",
    "aaaaa.groupby('STATE_NAME').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a84a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbbbb = wba124_initial.loc[(wba124_initial.STATE_NAME.isin(waitings))]\n",
    "bbbbb.groupby('STATE_NAME').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bc9542",
   "metadata": {},
   "outputs": [],
   "source": [
    "aaaaa.loc[aaaaa.STATE_NAME==waitings[2]].sort_values('TIMESTAMP_START').head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6590ead3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENV-MAL-VE",
   "language": "python",
   "name": "env-mal-ve_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
